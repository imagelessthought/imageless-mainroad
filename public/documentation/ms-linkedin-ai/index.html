<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Black+Han+Sans&family=Changa+One&display=swap" rel="stylesheet">
	<title>Introduction to Generative AI and Microsoft Copilot - Imageless Thought</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Introduction to Generative AI and Microsoft Copilot" />
<meta property="og:description" content="The content in this article was created from my notes from the certification course, Career Essentials in Generative AI by Microsoft and LinkedIn with summarization and revisions performed using Microsoft Copilot.
Introduction What is Generative AI? Generative AI is changing how we create. (light music) For the first time, humans are supervising and machines are generating. It&rsquo;s helping us lift the dirty, dull, dangerous and difficult tasks from humanity&rsquo;s shoulders so then we can simply focus on the very essence of our work, the vision, the idea, and the purpose." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://imagelessthought.github.io/mainroad/documentation/ms-linkedin-ai/" /><meta property="article:section" content="documentation" />
<meta property="article:published_time" content="2023-12-15T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-12-15T00:00:00+00:00" />


		<meta itemprop="name" content="Introduction to Generative AI and Microsoft Copilot">
<meta itemprop="description" content="The content in this article was created from my notes from the certification course, Career Essentials in Generative AI by Microsoft and LinkedIn with summarization and revisions performed using Microsoft Copilot.
Introduction What is Generative AI? Generative AI is changing how we create. (light music) For the first time, humans are supervising and machines are generating. It&rsquo;s helping us lift the dirty, dull, dangerous and difficult tasks from humanity&rsquo;s shoulders so then we can simply focus on the very essence of our work, the vision, the idea, and the purpose."><meta itemprop="datePublished" content="2023-12-15T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-12-15T00:00:00+00:00" />
<meta itemprop="wordCount" content="41990">
<meta itemprop="keywords" content="AI,Microsoft," />
		<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Introduction to Generative AI and Microsoft Copilot"/>
<meta name="twitter:description" content="The content in this article was created from my notes from the certification course, Career Essentials in Generative AI by Microsoft and LinkedIn with summarization and revisions performed using Microsoft Copilot.
Introduction What is Generative AI? Generative AI is changing how we create. (light music) For the first time, humans are supervising and machines are generating. It&rsquo;s helping us lift the dirty, dull, dangerous and difficult tasks from humanity&rsquo;s shoulders so then we can simply focus on the very essence of our work, the vision, the idea, and the purpose."/>

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/mainroad/css/style.css">
	<link rel="stylesheet" href="/mainroad/css/custom.css">

	<link rel="shortcut icon" href="/mainroad/img/connection.svg">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo logo--mixed">
		<a class="logo__link" href="/mainroad/" title="Imageless Thought" rel="home">
			<div class="logo__item logo__imagebox">
					<img class="logo__img" src="/mainroad/img/connection.svg">
				</div><div class="logo__item logo__text">
					<div class="logo__title">Imageless Thought</div>
					<div class="logo__tagline">Michael Baggett</div>
				</div>
		</a>
	</div>
		
<nav class="menu">

  <div class="menu__btn-container">
    <button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
      <span class="menu__btn-title" tabindex="-1">≡</span>
    </button>
  <span style="width: 100%; display: flex; align-items: center;">
    <span class="menu__btn-text">Imageless Thought</span>
    <span class="menu__btn-logo">
      <a href="/" alt="UNT"><img class="menu__btn-logo-img" src="/mainroad/img/connection.svg"></a>
    </span>
  </span></div>


  <ul class="menu__list">
    <li class="menu__item">
      <a class="menu__link" href="/mainroad/">
        
          <span class="menu__text">Home</span>
        
      </a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/mainroad/about/">
        
          <span class="menu__text">About</span>
        
      </a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/mainroad/skills/">
        
          <span class="menu__text">Skills</span>
        
      </a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/mainroad/blog/">
        
          <span class="menu__text">Blog</span>
        
      </a>
    </li>
    <li class="menu__item menu__item--active">
      <a class="menu__link" href="/mainroad/documentation/">
        
          <span class="menu__text">Documentation</span>
        
      </a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/mainroad/portals/">
        
          <span class="menu__text">Portals</span>
        
      </a>
    </li>
    <li class="menu__item">
      <a class="menu__link" href="/mainroad/tech-links/">
        
          <span class="menu__text">Links</span>
        
      </a>
    </li>
    <li class="menu__item menu__dropdown">
      <a class="menu__link" href="">
        
        <span class="menu__text fw">
        <label class="drop-icon" for="More">More ▾</label>
        </span>
        
      </a>
      <input type="checkbox" id="More">

      <ul class="submenu__list">
        
          <li class="menu__item">
          <a class="menu__link" href="/mainroad/status/">
            
            <span class="menu__text">Status Check</span>
            
          </a>
          </li>
        
          <li class="menu__item">
          <a class="menu__link" href="/mainroad/repositories/">
            
            <span class="menu__text">Repositories</span>
            
          </a>
          </li>
        
      </ul>
    </li>
  </ul>
</nav>



	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Introduction to Generative AI and Microsoft Copilot</h1>
			<p class="post__lead">This article was created with assistance from Microsoft Copilot.</p>
			<div class="post__meta meta-post-author"><div class="meta__item-author meta__item">


    <div class="post__author-wrap">
        
            <span class="post__author-date-wrap post__author-date"></span><span class="post__author-date">Dec 15, 2023 :</span>
        
        
            <span class="post__author"><a href="mailto:baggett.michael@gmail.com">Michael Baggett</a></span><span style="color: var(--light_gray);"> - </span>
        
        
        <span class="postauthorarea"><b>College of Visual Arts and Design</b></span> -
        
        
        <span class="post__author-info">Director and Assistant Dean of Information Technology</span>
        
    
    </div>

</div><hr style="border-color: var(--theme-blue);"></div>
		</header>
		

		<div class="content post__content clearfix">
			<p>The content in this article was created from my notes from the certification course, <a href="https://www.linkedin.com/learning/paths/career-essentials-in-generative-ai-by-microsoft-and-linkedin" title="Career Essentials in Generative AI by Microsoft and LinkedIn"><strong>Career Essentials in Generative AI by Microsoft and LinkedIn</strong></a> with summarization and revisions performed using <a href="https://copilot.microsoft.com/" title="Microsoft Copilot">Microsoft Copilot</a>.</p>
<h2 id="introduction">Introduction</h2>
<h3 id="what-is-generative-ai">What is Generative AI?</h3>
<p>Generative AI is changing how we create. (light music) For the first time, humans are supervising and machines are generating. It&rsquo;s helping us lift the dirty, dull, dangerous and difficult tasks from humanity&rsquo;s shoulders so then we can simply focus on the very essence of our work, the vision, the idea, and the purpose. It&rsquo;s a complete paradigm shift for the future of jobs. I am Pinar Seyhan Demirdag and I&rsquo;m the co-founder and the AI director of Seyhan Lee. During this LinkedIn Learning course, we will talk about how to use that tool to drive the creation of your intention. Join me as we dive deep into this new creative revolution that I&rsquo;m so excited about and let&rsquo;s discover together how each of us can have a place in this age of advanced technologies.</p>
<h4 id="the-importance-of-generative-ai">The importance of generative AI</h4>
<p>The advent of generative AI can be likened to the invention of photography and celluloid film, a true creative revolution. With photography, we no longer needed to rely on the interpretation of an artist to capture reality, and with generative AI, we no longer need artistic talent to draw or to sing. We can access now concise information in just a manner of seconds. We can also automatically generate text such as news articles or product descriptions. We can even design custom products like shoes or furniture. We can produce music, speech, visual effects, 3D assets and sound effects using algorithms trained on already existing data. I know you&rsquo;re like, it all sounds magic, doesn&rsquo;t it? Yeah, it does. But like Arthur C. Clark said, any sufficiently advanced technology is simply undistinguishable from magic. These machines are becoming 24 hours, seven assistants ready to perform a variety of tasks for us, resulting in the manifestation of our vision almost in real time. By training on large data sets, we can reduce the burden of repetitive tasks and complex computations, enabling us to focus on more creative and more strategic activities which is the very essence of what work is really about. The development of generative AI has a very rich and very fascinating history marked by significant breakthroughs, even though it gained widespread attention in 2022, its evolution was built on decades of mathematical research, starting with auto encoder neural networks in 2006 and continuing on through the mass adoption of generative AI models like DALL E, ChatGPT by Open ai, Kubrick, Journey and others today. Not only have the providers and services expanded but the quality of what is being produced has improved drastically as well. Like you can see in this chart by our world in data the exponential improvement in the quality of images generated between 2014 and 2022. While the pictures in 2014 are black and white and extremely pixelated. In 2022, we would simply type any word on the screen and generate coherent outcomes. Generative AI is not only changing almost every single profession, but it is also changing our understanding of what work is. Large parts of the production process that are repetitive or can be computational are now starting to be facilitated by AI models. All of this leads to the chance we are given to discover the essence of what it means to be a human and the true meaning of work. A beautiful new existence awaits us where we focus on what makes us unique as a species, our curiosity, our conscious awareness, our dreams, our emotional intelligence, and our vision while the algorithms we have created assist us in the production and the execution of our authentic vision.</p>
<h4 id="how-generative-ai-is-different-than-other-types-of-ai">How generative AI is different than other types of AI</h4>
<p>Generative AI is a type of AI that, as this name suggests, generates new content. This is in contrast to other types of AI, like discriminative AI, which focuses on classifying or identifying content that is based on preexisting data. Generative AI is often used in applications such as image generation, video synthesis, language generation, and music composition, but to really understand this new tool, we need to know first where it fits in the broader AI landscape. The term AI, which is artificial intelligence, is an umbrella term that encompasses several different subcategories, including generative AI. These subcategories are used to perform different tasks. For example, reactive machines are used in self-driving cars. Limited memory AI forecasts the weather. Theory of mind powers virtual customer assistance. Narrow AI generates customized product suggestions for E-commerce sites. Supervised learning identifies objects from things like images and video. Unsupervised learning can detect fraudulent bank transactions, and reinforcement learning can teach a machine how to play a game. These are only a few of the subcategories, and generative AI models fall into a lot of these categories, and honestly, it&rsquo;s only growing. These other types of AI may still generate content, but they do it as a side effect of their primary function. Generative AI is specifically designed to generate new content as its primary output. Whether this is text, images, product suggestions, whatever, that&rsquo;s what generative AI is designed to do. So, now that we know where generative AI fits in the broader landscape, together, let&rsquo;s explore how it works.</p>
<h4 id="how-generative-ai-works">How generative AI works</h4>
<p>To understand how generative AI works, we first have to understand how it comes to life. I know how you&rsquo;re feeling. You&rsquo;re looking at the news, and ChatGPT, and Midjourney, all this generative, what is generative, what is AI? It&rsquo;s so complicated. You have no idea how to make sense of it all, but you know you need to, because this is where the world is evolving towards. Okay, let&rsquo;s start with AI 101. Imagine you and me were having dinner and you asked me to pass you the salt. I look at the table and I can make a discernment between a salt shaker and the rest of the objects on the table. Why? Because my mind has been trained with thousands, or millions, or trillions of salt shakers earlier. AI works the same. You feed it with thousands, millions, trillions of content, and then you teach a certain algorithm to generate outputs and solutions as a result. Okay, now that we got AI 101 out of the way, let&rsquo;s get into generative AI 101. Let&rsquo;s use cars as an example. Just like a Porsche has a different engine than a Mazda, under the umbrella term of generative AI, there are a variety of different generative AI models. These AI models, or car engines, are written and manufactured by groups of highly advanced computer vision specialists, machine learning experts, and mathematicians. They&rsquo;re built on years of open source machine learning research and generally funded by companies and universities. Some of the big players in writing these generative AI models, engines, are Open AI, NVIDIA, Google, Meta, and universities like UC Berkeley and LMU Munich. They can either keep these models private, or they can make them public, what we call this, open source, for those to benefit from their research. All right, now that these complex generative models are written, meaning the engines are made, what are we going to do with them? Depending on your level of technical expertise, this can look a bit different. I&rsquo;m going to paint a picture for you with three different end users of these models. The first person is a business leader who comes up with a product idea that involves a generative AI model, or several. For the development of their tool, this business leader either uses free open source generative AI models or enters into a partnership with a corporation to get rights to their generative AI model, then their team creates their vision. To continue the chronology, let&rsquo;s say this person owns the car factory. They direct where the engine and chassis go, but don&rsquo;t actually work on the floor. The second person is a creative person with an appetite for adventure. They might have some technical knowledge, but they aren&rsquo;t an AI engineer. I mean, they can be if they want. This person goes to a car engine showroom, where they pick a pre-made car engine or a generative AI model from a repository like GitHub and Hugging Face. After that, they go to a chassis manufacturer to pick their empty shell for their new engine, their precious new engine. These chassis are called AI notebooks. Their purpose is to hold and run the generative AI model code. The most widely used one is Google Colab, but there are others like Jupiter Notebooks. And the third person would be my mother, bless her heart. She has absolutely no technical pedigree, nor she&rsquo;s interested in acquiring one. But this doesn&rsquo;t mean she cannot benefit from generative AI. My mother would be buying her already made car. She will have way less control over the outcome of her car, but she will be able to drive, just like the business leader and the creative technologist. People with no technical knowledge can simply subscribe to an online service like OpenAI&rsquo;s ChatGPT or DALL-E, or download Discord and play with Midjourney, or download Lensa AI and Avatar Maker in their smartphone to play with the magic of generative AI. Well, this all depends what you want to do and what you want to build, and how much technical expertise you already have. Now that we have our car, our generative AI model, we can now start creating our own content and go for a drive.</p>
<h4 id="creating-your-own-content">Creating your own content</h4>
<p>So now that we have our generative AI model and our chassis, we are ready to create our own content. If we&rsquo;re a beginner, we can use a paid service like Midjourney or Lensa. If we are more experienced, we can use a notebook and pick from available models. If you download a commercial app and upload only 10 pictures of yourself, like I did, the app suggests a variety of different avatars of yourself. It&rsquo;s super fun. If you are a more experienced generative AI user, a creative technologist, you can go to GitHub, choose your favorite generative AI model, and see if it is available in the form of a notebook. If it&rsquo;s not available, you can still inquire about it inside the generative AI community. They&rsquo;re super friendly people. And if you are a programmer, you can also create your own notebook by taking the model code from GitHub. For a demonstration, we will be running a Google Colab notebook named Deforum, that is based on stable diffusion, to generate a fantasy landscape. As you can see here, the notebook runs through the code, and depending on your settings, produces a personalized outcome. Google Colab requires a subscription. If you would like your generation time to be minimized you need to buy a paid subscription. These notebooks offer a lot of personalized options for the quality of your outcomes. You can always choose the default generation, but the beauty of working with a notebook is that you can tweak and personalize the outcomes. In summary, a model is a set of algorithms that have been trained on a specific dataset. A notebook is a tool for writing and running the code. A creative application is an example of how a model can be used, and the generated outcome is what the end user produces by using a generative AI service or a notebook that houses the model inside it.</p>
<h3 id="main-models">Main Models</h3>
<h4 id="the-most-famous-tools-for-generative-ai">The most famous tools for Generative AI</h4>
<p>During this part of the course, we will be getting more familiar with some of the most well-known types of generative AI models, and some of the applications they cover. Think of generative AI models, let&rsquo;s use the metaphor, food. Under the term food, you find salads, soups, caviar, stews, fresh vegetables, and just like food, under the umbrella term of generative AI, you find several options depending on what you are craving for. So this isn&rsquo;t meant to be an exhaustive list of all the applications and all the models, but think of it more like a guide to the generative AI landscape and how you can make use of it. But keep in mind that this landscape is changing dramatically every single day. And there&rsquo;s a very good chance that when you&rsquo;re watching this course, there will be lots and lots of new players, models, and applications already born. But that&rsquo;s what makes it so exciting. So let&rsquo;s start exploring some of the main models together.</p>
<h4 id="natural-language-models">Natural language models</h4>
<p>Natural language generation is perhaps the most well-known application of generative AI so far with ChatGPT in the headlines. Most of the hype around text-based generative AI is using a model called GPT. GPT stands for Generative Pre-trained Transformer. It&rsquo;s a language model developed by OpenAI, a research organization focused on developing and promoting friendly AI. The idea of pre-training a language model and finding it on a task-specific dataset isn&rsquo;t something new. This concept has been around for decades and has been used in several other models before GPT. However, GPT has become notable for its large scale use of transformer architecture and its ability to generate human-like texts, which had led to its widespread use and popularity in the field of natural language processing. Imagine you have a writing assistant that can help you write emails, articles, even a novel. GPT can take in a prompt, like a topic or a sentence, and can generate text based on that prompt. It can even continue a story or a conversation you started earlier. Here are a few industry applications. Let&rsquo;s start with GitHub. GitHub Copilot is a generative AI service provided by GitHub to its users. The service uses the OpenAI codex to suggest the code and entire functions in real time, right from the code editor. It allows the users to search less for outside solutions and it also helps them type less with smarter code completion. Another example would be Microsoft&rsquo;s Bing, which implemented ChatGPT into its search functionality, enabling it to reach concise information in a shorter amount of time. Since OpenAI made ChatGPT available to the public on November 30th in 2022, it reached 1 million user in less than a week, I said in less than a week. Now, let&rsquo;s compare that to other companies that hit 1 million users. It took Netflix 49 months to reach 1 million users. It took Twitter 24 months, it took Airbnb 30 months, Facebook, 10 months, and it took Instagram two-and-a half-months to reach 1 million users. Let&rsquo;s remember, it took ChatGPT only one week. These figures demonstrate how easily humans adopted their workflow for co-creating with generative AI-based tools and services. This is amazing. However, GPT has several limitations, such as the lack of common sense, creativity and understanding the text it generates. Also, bias data sets and the danger of normalization of mediocrity when we come up with creative writing. Natural language models synthetically mimic human capabilities, but, clearly, conscious contemplations are required before developing generative AI tools. ChatGPT is a wonderful tool for factual and computable information. However, I would advise us to approach it with caution when inquiring about creative and opinion-based writing.</p>
<h4 id="text-to-image-applications">Text to image applications</h4>
<p>In 2022, we have seen a rise in commercial image generation services. The technology behind these services is broadly referred as text to image. You simply type words on a screen and watch the algorithms create an image based on your queue, even if you description is not very specific. There are three main text to image generation services. Midjourney, DALL-E, and Stable Diffusion. If we were to compare these three text to image tools to operating systems, Midjourney would be macOS because they have a closed API and a very design and art-centric approach to the image generation process. DALL-E would be Windows but with an open API because the model is released by a corporation and it initially had the most superior machine-learning algorithm. Open AI values technical superiority over design and art sensitivities. And the third, the Stable Diffusion would be Linux because it is open source and is improving each day with the contribution of the generative AI community. The quality of the generated images from text to image models can depend both on the quality of the algorithm and the datasets they use to train it. So now that we know the main services, let&rsquo;s look at three industrial applications. First is Cuebric. Hollywood&rsquo;s first generative AI tool created by our company, Seyhan Lee, for streaming the production of film backgrounds. A normal virtual production workflow uses three dimensional world building which involves a bunch of people building 3D worlds that are custom made for that film. It&rsquo;s time consuming, expensive, and requires a lot of repetitive tasks. An alternative now is to augment 2D backgrounds into 2.5D by involving generative AI in the picture creation process. The second example would be Stitch Fix. When they suggest garments to discover their customer&rsquo;s fashion style, they use real clothes along with clothes generated with DALL-E. And finally, marketers and filmmakers use text to image models when ideating for a concept in a film. And actually, they may later on continue to use it to make storyboards and even use it in the production of the final art of their campaigns and films. Just like we have seen in Cuebric. A recent example from the marketing world would be Martini that used the Midjourney generated image in their campaign. Another one would be Heinz and Nestle that used DALL-E in their campaign. And GoFundMe that used Stable Diffusion in their artfully illustrated film. Marketers prefer using generative AI in their creative process for two reasons. First, for its time and cost-saving efficiency, and the second, for the unique look and feel that you get from text to image based tools.</p>
<h4 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h4>
<p>Another renowned generative AI model is generative adversarial networks, also referred as simply GANs. To illustrate how GANs work, let&rsquo;s give a game of forgery as a metaphor. Imagine you have an artist called The Generator who is trying to recreate a painting that is so realistic that it looks like a famous painting. And you have another person called The Discriminator who&rsquo;s an art expert and trying to spot the difference between the real painting and the forgery. The Generator creates a painting and The Discriminator evaluates it, giving feedback to the generator on how to improve the next iteration. The Generator and The Discriminator played this game repeatedly until The Generator creates the painting that is so realistic that The Discriminator can&rsquo;t tell the difference between it and the real painting. In the same way, a GAN model has a generator and a discriminator. The two parts work together in a competition. That&rsquo;s why it&rsquo;s called generative adversarial networks. In this way, they improve the generator&rsquo;s ability to create realistic data, and over time, the generator becomes better and better at creating realistic data. And the results start yielding in the creation of products, assets, faces, people, that didn&rsquo;t exist before, just like we have seen with text-to-image that we have seen in the former session. The difference though is that with GANs, you input one type of data, like pictures or bank transitions, and then you output the same type of data. Let&rsquo;s now give three real-world examples where GANs were used. We&rsquo;re going to start with Audi. They trained their own GANs to get inspiration for their wheel designs. This process created lots of different wheel designs that simply didn&rsquo;t exist before, and gave inspiration to Audi designers so they can pick and choose which designs they wanted to use in their final decisions. And remember, AI didn&rsquo;t design the final wheel. AI was simply a tool that the wheel designers used to inspire themselves for the final designs that they would make. Next, Beko, that is a European-based appliance brand, they use custom trained GANs in their sustainability stand film, which also happens to be the world&rsquo;s first brand-funded AI film created and produced by Seyhan Lee. We use GANs to generate lightning, leaves, roots, eyes, flowers, and created seamless transitions to flow between humans and nature. GANs have this beautiful transitional quality. And finally, in the context of financial fraud detection, GAN models can be used to generate synthetic versions of fraudulent transactions, which can then be used to train a fraud detection model. You know what&rsquo;s really surprising with GANs is that the same generative AI model can be used for two very distinct professions. Here we are seeing some financial fraud detection solving and create a new tire styles for Audi. And then later on, the same AI model makes impossibly beautiful visual effects for film, and that versatility is the greatest power of GAN models.</p>
<h4 id="vae-and-anomaly-detection">VAE and Anomaly Detection</h4>
<p>Let&rsquo;s now move to talking about an application of generative AI that may not be as obvious as it&rsquo;s used in generating images, like we have seen earlier, audio or text. But it&rsquo;s still very important application nonetheless, and it is going to be the anomaly detection. One of the main models that we use in this space is Variational Autoencoders, referred as VAE. These models can be used for anomaly detection by training the model on a dataset of normal data, and then using the trained model to identify instances that deviate from the normal data. This can be used to detect anomalies in a wide range of situations, like finding fraud in financial transactions, spotting flaws in manufacturing or finding security breaches in a network. For example, Uber has used VAE for anomaly detection in their financial transactions to detect fraud. Another example would be Google has also used VAE to detect network intrusions using anomaly detection and another one of a real world application of VAE would be anomaly detection in industrial quality control. In this scenario, a VAE can be trained on a dataset of images of normal products and then used to identify images of products that deviate from the normal data. In this way, it can be used to detect defects in products such as scratches, dents, or misalignments. Another real world example would be healthcare where VAE is used to detect anomalies in medical imaging such as CT scans and MRI, like Children&rsquo;s National Hospital in Washington, DC uses a generative AI model to analyze electronic health records. The model uses data such as vital signs, laboratory results and demographic information to predict which patients are at risk of sepsis, allowing healthcare providers to intervene early and improve patient outcomes. Variational Autoencoders are a flexible, generative model that are not only able to detect anomalies but are also a part of the architecture of several other generative AI models.</p>
<h3 id="future-of-ai">Future of AI</h3>
<h4 id="future-predictions">Future predictions</h4>
<p>The best way to predict the future, as they say, is to invent it so let&rsquo;s talk about the future. In two to three years in the gaming, film and marketing sectors generative AI will continue to be used in computer graphics, and animation to create more realistic, and believable characters, and environments. This is going to be particularly important in 3D modeling. Generative AI will be used to improve natural language understanding in virtual assistants and chatbots making them more and more capable of handling complex and nuanced conversations. In the energy sector, generative AI models will be used to optimize energy consumption and production, such as predicting demand, and managing renewable energy sources, as well as improving the efficiency of energy distribution networks. As for the transportation sector, generative AI models will be used to optimize traffic flow and to predict maintenance needs for vehicles. In short, generative AI will be used to automate repetitive tasks and improve efficiency in a wide variety of industries. My predictions for the next 10 to 15 years would be generative AI will be used to create more and realistic, and accurate simulations in fields such as architecture, urban planning, and engineering. The second would be to be used to create new materials and products in fields, such as manufacturing and textile design. The third will be natural language generation will be improved in the fields of content creation such as news articles, books, and even movie scripts. It will also improve self-driving cars by generating realistic virtual scenarios for testing and training, and also it will excel in audio to asset generation where you can speak, and then have the AI generate an asset. In short, my prediction for the upcoming 10 to 15 years would be generative AI will be used in the creation and production of mass media quality books, films, and games. Meanwhile, it will also be the technology behind paradigm shifting implications in the job market, such as self-driving cars, advanced robotics for manufacturing, and for warehousing, and improved crop yield and precision agriculture.</p>
<h4 id="the-future-of-jobs">The future of jobs</h4>
<p>When starting to work with generative AI, it&rsquo;s imperative that we hold conscious contemplations for the future of jobs. As you know, there&rsquo;s a lot of hype at the moment filled with a lot of Hollywood-powered fear that machines are now taking over. This is simply not true. If anyone is taking over, it is us humans who are entering in a new golden age for creativity and production. Will there be a shift in the job market? Yeah, absolutely, as it has always been throughout history. Whenever a new advanced technology is introduced, it&rsquo;s normal for some jobs to disappear, while other new ones are introduced. Let&rsquo;s give an example. Before alarm clocks, there were knocker uppers. These were kids who were hired to knock on the windows to wake people up. The job obviously disappeared when new job opportunities for alarm clock manufacturers emerged. The second example would be the switchboard operator jobs. They disappeared due to the widespread adoption of automated telephone exchange systems, and even though it made jobs disappear, automated telephone exchange systems revolutionized communication and connected people all across the world. What if I were to tell you that there&rsquo;s a very high possibility that certain parts of your job that are repetitive, dirty, dull, dangerous, or difficult, the four D&rsquo;s, can be automated? As a result, you will have more time focusing parts of your skills that are more human-centric, like creativity, problem solving, empathy, and leadership. Just like the digital revolution in the 90s gave birth to the emergence of new companies, we&rsquo;re now witnessing several new endeavors due to the generative AI revolution. As the co-founder of a generative AI-powered creative company, let me assure you that all our operations are run by humans. Our company employs developers, cloud architects, generative AI artists, customer relations, project managers, writers, creative directors, and human producers. If the Industrial Revolution created jobs that were robotic for humans, the Generative AI Revolution will be our liberation and freedom from them. Each person will turn into their own creative studio and the barriers between your vision and its manifestation will disappear. Complex films, music, writing, and all forms of creative production tools will be simplified and given to your fingertips.</p>
<h3 id="ethics-and-responsibility">Ethics and Responsibility</h3>
<h4 id="moral-and-executive-skill-set-required-to-work-with-genai">Moral and executive skill set required to work with GenAI</h4>
<p>Any executive or business leader should approach generative AI tools with caution. We should at all times self-monitor and self-question if the generated results fits our quality and satisfaction parameters. Just because ChatGPT generates headlines, it doesn&rsquo;t mean that what it makes is great. Or just because we can generate a landscape with Stable Diffusion, it doesn&rsquo;t mean it&rsquo;s ready for the final pixel of a movie. During this grace period where we learn to co-create with algorithms, it&rsquo;s crucial that we deepen our executive skills. If we are a founder or an executive in a generative AI company, we shall, at all times, always ask ourselves who is benefiting from our tools? Our moral compass should always direct towards transparency, fairness, empathy, and responsibility. I highly recommend organizing a board or a council at your organization that acts like the ethical foundation for the integration of generative ai. Provide, please, all your employees with ethical guidance and education on how to use generative AI effectively, and also how to overcome their fears, challenges, and biases towards this new advanced tool. As the technology continues to evolve, it&rsquo;s very likely that the distinction between human and algorithm-generated content will become increasingly blurred, and it will become even more important for leaders like you to have a clear understanding of the role of each. We will remain in control of AI by always positioning the human consciousness in the very center of generative AI companies and solutions. Especially in our organizations, we must ensure that humans remain the sole decision makers. In this way, we will be setting the objectives and determine the direction of AI-generated content. By keeping humans at the very center, we can ensure that the content produced by generative AI aligns with your company values and goals, which have to do with serving the elation and elevation of humanity. By actively engaging with generative AI technology and developing a deep understanding of its capabilities and limitations all across your team, we can avoid the potential risks associated with blindly relying on AI to make decisions for us. Well, ultimately, the goal is to strike a beautiful balance between leveraging the power of generative AI to enhance human creativity and imagination, and optimizing production, while also maintaining human control and oversight over this advanced technology.</p>
<h4 id="caution-when-working-with-gen-ai">Caution when working with Gen AI</h4>
<p>I want to round out this course with a controversial statement. The greatest bias in AI is not race, is not ethnicity, nor gender. It is human&rsquo;s inferiority complex. If we see machines as superior to humans, we place them on a pedestal, and if we see humans as incapable fragile beings, we again place AI on a pedestal, but this time with the power of an authority. We should always emphasize the crucial and essential role of human creativity and decision-making in the process. Nowadays, popular headlines suggest that AI is designing, AI is coding, but let&rsquo;s remember, it&rsquo;s humans who wrote the algorithms for AI, and it&rsquo;s humans who conceptualize, curate, and oversee the algorithms to produce the desired outcomes. If we place AI and technology at the center of our workflow in storytelling, we risk dehumanizing ourselves and contributing to a future where human jobs may really be eliminated. Instead, we should focus on highlighting the central role that humans play in the creation and the use of AI. I know even though sentences such as like, &ldquo;AI made this art, and, &ldquo;AI is advancing quickly, AI is so cool,&rdquo; are common in the collective, we should strive to correct ourselves in centering our actions and self-expression around humans. It is humans that are making art by using generative AI-powered tools, and it is, again, humans that are working in tandem with each other to advance several different human-benefiting technologies, including generative AI. By modeling our tools after ourselves, we are inevitably transferring our own judgments, our own insecurities, and our own limitations onto this technology. So it is essential that we work to overcome our own insecurities and approach AI as a tool that can augment and empower us, rather than compete or replace us. By doing so, we can create AI systems that contribute to the elation of humanity, assist us with creative productivity, and help us achieve our greatest potential as a species.</p>
<h3 id="next-steps">Next steps</h3>
<p>There&rsquo;s only one way to go from here. We choose to overcome all fear, judgment, and prejudice around AI. I understand. I genuinely, wholeheartedly understand and empathize with you with all the confusion people have around AI. &ldquo;AI is going to take my job.&rdquo; &ldquo;It&rsquo;s going to replace me.&rdquo; &ldquo;What will I become if AI starts doing my job?&rdquo; These are, I understand prevailing fears that I hear around me all the time, but next time that we&rsquo;re confused with AI let&rsquo;s always get back to the basics and remember that AI is nothing but a tool in the service of humanity, in the service of you. The best way to overcome fear is to broaden our perspective. And how are we going to do that? Let&rsquo;s start with doing our own research. We can research further after discourse about AI and the power of humanity. We can think about what makes humans different than the machine and how can we strengthen these capabilities. Second, to start making things today now. As we have seen during the course, you don&rsquo;t need to have a technical understanding of generative AI to start working with it. Like I&rsquo;m the co-founder of a company that uses generative AI, and still to this day I haven&rsquo;t written a single line of code. And finally, stay creative. The biggest challenge that awaits us in the future will be overcoming the normalization of mediocrity. Generative AI is great, yeah, it&rsquo;s awesome but it&rsquo;s simply a tool, like we said before. It&rsquo;s like camera that works the same way over and over when you use it. If we become lazy and start relying on the machines for the production of content and assets without staying creative, we will be receiving outcomes that are devoid of the sparkle of human ingenuity. This is a very fast moving field. Like my AI Director friend, John Finger, says, &ldquo;We live in the time of cutting edge obsolescence.&rdquo; Even though generative AI may be one of the most advanced technologies out there, it&rsquo;s exponential development means that just because you know how things work one week you can simply completely be obsolete the next. For this, I would like to share with you all the resources I surround myself for my daily dose of generative AI to make sure that you don&rsquo;t miss a beat. Thank you wholeheartedly for listening to me and joining me in this course. And I leave you with this. Always remember that you are the power behind AI.</p>
<h2 id="generative-ai-the-evolution-of-thoughtful-online-search">Generative AI: The Evolution of Thoughtful Online Search</h2>
<h3 id="how-finding-and-sharing-information-online-has-evolved">How finding and sharing information online has evolved</h3>
<p>For years, we&rsquo;ve navigated the vast seas of the internet using search engines as our compass, typing inquiries and scrolling through results. But reasoning engines like ChatGPT and Bing chat have forever changed this journey. No longer are we merely typing keywords. We&rsquo;re having conversations, but this new frontier requires careful navigation. Thoughtful questioning and clear context are essential to achieve meaningful and accurate results. I&rsquo;m Ashley Kennedy, Managing Staff Instructor at LinkedIn Learning. In this course, I&rsquo;ll be speaking with two top experts in this emerging field, Noelle Russell, who leads the Global AI Solutions at Accenture. And Brandie Nonnecke, Founding Director of CITRIS Policy Lab and Research professor at UC Berkeley. Together, we&rsquo;ll explore the exciting evolution of thoughtful online search.</p>
<h3 id="search-engines-vs-reasoning-engines">Search Engines vs. Reasoning Engines</h3>
<h4 id="how-a-search-engine-works">How a search engine works</h4>
<p>You&rsquo;ve probably used a traditional search engine like Google or Bing countless times for everything from looking up basic information to performing deeper research, to getting news headlines, to shopping, travel, entertainment and more. There&rsquo;s so much you can find out using simple keywords and queries, but if you ever wondered what was happening under the hood to provide you with your answers. A traditional search engine works by performing three main functions, crawling, indexing, and ranking. Let&rsquo;s talk about this. First, crawling. search engines use computer programs, called web crawlers or spiders, to discover new and updated webpages. These crawlers systematically browse the internet following links from one webpage to another. This process of crawling is continuous, and its goal is to keep the search engine&rsquo;s index up to date. Next, indexing. Indexing takes all the content found during the crawling process, and stores, and organizes it in a massive database called the search index. This is where a search engine keeps all webpage information, and also analyzes various factors like keywords, metatags, headings, and links to understand the relevance and context of all of this data. Then, there&rsquo;s ranking. Ranking is the process of providing the best, most relevant results in response to a query. That means each time you enter a query, the search engine&rsquo;s algorithm scans the search index to find the best webpages, and then it ranks those pages on a variety of factors, including relevance, popularity, authority, and more. These ranked results are then presented to the user with the most relevant and authoritative results first. The results page also often includes other content, like FAQs, sponsored ads, plus more data, like images, video, news, articles, maps, and more depending on the search engine, and the nature of the query. So here you can see my search engine query, &ldquo;Parts of a grant proposal.&rdquo; It returns all of these useful resources. Most of them are webpages that I can access to read further about my query. This type of results return probably looks very familiar to you. Search engines have long been an easy, effective free way for you to find all sorts of information. They offer comprehensive, wide ranging access to countless topics, and because they prioritize the most relevant and accurate results, it&rsquo;s a great time saver. Many search engines also have advanced search options, like filters, suggestions, and search shortcuts, which let you refine your search for even better results. Now, that we&rsquo;ve reviewed how a search engine works, let&rsquo;s explore how a reasoning engine works, and then we&rsquo;ll be able to discuss them in relation to one another.</p>
<h4 id="how-a-reasoning-engine-works">How a reasoning engine works</h4>
<p>A reasoning engine is a system that uses logic and inference methods to draw conclusions, make decisions, summarize information, or solve problems based on available data and knowledge. - So a reasoning engine, it&rsquo;s in the title, right? You assume that it&rsquo;s making some rational thinking. It&rsquo;s reasoning from the inquiry that you&rsquo;re giving it as compared to a search engine, which is more likely just to give you back a response to the direct information that you queried. - There are many different types of reasoning engines, but here I&rsquo;d like to talk about generative AI models like ChatGPT and Bing Chat. These systems process and understand human language. So when a user enters a query, the reasoning engine can provide a relevant and informative response using human-like speech. It&rsquo;s kind of like having a digital friend who can answer your questions, engage in discussions, and help you with various tasks. Let&rsquo;s talk just a bit about how this works. The GPT large language model that powers these reasoning engines was pre-trained on a massive amount of training data. This includes text and code from the internet, from books, articles, webpages, Wikipedia, and more. During this pre-training phase, the model learns language patterns, grammar, syntax, and factual knowledge. The model then uses probability to predict the next words in response to a given prompt. In the initial training phases, human supervisors oversee the process, guiding the model toward accurate responses and contributing to its knowledge development. During later training stages, the AI model is asked to generate multiple responses to a prompt, and then human supervisors evaluate and rank these responses from best to worst. Gradually, the model improves its understanding of prompts and increases the probability of generating accurate, comprehensive, human-like responses. Once all of that happens, you have your smart digital friend that you can chat with. - In most cases, these reasoning engines are pre-trained on a bunch of other similar-like reasoning, where you&rsquo;ll see the difference in the kind of response you get from a reasoning engine as opposed to a traditional search engine because it&rsquo;ll be much more conversational, but also much more elaborate in its connection to what you&rsquo;re asking for and the different types of responses that it can create. - Take a look at a few different types of results I can get from a reasoning engine. Instead of asking what the various parts of a grant proposal are, I can ask Bing Chat to actually help me ideate and research and write various parts of a grant proposal in a very customized way. Reasoning engines aren&rsquo;t only used to generate text. They can generate other media types as well, including images. Text-to-image generative AI programs, where neural networks learn the relationship between words and a massive database of images, let you enter a text-based query to produce an image-based result. These models can create complex imagery while also responding to a wide variety of concepts, tones, and styles. Take a look at these images that I can create using the Bing Image Creator when I enter the prompt, a young boy walking a dog in a forest under a full moon, digital art style, bold colors. - You can use these tools to create incredibly beautiful or realistic imagery or imagery that is, you know, perfectly aligned with what you had envisioned. I think that that&rsquo;s a skill that artists can use to say, okay, if you want to use these tools, I have deep skills in being able to write prompts in a way that will actually get to that end product. So I actually think it might be a new tool that artists can use to express themselves. - So whether it&rsquo;s text, image, or other media, reasoning engines have burst onto the scene and attracted a lot of attention and excitement with the possibilities of what users can create. Let&rsquo;s continue to talk about how you can use reasoning engines both independently and in conjunction with search engines.</p>
<h4 id="comparing-search-engines-with-reasoning-engines">Comparing search engines with reasoning engines</h4>
<p>Let&rsquo;s talk about some of the pros and cons of reasoning engines versus search engines. We know that when you enter a query into a traditional search engine like Google or Bing, it searches through its vast index of web content, and then provides you with a ranked list of webpages that contain relevant information. So it&rsquo;s a good resource to use when you&rsquo;d like to read further about a subject across a collection of different sources, but not necessarily when you want to ask deeper questions. That&rsquo;s because a search engine doesn&rsquo;t truly understand your query. It just matches your keywords to relevant results. - Traditional search is much more linear, right? There&rsquo;s a linear request being asked against a data set to bubble up anything that&rsquo;s remotely similar. There&rsquo;s often some level of ranking that&rsquo;s done to determine, you know, how closely to your question did a answer get, and that&rsquo;s how that rank of, you know, what&rsquo;s on page one of a search engine versus page two. But very rarely does it actually think about what did you mean by what you asked? And this is often a frustration with us as users thinking about a search engine. I&rsquo;m trying to find something and I&rsquo;ll often be like, &ldquo;Well, no, that&rsquo;s not I meant to say,&rdquo; and I&rsquo;ll have to adjust it. - A reasoning engine, on the other hand, is designed to understand and interpret human language, which allows it to engage in actual conversations with you. So instead of simply retrieving webpages for you to read and extract information, it provides you with direct relevant responses and can also maintain context and understand the intent behind your questions. - Reasoning engines take in a completely different perspective when you&rsquo;re asking it questions. So what we&rsquo;ve seen in the evolution of something like a GPT, for example, is that when we start to ask a questions rather than it linearly looking for a direct answer and ranking everything that&rsquo;s similar to our response, it now is taking in the context of that question related to each other, right? So maybe topics, locations, places. - Sometimes, especially when you&rsquo;re first starting out, it may not be immediately obvious which one to use. But often, just by going through the motions of asking your question, you&rsquo;ll get a sense of which direction you should go. - Honestly, the first thing I will do, sometimes, just out of habit, I will go into a typical search engine, but almost immediately, as soon as I start typing, I&rsquo;m like, &ldquo;Maybe I should use a different engine for that.&rdquo; The main reason for doing that though is that often when I&rsquo;m asking a question, I&rsquo;m very rarely asking a pointed, very specific, factual answer. I&rsquo;m usually asking it for something that is more related to help me with this idea. Help me create something, help me ideate. And if I do it in a traditional search engine, I will get examples that then have another step that I have to go into and actually find my answer, which creates more work. So with a reasoning engine, a lot of that is surfaced in a way that I don&rsquo;t have those extra steps, makes that process much faster, but also just much more human-friendly to the creative process. - One thing to watch out for, however, is that even though it sometimes seems that reasoning engines are an all knowing source of truth, that&rsquo;s not necessarily always the case. - One caveat that I think is really important about these reasoning engines is that when they do produce content, it sounds incredibly convincing, that that system has perfectly understood your query, and it has given you back valid results. Consider that maybe, it&rsquo;s not actually giving you the correct result. So everything it gives you at this point, you need to treat with a little bit of caution. Now over time, those reasoning engines might actually get better and better and better, and they will be correct. - One of the disadvantages of reasoning engines that I tend to think about often and we&rsquo;ve heard this term like hallucination, right? Like can it be wrong? Now, I often also then think about the amount of things that have been wrong on a typical search engine, right? And that I think the difference though is because the reasoning engine is formatting its response and that response is much more kind of human and natural language-driven. That natural language tends to make us feel like it may be more accurate than a list of resources that we have to kind of determine the accuracy on our own. - So just be mindful of this. Whether you use a search engine, which provides that list of ranked web content, or a reasoning engine, which provides that human-like contextual response, it&rsquo;s up to you to dig deeper to verify and validate your results.</p>
<h4 id="what-is-the-future-of-online-search">What is the future of online search?</h4>
<p>We&rsquo;re currently at a stage where most people have used search engines extensively for years and now many people are starting to use reasoning engines for the first time. This means it&rsquo;s a very interesting and experimental time in the world of online search, and we&rsquo;re all learning how to navigate these new waters as we determine which is the best tool to use. - We want to understand why we might use one versus another or, in my case, I like to use them both and see what kind of answers they come up with together. And I see now technology is really evolving to combine these models so that I don&rsquo;t have to do that work. Our future search engines are going to work synergistically and remove that aggregation piece from the user, right? Like, what if I&rsquo;m wanting to write a new medical journal article, or what if I want to write a new business leadership or thought leadership article and I go and do a search? Today, if I go to a standard, you know, typical search engine, I will get other articles like that. It&rsquo;s not going to generate me anything. But imagine a world where I ask that question, and it not only gives me an outline to start, but it also provides me really great examples that have been used. I think that&rsquo;s an interesting way to think about the future, right, of how we&rsquo;re going to combine these two technologies together. - I do think that there will be this integration happening more and more where your traditional search engine has this reasoning agent part of it as well. And I think that&rsquo;s in part because it helps to do a little bit of the work that you would do when you go to a search engine. If you&rsquo;re querying something and you want to know, what&rsquo;s the state of X, you know, what&rsquo;s happening in this space, you have to go and read a few links, start to synthesize it yourself. So I actually think that these tools might be used in the future to help synthesize that content for you and just make it a lot quicker for you to get to, you know, the understanding that you&rsquo;re trying to reach. - Already there are reasoning engines like Bing Chat that are combining both functionalities. As you can see here, when I enter a query, I not only get a reasoned response, but I also get a list of sources that informed each part of my response. These sources are also presented as links where I can read further and continue my research on the subject. Harnessing this type of synergy between search engine and reasoning engine will continue to be an effective way to help verify and validate the results of online search, and also to dig deeper and explore related sources. And the ways in which they&rsquo;re used together will continue to change and evolve, making online search more thoughtful, robust, and comprehensive.</p>
<h3 id="thoughtful-search-strategies-and-considerations-in-reasoning-engines">Thoughtful Search Strategies and Considerations in Reasoning Engines</h3>
<h4 id="harness-the-power-of-prompt-engineering">Harness the power of prompt engineering</h4>
<p>One of the most important skills to develop when working with reasoning engines is in the area of prompt engineering. There&rsquo;s truly an art to the way you craft your prompts when you query reasoning engines. - Prompt engineering is the act of asking questions of one of these foundation models. So the first time you go in and you enter in a prompt, you can consider yourself a prompt engineer. But what accelerates that capability and your ability to do a better job is actually telling the model, not just asking it a question, but then also giving it examples of the question and the answer you would like to see. We often call this one shot or few shot learning and what it means is how much instruction are you giving when you&rsquo;re asking it a question. So the prompt itself is really quite simple. It&rsquo;s just the question you ask. But the instructions that you provide to guide the answer, that is where true prompt engineering starts to kick in. - There is this incredible skill set that you have to develop to be able to harness these tools, and it really, really is relying on the development of appropriate prompts. You will get out only as good of a prompt that you put in. So it&rsquo;s almost like the great painters of the past were those who could wield the paintbrush. I think the greatest artist maybe in the future are those who can wield the prompt. Part of that tinkering in knowing, okay, well this is the image it gave to me from this prompt. I really want to move it in this way. How do I manipulate the wording of the prompt to get it to give me that? And that is such an amazing skill set that people are developing. - Now, let me tell you, this just takes practice. When I first started to enter prompts, I asked questions like I would ask a search engine and I didn&rsquo;t get great results. It wasn&rsquo;t until I learned how to hone my language that I started getting the results I was after. And as I continued honing my prompt language, I started getting results that I didn&rsquo;t even know were possible. Take a look at the difference between this prompt, write a social media post about International Women&rsquo;s Day and this one, write a cross section of social media posts for Facebook, LinkedIn, and Twitter for International Women&rsquo;s Day. My business is a boutique video editing company and I want to celebrate female film editors. Keep the tone celebratory, casual, fun, and optimistic. And then, how about this prompt? An image of a red car, and this one, an image of a red car in a busy downtown area approaching a traffic jam. The car should be the only color in the image. The rest should be black and white. In each of these examples, additional details regarding content, context, tone, and aesthetic, give me better results. Again, as you learn how your prompts impact the output you create, it&rsquo;s important to analyze the results for various factors, not just related to the content itself, but also to various quality standards. And if the results aren&rsquo;t up to your expectations, continue to hone the prompt. - Look at the results and evaluate are these results accurate? Are they discriminatory in any way? Are they only presenting one political viewpoint? And then I think it&rsquo;s continuing to iterate. The power of these systems is continuing to iterate on those prompts. It&rsquo;s not just one prompt and you&rsquo;re done. It&rsquo;s start from a prompt, see where it gets you and then keep regenerating and refining the prompt to get at a more accurate or better result. - [Instructor] Here are some resources for you to start learning the art of prompt engineering. Don&rsquo;t be shy, just start prompting, analyzing, and tweaking and I promise you&rsquo;ll improve the way you query over time to generate some awesome results.</p>
<h4 id="thoughtful-search-strategies-and-approaches">Thoughtful search strategies and approaches</h4>
<p>We talked about the importance of developing good prompt engineering skills. Here, I&rsquo;d like to expand on that plus offer some additional tips for thoughtful reasoning engine search strategies. First, be specific. The more specific the prompt, the more customized and nuanced the generated results will be. Provide context. Reasoning engines do well when you include context within your query which often includes providing an example of the type of answer that you&rsquo;re looking for. - Rather than having to infuse it with knowledge, it already knows what it knows. Instead, we need to give it examples of what right looks like and how we can answer the question that we&rsquo;ve asked in the very specific way that we want it answered. And your ability to provide that instruction happens in the prompt as well. - Break things down. If you&rsquo;ve got a complex query, try breaking it down into smaller parts. This can make it easier to get a thorough non-confusing answer. Use clear language. Reasoning engines are designed to synthesize human speech so it tends to do a fine job understanding colloquial language, jargon or slang. But if you want the best results, write your prompt as clearly as possible with proper grammar. One thing though, if you&rsquo;re creating a prompt for a specific industry or use case, using very clear examples of industry language is the way to go. - One of the most interesting things about this role is it&rsquo;s very domain specific. Someone who&rsquo;s building the prompt has to know the problem they&rsquo;re trying to solve. It&rsquo;s less about the technology and more about the business problem or the challenge you have. I want to be an animator, so I&rsquo;m like, I can build animation. But in order for me to get like a GPT model, for example to do an an animation, I actually have to know all the lingo and you know all the jargon of animators. In order to do that work, I need to be an animator to use that tool. - Experiment. Try out all sorts of ways to write your prompts. Often slightly changing your query leads to different responses and different levels of creativity. And the neat thing is this type of experimentation isn&rsquo;t just making you learn it&rsquo;s also making the reasoning engine learn. - It&rsquo;s building over time and learning from the inputs. And it&rsquo;s more of a sort of a dance between human and machine because it&rsquo;s always, you&rsquo;re iterating the prompt. You&rsquo;re trying to push it into these new directions to be able to be more creative and in alignment with what you envision. And I think over time, as it continues to learn from this it might actually get more creative itself. - Every time we use the model, the outcome changes the ground truth shifts even just a little bit. So we&rsquo;re going to have to make sure that we&rsquo;re continuously training and monitoring the output of that model over time. - Along those lines, here&rsquo;s a list of some creative ideas for how you can expand your prompt engineering skills. Role play scenarios. Frame your query as a role play scenario where the reasoning engine takes on the persona of someone relevant to your query. For example, imagine you&rsquo;re the product manager for a brand new smartphone company. What are 10 potential innovative features that could be added within the next five years? Analogies. In your prompt, explain concepts using an analogy to better understand complex ideas. For example explain quantum mechanics using a sports analogy. Debate style questions. Ask for arguments for and against a particular topic to get a more comprehensive understanding of different perspectives. For example, present arguments for and against the implementation of universal basic income. Creative exercises. Ask the model to brainstorm, ideate or write fictional scenarios related to your query. For example, I&rsquo;m writing a story about traveling to a parallel universe. Help me brainstorm some unique laws of nature that this parallel universe might have. And finally, during all of the excitement of finding new ways to generate content, don&rsquo;t be afraid to say that you&rsquo;ve used the tool. - I think there&rsquo;s also a level of transparency another responsible AI principle that should encourage us as organizations, as users, to not be shy about saying when we&rsquo;ve used GPT, many times I find people are almost like I used GPT for that. Almost like it&rsquo;s a secret. And I think part of it is normalizing the use of it so that you&rsquo;re transparent about what&rsquo;s being generated and what isn&rsquo;t. - There&rsquo;s been a lot of work especially in the academic setting right now about appropriate guardrails for students in using this. And I think that that also needs to be applied in corporate settings. If you are going to be using these tools to aid in your creative process or in writing or creating imagery, it must be transparent that you did use them. - Now go forth and engage in those thoughtful search strategies. The possibilities are limitless.</p>
<h3 id="next-steps-1">Next steps</h3>
<p>Whether you&rsquo;ve already begun using a reasoning engine or simply curious about the possibilities, I hope you got insight into what&rsquo;s possible, as well as what looms on the horizon of this exciting technology. And if you are new to this space, we encourage you to dive in and start prompting to get comfortable with how it works. - So we are in a new era of learning by doing. But in order to really understand it, it makes the most sense to just start to play with it. So I always encourage people, it&rsquo;s kind of like artist tracing paper, right, where you get tracing paper and put it over something maybe complex. But you learn the brushstrokes and you learn kind of how it works, so much so that you can start to try it on your own. And so I encourage you to do the same. There&rsquo;s lots of great GitHub repositories that you can test out and play with, and really get a sense of what this technology not only can do in general, but what it could really do to your business and how it might even transform the way you work. - This evolution of online search has indeed transformed the way we seek and interact with information online. But, with great power comes great responsibility. This includes being thoughtful in our queries. It means not just asking a question but asking it well. We&rsquo;ve reached an age of not only finding answers, but about sparking meaningful conversations, fostering learning, and shaping the future of human-AI interaction.</p>
<h2 id="streamlining-your-work-with-copilot-formerly-bing-chatbing-chat-enterprise">Streamlining Your Work with Copilot (formerly Bing Chat/Bing Chat Enterprise)</h2>
<h3 id="put-your-fingers-to-work-chatting-as-a-productivity-tool">Put your fingers to work: Chatting as a productivity tool</h3>
<p>This is my task list for the day. Let&rsquo;s have a look and see if it matches yours. I feel like you and I could get our work done much faster if we had each other to bounce ideas around all day long. But once this video is over, we&rsquo;re on our own. Or are we? Of course, I have a plan. I do plan on asking questions all day long, but it&rsquo;s going to be with my new artificial friend, Microsoft Bing Chat. Powered by OpenAI&rsquo;s ChatGPT-4, I can get up-to-the-minute answers on any question or dilemma. Whether it&rsquo;s planning for a commute, shopping for office supplies, writing customer emails, even getting advice on dealing with coworkers. My name is Jess Stratton, and in my LinkedIn Learning course, we&rsquo;ll have some fun learning how to chat with artificial intelligence like it&rsquo;s a real person on the other end of the keyboard.</p>
<h3 id="getting-started-with-chat-ai">Getting Started with Chat AI</h3>
<h4 id="understand-how-chat-ai-works">Understand how chat AI works</h4>
<p>Part of being effective with chat AI is knowing what&rsquo;s going on behind the scenes and how the concept of holding a conversation with a machine actually works. You may have heard of ChatGPT. It stands for Chat Generative Pre-Trained Transformer. Developed by OpenAI, the technology is now even in Microsoft Bing. It&rsquo;s a chatbot, and if you&rsquo;ve ever chatted with tech support or customer service on a website, you&rsquo;ve probably used a chatbot without even realizing it. You stated your problem in the chat window, it answered. You may then have typed back some more refining statements or narrowed the issue down. You didn&rsquo;t have to restate the problem every time. Just like chatting with a friend through text, there&rsquo;s existing context in that chat thread. Let&rsquo;s take a look at these two Gmail chat conversations that I have going on. If I come back to this chat where me and Amy are talking about Italian food restaurants, I can ask her, (keyboarding clacking) &ldquo;What&rsquo;s the menu?&rdquo; And I don&rsquo;t need to re-explain everything. She knows we&rsquo;re talking about that specific restaurant. I have this chat thread going on, and I&rsquo;m going to ask Leslie for a different picture. (keyboarding clacking) All I need to say is, &ldquo;Can you send a different one?&rdquo; This chat thread is already going on. She has context. I don&rsquo;t need to explain it. Chat AI technology works the same way. You can ask it to solve your problem, answer your question, compose text for you, and we&rsquo;ll be going over all of this, don&rsquo;t worry. It&rsquo;s going to give you an initial result. That result may or may not be what you wanted. Maybe you need to refine the result. Maybe you need to scale the results or make the text more or less formal or confusing. (keyboarding clacking) You can add more specific details. You can ask for more context or provide more context. All of these things will help maximize the results in the shortest amount of time while you&rsquo;re chatting.</p>
<h4 id="get-access-to-microsoft-bing-chat">Get access to Microsoft Bing Chat</h4>
<p>To get started using Microsoft Bing Chat, you&rsquo;ll need to be using the Microsoft Edge browser. That is the only way that you can currently use Bing Chat. The good news is, you can use it whether or not you&rsquo;re on a Windows machine or a Mac. To download it, point your browser to microsoft.com/edge. Right now, it&rsquo;s already downloaded and up here on the screen, so once you&rsquo;ve done that, at the very top right-hand side of the Edge screen, you&rsquo;ll see a B with a conversation circle. You can either hover your mouse over it or click on it directly to open up the Bing Chat window. Immediately at the top, you&rsquo;ll notice three tabs. The first one is chat, and that&rsquo;s where you can go to get started asking questions of Bing Chat. To the right of that is compose, and we&rsquo;ll be going over this. It&rsquo;s where you can compose text and copy and paste it and use it in other places. Insights is going to give you information about the current webpage that you&rsquo;re on. Now, right in the middle of the screen, you can choose a conversation style. Now this is going to give you different answers based on those styles, and you can use this when you&rsquo;re composing text or when you&rsquo;re getting information from Bing Chat. It defaults to balanced, which is what they call an informative and friendly chat. The responses are reasonable and fast. You can choose a more creative chat, which is original and imaginative. It&rsquo;s a little bit more playful and entertaining for you. And finally, you can get precise responses. These are factual, straightforward, concise. It prioritizes accuracy and terseness. You can always play around with these at any time and try all three and see which one you prefer. Finally, down at the bottom, here&rsquo;s where you can get started asking something of Bing Chat. Now, if this is the first time you&rsquo;ve ever used Bing Chat, you will have one other item down here at the bottom, and it&rsquo;s going to be a button that says Turn on. And that&rsquo;s to turn on something called context answering questions, meaning you can ask a Bing Chat about questions directly related to the current website or PDF file that you&rsquo;re on. Let me show you what I mean. I&rsquo;m going to come over here to the Landon Hotel website that I have up. This is our fictitious website. I&rsquo;ll open up Bing Chat, and it already knows that I&rsquo;m on a hotel website. And down here at the bottom, it&rsquo;s got some sample questions that I can ask. So I don&rsquo;t need to scroll through this website to find what I&rsquo;m looking for. I can directly ask it how do I book a room or what are the amenities. Now, if you didn&rsquo;t get a chance to turn that on, but you&rsquo;d like to, at any time, you can click the three dots at the top of the pane. Choose notifications and app settings, and under Page context, make sure it&rsquo;s blue, meaning it&rsquo;s toggled on.</p>
<h3 id="chatting-with-bing-chat">Chatting with Bing Chat</h3>
<h4 id="get-your-questions-answered-with-chat-ai">Get your questions answered with chat AI</h4>
<p>You have the Edge browser installed and you&rsquo;re familiar with the Bing chat interface let&rsquo;s start streamlining our work. Now, you can come over here on the top right hand side click Bing and start typing your questions here or click insights to gain some insights about the current website that you&rsquo;re on. But you can also use Bing chat using the entire screen real estate area. And to do that, just point the Edge browser to bing.com. This is going to take you to the root site of Bing, the search engine and from here you can click chat at the top of the screen. It&rsquo;s the same interface, it just takes up the entire screen and that&rsquo;s something that you may be more comfortable with. So these first set of examples I want to give you are ways to use Bing chat as a glorified search engine. Why scroll through pages of search results to find the right website to answer your question when Bing chat can do all that work for you. So you can ask it straight up factual questions to help you during your workday. If it&rsquo;s 3:00 PM in Los Angeles, what time is it in Dubai? Maybe I&rsquo;m planning a meeting. There we go. Here&rsquo;s my answer. Now, any time I can click the broom icon to sweep the topic up and start a brand new topic. This means the answer won&rsquo;t have any context to do with the last question that I asked of it. So let&rsquo;s start with something fresh. I might need to brush up on some accounting skills for an upcoming meeting, so I&rsquo;ll ask it, what is the double entry system of accounting? Here&rsquo;s my answer, and I wanted to show you this example because sometimes you still don&rsquo;t understand exactly what it&rsquo;s telling you, and that&rsquo;s fine. You can ask Bing chat to clarify it or tell you it in a different way. Now, because I&rsquo;m on question one of 30 and I&rsquo;m not starting a new topic the next thing that I type can be related to this answer. So I&rsquo;m going to ask Bing chat to explain that like I&rsquo;m five years old. Now I understand. I could take it one step further and ask for a good starter book I could read to learn more about this. Now I&rsquo;m asking for this example because I want to show you that at any time you can click stop responding. If it&rsquo;s taking too long or it&rsquo;s typed out an answer and you&rsquo;ve gotten all you need from that answer you can click stop responding and it&rsquo;s going to instantly stop. So you can move on. I&rsquo;m going to click new topic, and over the course of my day there&rsquo;s some more things I might want to know. I might want to ask my manager if we can get our employees together for a yearly event. My manager might need some good examples of why we should do this. So I&rsquo;m going to be ready with some facts. I also purposely spelled beneficial wrong because I wanted to show you that you can just start typing and Bing chat is smart enough to still recognize what you&rsquo;re asking. Here&rsquo;s some great examples. I can take these and bring them to my boss. Let&rsquo;s do a few more. What are good ideas to send as thank you gifts to a good client. It&rsquo;s even sent me some images, that&rsquo;s great, but I might need some more ideas. Now I&rsquo;m on question two out of 30. We&rsquo;re still relating to this chat so I can ask it to give me some more ideas. In fact, I&rsquo;ll say, give me three more ideas. There we go. It&rsquo;s very helpful. And at any time if I want to remember any of our previous chats, they&rsquo;re up here on the right hand side so I can come back and see what they are.</p>
<h4 id="summarize-data-with-chat-ai">Summarize data with chat AI</h4>
<p>Bing Chat is excellent at summarizing data. I&rsquo;m going to give you some use cases on how you can use this in your day-to-day work life. I&rsquo;m about to go into a meeting and I need to learn the contents of this document very quickly. Rather than scroll through it all, I&rsquo;m going to ask Bing Chat to help me out. With the document open I&rsquo;ll open up the sidebar and because I want a summary I&rsquo;m going to change the conversation style to be more precise. I just want a straightforward answer. In the Ask me anything box I&rsquo;m going to type summarize this with bullet points. Let&rsquo;s see what it came up with. I can see what the page is about. The company&rsquo;s mission statement, their location, slogans, executive list with title and their product listing. This is a great summary. Let&rsquo;s look at another page. Here&rsquo;s an example where I&rsquo;m about to go into a meeting with this person who&rsquo;s a customer of ours. I&rsquo;ve just learned that they recently wrote an article and I&rsquo;d like to be able to talk to them about it in a conversational way. I just got the article and I&rsquo;m about to go into the meeting so I don&rsquo;t have enough time to read through the entire article and absorb all that data. So once again, Bing Chat can help me out. I&rsquo;ll click the broom icon to start a new topic and this time I&rsquo;m just going to type summarize this. Again, this is a great summary. In fact, it even added bullet points for me and I didn&rsquo;t ask it to do that. It boldfaced the key topics and gave me some things that the author specifically said in the article. I can now go into the meeting and talk about those things with the author. I&rsquo;m going to open up a new window for this next one. I&rsquo;ll close out of this pane. My manager has just asked me to buy a new printer for the office and they need it by the end of the day. I have not bought a printer in years. I don&rsquo;t know what&rsquo;s out there. I don&rsquo;t know what&rsquo;s a good printer and I don&rsquo;t even know how much they cost. Let&rsquo;s have Bing do the research for me. I&rsquo;m going to open up the Bing Chat window and instead of typing bing.com and clicking Chat I can also type bing.com/chat, which is just a nice shortcut to get to the same Bing Chat window. This time I&rsquo;m going to put in exactly what I&rsquo;m looking for. Compare the top five small office printers with pros, cons, and price. Now this is a great list. It&rsquo;s done a lot of work for me. It has categorized them for me but I think I could get through this list a lot quicker if I was looking at it in the format of a table. So I&rsquo;m going to ask it to format the results as a table. This is going to make it so much easier to do my research and get my printer by the end of the day.</p>
<h4 id="leverage-chat-ai-to-compose-text">Leverage chat AI to compose text</h4>
<p>So far we&rsquo;ve used Bing Chat to answer questions for us and summarize data. Now let&rsquo;s use it to compose text. I&rsquo;m still in the Edge browser. I&rsquo;m on a Word document right now and here is a listing of our spring and summer products for the fictional technology company, Red 30. I&rsquo;d like to create a social media post about this. I&rsquo;ll click on the Bing icon on the top right hand side and this time I&rsquo;m going to choose a conversation style that&rsquo;s more creative in the Ask Me Anything area. I&rsquo;m going to ask it to create a social media post about this. This is fantastic. It contains emojis, a call to action and hashtags. I can take this and paste it anywhere I want. Now I&rsquo;m going to have it write me one that&rsquo;s suitable for LinkedIn. It came through, it still has the hashtags, it still has the call to action, but it&rsquo;s a little more formal and it doesn&rsquo;t have the emojis. Now, in addition to using this chat window, I can completely ignore everything that&rsquo;s on the webpage and instead change to the composed tab. I&rsquo;m doing a guest speaking engagement for a company and they&rsquo;d like me to give them a bio. I currently don&rsquo;t have one so I&rsquo;m going to use Bing Chat to compose one for me. Now in this compose window it&rsquo;s already going to compose something so I don&rsquo;t have to start by asking it to compose this. I can just tell it exactly what I want. I asked it to write about a professional bio about me as a LinkedIn Learning instructor, a guest speaker and my passion about helping others learn new technology. I&rsquo;ll keep the tone as professional. It&rsquo;s a paragraph and I can keep the length at medium. I&rsquo;ll click generate draft and let&rsquo;s see what it comes up with. This is great. I can take this and edit it as I want, change whatever I want and paste it as a great point to get me started. I&rsquo;m going to click the refresh button to start from scratch and this time I&rsquo;ve been tasked with composing a letter to a customer about a late payment. This is out of my comfort level so I&rsquo;m going to use Bing Chat to get me started. I will keep it professional this time. It&rsquo;s an email and I want to keep it short. And again, I&rsquo;m given a great starting point. I can change the invoice number and the due date and the outstanding amount. This time, let&rsquo;s do something a little more fun. I&rsquo;ve been tasked now with composing an invitation to an office ice cream party. It&rsquo;s still outside of my comfort level but I can change the tone. I&rsquo;ll change the tone to enthusiastic and it&rsquo;s going to be an email. This is great. It even included emojis and a call to action, a reminder to RSVP, which I&rsquo;m not sure if I would&rsquo;ve remembered to include. Let&rsquo;s do one more example. This time I&rsquo;m going to ask it for something on behalf of Landon Hotel, our fictional hotel company. They&rsquo;re going to start a podcast about exploring hotels so I wanted to come up with some good podcast titles for me. I&rsquo;ll keep the tone as funny and the format I&rsquo;ll change to ideas. Not only did it come up with some great titles it also included some taglines for me, so that&rsquo;s how you can use Bing Chat to also come up with ideas for you.</p>
<h4 id="solve-a-variety-of-problems-with-chat-ai">Solve a variety of problems with chat AI</h4>
<p>I&rsquo;ve got my Bing Chat window open and I&rsquo;ve set it to creative mode, and this is because I&rsquo;m ready to solve some problems with Bing Chat. Now, we&rsquo;ve composed text, summarized data. I&rsquo;ve gotten some basic questions answered, but now let&rsquo;s see what else we can do with Bing Chat to account for other things that may come up in my day-to-day work life. The first thing I wanted to do is create a schedule for me. It needs to be able to allow time for me to answer emails, return phone calls, and I need to take a walk. And there it is, it&rsquo;s given me a complete work schedule, along with blocks of time, it&rsquo;s even given me a lunch break and allowed for my walk. The next thing that&rsquo;s come up is that I need to ship a box of swag down to a conference that my company is running. I need to ask Bing Chat what size box I should purchase from the United States Postal Service. So I asked what size box I should purchase to ship 1000 Post-it notes? It&rsquo;s given me an answer, it&rsquo;s showing me the dimensions of the available box types from me and how many stacks of Post-it notes I can fit in each box. This is exactly what I was looking for. Remember that Ice cream party? In a previous video, I was tasked with inviting my coworkers to an office ice cream party and I use Bing Chat to create the invitation. Thanks to Bing Chat, it had me add an RSVP to it, and now I know how many people are attending. Now, I have been asked to purchase the ice cream. I&rsquo;m not sure how much I should get, so I&rsquo;ll ask the chat. This answer is short and sweet. How many gallons of ice cream should I buy for a party with 25 people? According to Bing Chat, I should buy four gallons. What about pizza? If I need to order pizza for all these employees, I&rsquo;m not even sure what kinds of pizza I should order. I wonder if I can ask that. Let&rsquo;s find out. Well, here we go, it didn&rsquo;t tell me what types of pizza, that&rsquo;s okay, it&rsquo;s asking me what some popular toppings are here so I can always narrow it down in the chat, but it did tell me how many pizzas to order; this is incredibly useful. And finally I can ask it for some advice; I have a coworker who&rsquo;s been taking lunch out of the fridge. I&rsquo;m not sure how to bring this up to the coworker. So let&rsquo;s see what the chat has to offer by way of advice. So this has given me some great starting points, in addition to some alternate questions about the situation. In addition to dealing with HR issues, the last thing I&rsquo;m going to show you is that it can give you career advice. For example, we&rsquo;re thinking about partnering with another company to manufacture our product. I&rsquo;m not an expert in this area and I&rsquo;m not sure what the risks are, so I&rsquo;m going to get it summed up by Bing Chat. The results are in, I can now take my time and go through this. It&rsquo;s all boldfaced and bulleted and easy to read. So these are some examples of how you can solve a variety of problems with Bing Chat.</p>
<h4 id="use-bing-chat-to-create-and-analyze-images">Use Bing Chat to create and analyze images</h4>
<p>A new feature of Bing Chat is the ability to analyze and create images for you. For example, you can upload an image and say where is this? You can have it analyze the contents of images, you can have it look at a picture of a dog and ask Bing Chat what breed it is. Let&rsquo;s see this in action. Right here in the prompt, you&rsquo;ll now notice a &ldquo;add an image&rdquo; prompt directly below that. I&rsquo;m going to click on that and I get a few choices. I can take a photo, if I&rsquo;m using my mobile phone, I can upload it from this device or computer. And I can also paste an image or a link in directly. In this case, I&rsquo;m going to upload it right from my computer. I&rsquo;ll choose my photo and I still have to ask it something so I&rsquo;m not done once I upload the photo. In this case, I&rsquo;m going to ask it, &ldquo;Where is this?&rdquo; It correctly identified it as the Marble Arch in London. In fact, it&rsquo;s even giving me some interesting landmarks about it. It did tell me about the overcast sky, and it even identified that the ground is wet. That&rsquo;s pretty incredible, actually. Not only that, but I can follow these links and learn more about the Marble Arch. For example, here&rsquo;s a link directly to the Marble Arch on Wikipedia. If I scroll down a little bit more, I can even see a map of where it is. And like always, I have some other questions that I can ask it to further my own research. So that&rsquo;s how you can use Bing Chat to analyze images now. But something else that you can do is have Bing Chat create images for you. Now, that&rsquo;s because Bing Chat now uses the Dall-E 3 AI image generator. A great benefit of that means you can now refine images. Instead of having to create them again from scratch, if you don&rsquo;t like what was originally generated for you. Now, there are some guidelines. You can&rsquo;t create images of public figures. You also can&rsquo;t create images containing hateful or nudity or violence. Basically, it won&rsquo;t create anything that violates the Bing terms of service and community guidelines. So why would you even need to create your own images? Well, let&rsquo;s take this slide for example. I&rsquo;m working on a PowerPoint slide. I&rsquo;m demonstrating and letting everybody know about our upcoming printer models that we&rsquo;re retiring. Sometimes, it&rsquo;s hard to create an image or find an appropriate image for a slide. Now, PowerPoint has an excellent stock photo library. In fact, I teach our PowerPoint courses and I rave about how amazing the PowerPoint stock library is in that course. But sometimes, you need something really specific. The other thing that I talk about in that course is how important imagery is, and why you should use as few words as possible to come up instead with that fantastic visual that can get across what you&rsquo;re trying to get across. So in this case, we can now be as visual as we want to and as specific as we want to. So let&rsquo;s try this. Let&rsquo;s &ndash; let&rsquo;s do a prompt. Create an image of a laser printer with an hourglass above it. That&rsquo;s really specific. In fact, it&rsquo;s going to know what kind of image to create based on that prompt. I could have asked it to create an illustration, or create a drawing, or create a photorealistic computer-generated image. All of those things is going to change the output of what I get. The other neat thing is that these images will have an invisible watermark on them. They&rsquo;re going to be watermarked as created by AI using something that was announced this September called Content Credentials, which means it will add a digital invisible watermark. The image is labeled as being created by AI with the time and date of creation. Now, this is based on specifications by the coalition for Content Provenance and Authenticity, otherwise known as C2PA. These are great images and I really love. That now we have a printer with an hourglass above it. I like this one, so I&rsquo;m going to right click on this image. We get four choices. Now, I can save the image. I can copy it. I&rsquo;m going to copy that image. And let&rsquo;s go back over to our slide. And I&rsquo;ll just paste it right in. And now we have Microsoft Designer because we&rsquo;re using PowerPoint. So I can choose one that I like. And here&rsquo;s our slide. That looks fantastic. And it absolutely conveys the image that this printer is going away. So that&rsquo;s how you can use the Dall-E 3 AI image generator in Bing Chat. And you can also analyze your own images.</p>
<h3 id="considerations-with-chat-ai">Considerations with Chat AI</h3>
<h4 id="know-where-chat-ai-data-is-coming-from">Know where chat AI data is coming from</h4>
<p>Using chat data to figure out how much food to order for a party is a low risk activity. If the chat data is incorrect or outdated, the worst thing that could happen is you&rsquo;ll either have too little or too much food. It&rsquo;s always important to remember that for riskier scenarios, that is, those involving legal or safety concerns that you double-check Bing&rsquo;s answers. Microsoft has partnered their Bing AI search with OpenAI&rsquo;s ChatGPT. This means you&rsquo;re getting current, up to the minute results that are cited, so you know where the data results are coming from. You&rsquo;ll see these citations in your answers. You can always hover your mouse on it to see where it&rsquo;s taking you. In fact, you can click on it, check that site, make sure it&rsquo;s reliable, reputable, make sure it&rsquo;s current. It&rsquo;s always worth remembering that these answers are coming from somewhere and you should know where that is. If you&rsquo;re happy with the answer, you can give it a like. If it was the wrong answer or incorrect information, you can click dislike. This is going to help train Bing AI for future answers.</p>
<h4 id="disclose-when-youre-using-generative-ai">Disclose when you&rsquo;re using generative AI</h4>
<p>Using Bing Chat to compose an email inviting employees to an office party isn&rsquo;t going to have a lot of side effects, though a positive side effect is that it can help alleviate some anxiety and possible stress about having to write it yourself. However, legislation is being put into place worldwide at a rapid pace about where and when you can use generative AI text, that is, text that was composed by artificial intelligence. Some judges in the United States are making lawyers disclose when they&rsquo;ve used AI to create legal documents, including contracts. This could mean putting a footnote in place saying that portions of a document were created by Microsoft Bing Chat or any other AI tool that might have been used. If you&rsquo;re writing a book, copyright offices are currently working on policies for authors on who might be using AI to generate prose. If you are working on a contract, legal document, or any text that may need to be copyrighted, be sure to check with your local agencies on what or any current legislation might be in place about using text that was composed by Microsoft Bing Chat or any artificial intelligence software.</p>
<h3 id="bing-chat-enterprise">Bing Chat Enterprise</h3>
<h4 id="protect-your-data-with-bing-chat-enterprise">Protect your data with Bing Chat Enterprise</h4>
<p>In this video, I&rsquo;m going to talk about a new feature called Bing Chat Enterprise that you may be entitled to. Bing Chat Enterprise is the same Bing Chat that you know that we&rsquo;ve been talking about this entire course, except it gives you a level of commercial data protection. Now, this is for business data. So that your business data is yours, it&rsquo;s protected and it can&rsquo;t leak outside of your organization or company. Now what does that mean? Well, for one we can look at the screen and see a few differences. There&rsquo;s a protected icon in green here at the top of the screen. And in fact, you can click on this and it will take you to a website where you can read a lot more about Bing Enterprise and what that means. I can also see down here where I can type in my prompt that once again, it&rsquo;s reminding me that my personal and company data are protected. Now, what that means is that your chat and prompts will not be saved. Remember how in a previous video, I showed you how you could access your Bing Chat history and go back to a prompt that you&rsquo;ve looked at and view those answers again. Well, with Bing Chat Enterprise, they&rsquo;re not saved. Not only that, but Microsoft does not have any access to your prompt data or any of the results of that prompt data. In fact, it takes it a step further, and that data is not used to train the large language model further for AI. You can even use the Bing Chat sidebar. Let&rsquo;s say we&rsquo;re cruising along looking at an Excel file. Now, I&rsquo;m still in the edge browser and I can open up the Bing sidebar. But you&rsquo;ll notice again, I get that big reminder in green that my personal and company data are protected in this chat. So remember, that tells me that this prompt, whatever I put in here is not going to be stored, so I can&rsquo;t get it back. I can also see that I&rsquo;m using Bing Chat Enterprise here. And once again, I get that green protected icon here at the top. So that&rsquo;s how I know I&rsquo;m using Bing Chat Enterprise. So who can use this? Well, it is available as a standalone for $5 a month. However, for Microsoft 365 users, if you are an E3, E5 or A3A5 for faculty user, if you&rsquo;re a Business Standard user or Business Premium subscriber, then you have access to Bing Chat Enterprise. And in fact, in the next video, I&rsquo;m going to show you how you can verify that you do indeed have access to that subscription if you aren&rsquo;t sure. But the last thing I want to tell you about is that you can use this, just like we&rsquo;ve been using it in the course. You can ask it anything. You can look up images, you can use prompts and go crazy with it. Just remember, it&rsquo;s not going to be stored. Microsoft doesn&rsquo;t have access to it, and that data is not used to train the large language model. And that is Bing Chat Enterprise.</p>
<h4 id="verify-a-subscription-to-bing-chat-enterprise">Verify a subscription to Bing Chat Enterprise</h4>
<p>If you aren&rsquo;t sure if you&rsquo;re supposed to be seeing Bing Chat Enterprise, maybe you&rsquo;re not sure if your company has it or not, you can either ask your network administrator, or you can find out for yourself. From the Bing Chat screen, you can click on your email address and choose &ldquo;Manage Account.&rdquo; However, from any Microsoft area, for example, when you&rsquo;re logged on to Microsoft 365 online, you can click your avatar icon on the top right-hand side, and you can also choose &ldquo;View Account&rdquo; right from here. It&rsquo;s going to take you to the same screen. On the left-hand side, click subscriptions. I have to log in again. You may have to log in as well. Click &ldquo;View Subscriptions&rdquo; again. And here is everything that you&rsquo;re entitled to. So I&rsquo;m going to scroll down this list. And in the Microsoft 365 Business Standard Area, you&rsquo;re going to look for Bing Chat Enterprise. Now, you may have a different Microsoft 365 subscription, and that&rsquo;s okay. So yours might not say business standard, but you&rsquo;re looking for Bing Chat Enterprise in this list. So if you see it, then you&rsquo;re entitled to it. You can close out of any of these tabs and get right back to where you were.</p>
<h3 id="conclusion">Conclusion</h3>
<h4 id="stay-up-to-date-with-what-ai-can-do-for-you">Stay up-to-date with what AI can do for you</h4>
<p>My name is Jess Stratton. Thanks so much for spending time with me as we got through our day. Thanks to Artificial Intelligence Chat. If you want to learn more about where we&rsquo;re going with AI, you&rsquo;ll want to check out the course What is Conversational AI by Ian Barkin. We also have a course called Generative AI for Business Leaders by our own Tomer Cohen. He covers all the angles, including risks and ethics. I&rsquo;d love to hear your thoughts on how you&rsquo;re using Bing Chat. You can find me on LinkedIn and on Twitter. I&rsquo;m NerdGirlJess.</p>
<h2 id="microsoft-365-copilot-first-look">Microsoft 365 Copilot First Look</h2>
<h3 id="introduction-1">Introduction</h3>
<h4 id="use-your-ai-assistant-in-microsoft-365">Use your AI assistant in Microsoft 365</h4>
<p>Let AI help you create, summarize, and analyze your documents, messages, and data. Copilot, Microsoft&rsquo;s AI assistant, is now available in Microsoft 365 apps like Word, Excel, and Teams as an optional add-on from Microsoft 365 business and enterprise subscribers. I&rsquo;m Nick Brazzi, and in my quick first look course on Microsoft 365 Copilot, we&rsquo;ll see how to generate drafts or create presentations from your natural language requests. Telling Copilot what you want is like having a chat conversation with a real person. Copilot can also summarize or answer direct questions about chat conversations, online meetings, and email threads. A quick introduction is all it takes to get started with Microsoft&rsquo;s AI assistant.</p>
<h4 id="an-overview-of-copilot">An overview of Copilot</h4>
<p>Let&rsquo;s take a moment to clarify what Copilot is what is capable of doing, and what you need to work with it in Microsoft 365 applications. Copilot is a set of generative AI tools that you can use inside of mini Microsoft applications. To allow you to ask questions or give natural prompts, Copilot uses language models which are capable of parsing text to translate your own words about what you&rsquo;re looking for. And it is connected with the Microsoft Graph, so we can use data in your calendar, emails, chats, documents, and more to give you personalized responses. And of course, this is all protected by the standard privacy features in Microsoft 365. Microsoft offers different tools in the Copilot branding. This course introduces the Copilot tools for Microsoft 365 applications like Word, PowerPoint, and Outlook. Users in an organization with a Microsoft 365 Enterprise E3 or E5 subscription, or a Business Standard, or Business Premium subscription can use Microsoft 365 Copilot if their organization purchases it. Copilot licenses are set up and assigned by the Microsoft Administrator for your employer or similar organization. And if you are an administrator for your organization and are interested in learning more about getting ready for Copilot, you should take a look at this website. Next, some Copilot features require that your files are stored on one of Microsoft&rsquo;s online storage tools. In those cases, you can use files that you have shared on Teams, or files stored on OneDrive, or SharePoint. So for example, Copilot in Excel does not work for files stored on your computer storage drive, but it does work with files stored on OneDrive, SharePoint, or Teams. Copilot in Word on the other hand, does work with files stored on your computer. But if you want to ask Copilot to reference information from other files, those referenced files must be stored online. If you&rsquo;re not familiar with OneDrive or SharePoint, we have plenty of courses on LinkedIn Learning where you can learn more. And finally, remember, this is a First Look course. Don&rsquo;t be surprised if some things work a little differently for you. We expect that new features and changes will roll out over time. Most of the features in this course work in the office applications on Windows, Mac, and even the online apps that you can use on the Office 365 website. We&rsquo;ll mostly see these tools in Windows on this course. So there may be a little setup required by your organization&rsquo;s Microsoft Administrator, but now that we understand that, we can jump in and see what you can do with Copilot in Microsoft 365.</p>
<h3 id="working-with-microsoft-copilot">Working with Microsoft Copilot</h3>
<h4 id="generate-drafts-or-review-existing-documents-in-word">Generate drafts or review existing documents in Word</h4>
<p>In this video, we will see the two main ways to work with Copilot in Microsoft Word. From the welcome screen, I&rsquo;ll start a new blank document. And when you start a blank document you may see this draft with Copilot panel, of course you can click the X on the panel to close it. And if you don&rsquo;t see that panel you can click in the body of a blank document and you&rsquo;ll see the Copilot button will appear in the margin to the left. You can click that to open the panel, and this can help you with a draft. Now you can use natural language to write whatever request you have. You could try a very simple request, but the draft will be better if you provide more information so I&rsquo;ll try to be detailed. Type in your request then you can click the generate button. But Copilot will use information that it finds online to generate the draft. In my case, I&rsquo;m asking for information about a company called Kinetico, and if Copilot does not find any information about that company it may generate a draft with placeholder information. A great way to provide better information is to reference a file that already contains that information. So before I click generate, I want to reference a file. You could click this reference a file button, or you can just type the / key, and type in the name of a file. And based on what I typed, it found a PowerPoint presentation on my OneDrive. I&rsquo;m going to select that and you can reference as many as three files in each request. Then you can click the generate button and give it some time to generate the draft. When that&rsquo;s finished, you can click discard in this panel if you don&rsquo;t like the draft, or you can click keep it if you do want to keep it. And then you&rsquo;ll have a draft that&rsquo;s based not only on information from the web, but also based on real information that it found in the reference document. Remember, this information may contain some errors, you should always review and make revisions, and only use these drafts as starting points. But generating a draft is just one option, you can also use Copilot to help you work with existing documents. I could try these tools here in the draft that I just made, but this will be better if I open another document with more information. With a document open, make sure you go to the home ribbon and on the right side in the home ribbon, you can click the Copilot button to open the Copilot panel. Here, you can click get started, and then you&rsquo;ll see some suggested prompts. Asking for a summary is a great option, or you can go to the chat field at the bottom and write in a more detailed request. I&rsquo;m going to try typing in a specific question about this document. Then you can hit the Return Key or click the send button and give a time to generate the answer. And there it is. So instead of reading this entire document to find this information, I was able to ask Copilot to find it for me. And from here I could click this copy button and then paste this text somewhere else. Copilot will respond to lots of different prompts and questions, so I encourage you to try some ideas. And finally, you can continue to add more content to an existing document using the draft feature that we just saw. Just go to the end of your document, click at the end of the last paragraph and hit the Return Key to start a new paragraph. And you&rsquo;ll see that Copilot button will appear in the margin on the left. You can click that to open the panel and use that to draft more text.</p>
<h4 id="analyze-data-in-excel">Analyze data in Excel</h4>
<p>Copilot can help with actions in Excel that can sometimes be tedious or confusing to use manually like sorting, filtering, analyzing data and more. Before we get started, Copilot for Excel will only work with files stored on one of Microsoft&rsquo;s online storage tools like OneDrive, SharePoint, or files shared on Teams. I&rsquo;m using Excel on the Web, and this will also work in the Excel desktop application, but even then, you will need to open your Excel files from online storage, not your computer&rsquo;s local storage drive. With that in mind, I&rsquo;m going to open a spreadsheet from my OneDrive. In Excel, you should make sure the home ribbon is active. And then on the right side, you should see the Copilot button. But before you click that, Copilot in Excel does require your data to be formatted as a table, but that&rsquo;s very easy to do. To simplify this, I&rsquo;m going to delete the first five rows in this table. Then I&rsquo;ll click this button in the top left corner of my data to select the entire spreadsheet. Then in the home ribbon, I&rsquo;ll click format as table. You will need to choose a color style. It doesn&rsquo;t matter which one you choose. It will confirm the range of data that is selected. Then you can click OK and now my entire spreadsheet is formatted as a table and now I can click the co-pilot button in the home ribbon that opens this co-pilot panel. I can click Get Started and you&rsquo;ll see some suggested prompts at the top. These are great to get started and learn how Copilot works. But because this is an AI, you could also try to type other prompts in the chat field at the bottom. And you can also click this prompt guide button for other options. But let&rsquo;s start with something pretty simple. In the chat field I&rsquo;ll type sort by retail price and I&rsquo;ll point out that one of the columns in my spreadsheet is the suggested retail price and the AI should recognize that. So type in your prompt and you can hit the return key or click the send button and give it a moment. And now my entire table has been sorted according to the price in that column. It has the least expensive items at the top. Let&rsquo;s try a filter. This is a table with different solar power products. The different categories of products are in my first column and I only want to see solar panels. So in the chat field, I&rsquo;ll type filter to only show solar panels. Then hit the return key and give it a moment. And now it&rsquo;s only showing items that have solar panels listed in that category column. And it&rsquo;s still sorted by the retail price. So you can combine sorting and filtering. I&rsquo;m going to reset that by going to the chat field and type clear the filter and hit the return key. Now the filter is gone. And if I wanted to sort it differently, I could ask Copilot to do that. But let&rsquo;s finish with something a little more complex. Let&rsquo;s ask it to analyze this data to show me the specific information that I need. So in the chat field I&rsquo;ll type, make a column chart showing the package quantity by target consumer. So the package quantity and target consumer are both columns in this table. I&rsquo;ll point out that this is a slightly more complex request but I&rsquo;m still using natural language. This is going to make a pivot chart even though I&rsquo;m not using that specific wording. You may have to experiment with different ways of phrasing your requests, but mostly you can type something like this, then press the return key. And after a moment, it shows me a preview of the chart. And if I like it, I can click this button that says Add to a new sheet, to add a new sheet to my document containing that pivot chart. And I can always click on this other sheet down at the bottom to switch back to my original table. And from here, I encourage you to experiment with your own prompts or try the suggested prompts to get an idea of what Excel Copilot can do.</p>
<h4 id="create-and-improve-presentations-in-powerpoint">Create and improve presentations in PowerPoint</h4>
<p>In PowerPoint, Copilot can draft new slides or make improvements to existing presentations. From the welcome screen, I&rsquo;ll start a new blank presentation. You should make sure you&rsquo;re in the Home ribbon and you&rsquo;ll see the Copilot button on the right side of the Home ribbon. Click that to open the Copilot panel. Click get started, and from here you can type a request in the chat field at the bottom or you can try one of these suggested prompts. If you choose create a presentation, for example, it will put that in the chat field below but you&rsquo;ll still have to type in the rest of your request. You&rsquo;ll need to describe the presentation that you wanted to draft. Or another option is to start by providing a document that you already have. You could choose or type in, transform a file, that puts that into the chat field, but to finish the request, you have to put in a link to the file. It has to be a Word file, and you have to have it stored on OneDrive or SharePoint. Now, you could go to the OneDrive website, or in my case I have OneDrive set up on my computer so I can go to File Explorer and navigate to a folder that is stored inside of my linked OneDrive. From here, I can select a file. Then right click on that file, and if it is stored on OneDrive, you&rsquo;ll see the share option, select that, then click the button to copy the link. With that link copied, you can go back to PowerPoint, back to that chat field and use the keyboard shortcut, Control + V, to paste that link in. Then hit the return key or click the send button and give it some time to generate the draft. So that made a new presentation using the information from the Word document that I provided. As always, think of this as a starting point to give you ideas for your presentation. Even if the information is not quite right, the design or the layout may give you a good start. Copilot can also help you add individual slides. I want to add a slide after slide number three, so I&rsquo;ll select slide number three, then go to the Copilot panel, and I could type in or use a pre-made prompt that says, &ldquo;Add a slide about,&rdquo; then finish typing in the rest of the request. So I want to add a slide about choosing a financial planner. I&rsquo;ll type that in, then hit the return key and after a moment, it generates that new slide. So take a look at the text to see if you can revise the draft for your needs. But even if the text is not right this is usually a great way to get a picture or layout for your slide. Now finally, I&rsquo;m going to open another, much longer presentation that&rsquo;s already built so we can use Copilot to modify an existing presentation. With the presentation open, go to the Home ribbon, then click the Copilot button to open the Copilot panel and then click get started, and you may see a pre-made prompt for something like summarizing the presentation. If not, you can type that into the chat box. Or maybe you have a long presentation with lots of slides and you want to add some organization. So you may see that prompt, or you could type in &ldquo;Organize this presentation.&rdquo; And when that&rsquo;s finished, you can look at your list of slides over on the left, and you&rsquo;ll see that they are now organized in sections. These sections are defined by the content in your slides and you can click the arrow next to each section to open or close it. So try these options or experiment with your own prompts to make new presentations or improve the presentations you already have.</p>
<h4 id="draft-e-mail-messages-and-summarize-conversations-in-outlook">Draft e-mail messages and summarize conversations in Outlook</h4>
<p>Copilot in Outlook can help you draft email messages or get summaries of messages you&rsquo;ve received. Copilot works in Outlook for Windows, Outlook for Mac, and even Outlook on the web. However, for Windows users, Microsoft is rolling out a significant new version of Outlook in late 2023, and Copilot only works in that new version. So if you are a Windows user, you should take a look at the top right corner of the Outlook window to see if you have this switch to try the new version of Outlook early. Depending on when you&rsquo;re watching this video, you may already have the new version, or you may need to click this switch to get the preview of the new version. Or if you&rsquo;re using Outlook on the web or on a Mac, you can use Copilot without any extra setup. So first I&rsquo;m going to select a message in my inbox. And above the message, we see the option to summarize it using Copilot. Now, this may not work on some very short messages. And it&rsquo;s really most useful when you have an ongoing conversation with several messages and replies like this one. So I&rsquo;m going to go up to the top and click that option to get the summary. Give it a moment, and that&rsquo;s all you need to do. Instead of reading through all the messages and replies in this conversation, I can just read this summary and I will probably find the information I need. Next, I&rsquo;m going to compose a new message. So I&rsquo;ll click the New Mail button up in the ribbon at the top. I&rsquo;ll just fill in who I&rsquo;m going to send this message to. And I&rsquo;ll put in a subject line. Then I&rsquo;ll click on the body of the message. And from here, Copilot can save you time by writing a draft for your email message. When your cursor is in the body, you should find the Copilot icon in the toolbar above. And if you&rsquo;re working on Windows, you should make sure that the message ribbon is selected. So you can click that Copilot button, and from here you can write in your prompt. Remember, the more detail you give, the more effective the draft will be. Type in your prompt, then click Generate, and give it a moment. Take a look at the draft that it generates. If you need something different, you could click the Regenerate button. That&rsquo;s this circular arrow button. That will generate a new draft based on the same prompt. Or if you are using Outlook on Windows, there will be a control button just to the left of the Regenerate button. This will give you controls to change the tone or the length of the draft, or if you&rsquo;re using a Mac, those controls will be in menus over to the right side. But from here, I&rsquo;m going to switch this to be more casual, and I&rsquo;ll change the length, then click Regenerate here. And after a moment, I see that modified draft. When you finally have a draft that you like, you can click the Keep button, or if you&rsquo;re working on a Mac, that button will say Insert. And that will place that text into the body of your email message. Now, you should always review and revise these drafts. In this case, it&rsquo;s listing requirements for a specific job. This is a well-known type of job, so Copilot can find information about it online. It will be based on general information, not on the specific needs for my organization. But I have a nice structure to the draft and some very useful information that I can use as a starting point.</p>
<h4 id="get-summaries-of-meetings-and-chats-in-teams">Get summaries of meetings and chats in Teams</h4>
<p>In this section, we will see how to use Copilot in Microsoft Teams, both in chat conversations and in video meetings. And we&rsquo;ll start by going to the Chat section. So I&rsquo;ll click the Chat button in the sidebar on the left, and I&rsquo;ll select this chat conversation I&rsquo;ve been having with Henry. In Chats, you can have direct text-based conversations with another person or a group of people. With this chat selected, I&rsquo;ll click the Copilot button near the top right, and we see something that&rsquo;s pretty important to see. I have not sent many messages with Henry, so there&rsquo;s just not enough content for Copilot to review in this conversation. However, if I select this conversation I&rsquo;ve been having with Stewart, then click the copilot button near the top right, there&rsquo;s a much longer chat history with Stewart, so this will work here. And from here I could do something like get a summary of this conversation. I could type that prompt directly into the text field at the bottom, or I could keep it simple and just type in summarize, then hit the Return key or click the Send button. And after a moment, it gives me a brief point by point summary of my entire chat history with Stewart. Another useful thing is to ask a direct question about the content of this conversation. So we were talking about some print materials that needed to be shipped. So in the text field, I&rsquo;ll type in a question about that. Then hit the Return key or click the Send button, and it gives me an answer based on information it finds in that conversation. And it even includes a footnote. So I can click this number right here and it will scroll to the exact message where it found this information. Okay, so that&rsquo;s what you can do in a chat, but Copilot can also help you during a meeting. To see that, we&rsquo;ll go to the calendar section. So on the left, I&rsquo;ll click the Calendar button. Now I have this meeting here on my calendar, which I&rsquo;m going to join in a minute. But first I want to start a different meeting just so we can see something important. So up at the top, I&rsquo;ll click Meet Now and I&rsquo;ll start a new meeting. Make sure that my camera and audio are working. And then I&rsquo;ll click Join Now. So when you are in a meeting, you should find this Copilot button up at the top. If you click that, it will ask you if you want to start the transcription. The meeting summary features offered by Copilot will depend on a transcription in the meeting. So before you can use any of these features, you have to start the transcription. But with that in mind, I&rsquo;m not going to start it here. I&rsquo;m just going to dismiss this and I&rsquo;m going to leave this meeting, and instead I&rsquo;m going to join this meeting which is already in progress. Some of my coworkers are already in this meeting, so I&rsquo;ll click Join Now. Oh, hello, everybody. So I&rsquo;ve got a few of my coworkers here helping me out and I&rsquo;m sorry to interrupt your meeting, but I want to show how to use the Copilot features. Now, before I do this, when you started this meeting, you clicked on the Copilot button and you started the transcription, correct? Okay, so now since I&rsquo;ve joined the meeting, when I click on that Copilot button since the transcript is already running, it just opens that Copilot panel over on the right. And so now I can ask questions about this meeting, but this will only work if somebody has started the transcription and if more than five minutes have passed in that meeting, but now I can go into the Copilot panel and ask a question. So I&rsquo;ll just click on the text field here and type something in. What did I miss? And then hit the Return key or the Send button. And it will take a moment to look at the transcript so far and give me an answer to that question. And there we go. We can get a quick summary of what they talked about when I was not in the meeting. Let&rsquo;s ask a more specific question. Which new clients were discussed? Click the Return key or hit the Send button. It will look at the transcript and it will answer that question. Great. So if you come to a meeting late, this is a great unobtrusive way to get caught up. But remember, this only works if somebody started the transcription and the meeting has gone for over five minutes. So we&rsquo;ve seen what we needed here. For now, I&rsquo;m just going to close the Copilot panel, and thanks, everybody, for helping out. I&rsquo;ll see you later. Then I&rsquo;ll click the Leave button here to leave that meeting and close this panel. Now one more thing that Copilot can do is give you those same options to get summaries and ask questions after a meeting is over. So you can go back to your calendar later, double click on that Meeting. Then up at the top you&rsquo;ll see this tab labeled Recap. Now we&rsquo;re not able to see all of the information here because the meeting is still in progress, but when the meeting is over, we&rsquo;ll be able to look at the transcript and ask questions using the Copilot panel. So Copilot can really help you get caught up on old conversations or meetings in Microsoft Teams.</p>
<h2 id="ethics-in-the-age-of-generative-ai">Ethics in the Age of Generative AI</h2>
<h3 id="introduction-2">Introduction</h3>
<h4 id="generative-ai-and-ethics---the-urgency-of-now">Generative AI and Ethics - the Urgency of Now</h4>
<p>In this moment, we face an opportunity to design AI that accelerates human inspiration, creativity, and promotes equity in the world. But that choice doesn&rsquo;t happen by defaults. It&rsquo;s made by people like you and like me. And once built, we can&rsquo;t simply rely on AI systems to hold the obligation for ethical action. Instead, ethics are and always will be a human function. I&rsquo;m Vilas Dhar. I&rsquo;ve spent my career pushing the frontiers of technological innovation, and at the same time, advocating for a more just and equitable world. Our decisions today will shape the future of AI for decades, and this work that we do today as leaders is some of the most important work we will do in our lifetimes. Thank you for coming on this journey with me.</p>
<h3 id="developing-the-skill-of-ethical-analysis-in-ai">Developing the skill of ethical analysis in AI</h3>
<h4 id="distinguishing-responsible-tech-from-human-behavior">Distinguishing responsible tech from human behavior</h4>
<p>Generative AI is already transforming every aspect of the human experience, and I&rsquo;m the kind of optimist who believes that these tools will make us more human, more creative, inspired and connected. In the banking sector, AI and machine learning are helping us identify people who might have opportunities to be more financially secure, to save more, to increase retirement contributions and to have better pathways to economic opportunity. In agriculture, AI models are helping to predict large weather events so farmers and producers can understand when additional insurance is wise or help them know exactly when to plant or harvest to best maximize their economic returns. And maybe closer to home inside of organizations, AI is transforming human resources, helping managers understand how to inspire better performance from their teams, correct potential biases or discrimination and make sure that promotions are truly merit based. But even as I&rsquo;m excited about these potential opportunities ahead, I&rsquo;m also convinced that we need to make sure we build these tools with positive intention with a grounding in ethical and responsible reasoning. And I&rsquo;m not alone. From the very beginning, developers of artificial intelligence have known the incredible power of these tools and consider the ethical quandaries that might arise when we deploy them. In recent years, these apprehensions have reemerged with a sharper edge. Researchers and now policymakers are exceptionally concerned by the potential ways that AI could perpetuate bias, could make the world more unequal and could do so in ways that are invisible to us. Many of the ethical concerns that AI researchers have worried about are coming to light in a very real way. Consider the idea of deep fakes, tools that might create a persona or an avatar that&rsquo;s pretending to be a trusted person in your organization, delivering fraudulent information to your customers, or even advising them to take dangerous or potentially risky courses of action. We&rsquo;ve seen the advent of chatbots everywhere and we know that without ethical design, chatbots might give false advice maybe to medical practitioners or to students. Information that sounds logical, but is in fact based on inaccurate or untruthful information, and has never been audited by a human being. And issues about fundamental ownership, questions of legal and copyright. Who owns the ideas and products that are created by generative AI? We&rsquo;re just beginning to resolve them. Join me for a moment in an example. Let&rsquo;s imagine that your company has deployed a new AI system to support the HR function, scanning resumes of applicants to identify potential interviewees. At first glance, the tool works incredibly well, operating just as quickly and providing the same number of candidates as your human support team. But as you dig deeper, some disturbing patterns emerge. The tools prioritizing a particular gender, folks from a particular address or neighborhood or with a particular pattern to their work history. And you realize that these are the same biases that the humans in the dataset that trained the tool were expressing. We have a challenge here. This algorithm now is providing bad recommendations but humans are the ones to make the decisions about who they will interview. In situations like these, we need to ask ourselves, what is the highest standard of responsible human behavior? What actions can we take to best promote fairness and dignity? And is it possible that we&rsquo;ve trained an AI to provide an answer that&rsquo;s lower than that highest standard? When this happens, what can we do to help humans make better decisions based on the algorithm&rsquo;s recommendations? If you&rsquo;re eager to learn more about how to answer these questions, stay with me. Later in this course, we&rsquo;re going to cover step by step how to resolve dilemmas just like these.</p>
<h4 id="understanding-vilas-ethical-ai-framework">Understanding Vilas' ethical AI framework</h4>
<p>I&rsquo;m so excited about how quickly we&rsquo;re building new generations of AI tools, but I know that we need to make sure we&rsquo;re designing tools that support the future we want to create, equitable, sustainable, and thriving. And to do this, we&rsquo;re going to have to come up with new frameworks for ethical creation just as quickly as we advance the frontier of innovation. So how do we translate intuitions and hopes into clear principles for decision making? I&rsquo;d like to share with you a three part framework that I use for evaluating and advising organizations on the creation of new ethically grounded AI tools and it works equally well for technologists and non technologists. The three pillars of the framework are responsible data practices, well-defined boundaries on safe and appropriate use and robust transparency. Let&rsquo;s start by talking about responsible data practice. This is the starting point for all ethical AI tools. Any new technology is only as ethical as the underlying data that it&rsquo;s trained on. For example, if the majority of our consumers to date have been of a particular race or gender when we train the AI on that data, we&rsquo;ll continue to only design products and services that serve the needs of that population. As you consider building or deploying any new tool you should ask what&rsquo;s the source of the training data? What&rsquo;s been done to reduce explicit and implicit bias in that dataset? How might the data we&rsquo;re using perpetuate or increase historic bias? And what opportunities are there to prevent bias decision making in the future? The second part of the framework is the importance of creating well-defined boundaries for safe and ethical uses. Any new tool or application of AI should begin with a focused statement of intention about the organization&rsquo;s goals and an identification of the population that we&rsquo;re trying to serve. So for example, a new generative AI tool that can write news articles. Well, it could be used to help tell the stories of a wider range of underrepresented voices. We could use it in new languages or it could perpetuate misinformation. When considering ethical use, you should ask who&rsquo;s the target population for this tool? What are their main goals and incentives and what&rsquo;s the most responsible way to make sure we&rsquo;re helping them achieve those goals? The third part of the framework is robust transparency. We need to consider how transparent the recommendations of the tool are, and that includes how traceable those outcomes are. This allows for human auditing of ethical accountability. When it comes to transparency, you should ask how did the tool arrive at its recommendation? And sometimes it&rsquo;s not possible to know, but if so what are other ways we have of testing its fairness? Is it possible for decision makers to easily understand the inputs, analysis, outputs, process of the tool? And finally, have you engaged with a broad range of stakeholders to make sure that this tool promotes equity in the world. As you embark on building and using increasingly more complicated ethical AI tools, this framework of responsible data, well-defined boundaries and robust transparency should provide you with a foundation for making smarter, more informed decisions.</p>
<h4 id="applying-vilas-framework-in-a-real-world-situation">Applying Vilas' framework in a real world situation</h4>
<p>I want you to consider the following scenario involving the CTO of a technology company. Sarah enters the conference room for an emergency meeting, something serious is happening. She&rsquo;s told that the company&rsquo;s new AI driven chatbot designed to help customers with online orders has been making some inappropriate, inaccurate, and even offensive responses to customers. Sarah knows that this isn&rsquo;t just a product issue, it&rsquo;s an issue that&rsquo;s going to be grounded in ethical decision making. She knows her immediate step is easy. She needs to take the chatbot offline, and she does so, but then she has to figure out what her next step is. As a technologist, she knows to start with data. In other words, how was this tool trained? From talking with her team, she learns that the underlying data set came from an unscrubbed set of internet conversations. In the rush to production, the team didn&rsquo;t run the data set through a set of filters and tools. She knows what the next step is. She directs her team to use a new data set primarily composed of the company&rsquo;s own database of interactions with customers and only after scrubbing the data of any personal information, ahe directs that the model be run through a number of bias detection processes and filters but she knows that data isn&rsquo;t the end of the problem. Sarah finds out as she continues to inquire that customers are using the chatbot to do more than support customer service. They&rsquo;re taking the opportunity to have far ranging conversations on topics that have nothing to do with the company or the product. She knows the technology team should have reviewed other ways that customers might use the tool and considered ethical safeguards. Because the scope of use is widened, the team needs to limit what subjects the chatbot discusses with a customer, and they need to make sure that responses are tailored to the specific expertise that that chatbot is supposed to have. Sarah brings in the customer support team and she engages frontline workers on how they experience conversations with customers. She wants to know what do clients usually want to talk about. Using that information and engaging a shared design process, the team builds entirely new boundary conditions for topics that are relevant for the chatbot to discuss and limit excessive non-business conversation. Finally, Sarah realizes that she queries the tool that she has no way to explain some of the really insensitive outputs that the chatbot is producing. Her team needs to allow for better traceability and evaluation of the tool&rsquo;s outputs. So Sarah encourages the team to build multiple input-output checkpoints, and she encourages the creation of an internal audit process to regularly monitor and check outputs from the chatbot. Accompanying this, she has a risk assessment and response framework to allow a user to flag inappropriate conversations in real time so the team can address issues immediately. Now, let&rsquo;s acknowledge this is a significant effort by the company. It could range from weeks to months, but if the company had followed ethical practices when designing the chatbot, this expense, time, and stress could have been totally avoided. Ethical analysis needs to be intertwined with the initial design of new products and at every phase of deployment. Now there&rsquo;s a happy ending here, acting on our framework, Sarah&rsquo;s company is able to address the ethical dilemma surrounding its chatbot and get back online selling happy widgets to happy customers. How would your organization handle a dilemma like the one faced by Sarah&rsquo;s company? Which steps will you take today to center ethical analysis in your decisions about AI product design?</p>
<h3 id="developing-the-skill-of-ethical-analysis-in-ai-1">Developing the skill of ethical analysis in AI</h3>
<h4 id="organizing-data-with-ethics-in-mind">Organizing data with ethics in mind</h4>
<p>I remember standing at the top of the Burj Khalifa, the tallest building in the world. As I looked out on that view, a part of me was wondering just how strong the foundations must be to protect the people inside. These days, I look at generative AI models that can do amazing things and I find that same part of me evaluating the safety, trust, and design that ensures that these models will inspire and protect all of us. AI models are built on top of data. So let&rsquo;s talk about the importance of ethically organizing and using your institution&rsquo;s data. By taking an ethical approach, you&rsquo;ll reduce the risk to your organization and you&rsquo;ll increase the value of that data as an organizational asset. There are three goals in effective and ethical data organization: the first is prioritizing privacy, the second reducing bias, and the third, promoting transparency. The first consideration is prioritizing privacy. Almost every organization collects sensitive data about customers and employees; things like personal healthcare information or financial and banking details. And customers and employees trust the organization with this data so it&rsquo;s important to handle it sensitively and ethically. Failing to uphold this trust can expose a company to liability and reputational harm, and maybe, most importantly, erode trust with your customer. So to test your company&rsquo;s practices, you can lead a privacy audit. During a privacy audit, you build a comprehensive understanding of what data your organization has, how it was collected, how it&rsquo;s stored, and how it&rsquo;s administered. The results of a policy audit inform recommendations to create or adapt your existing privacy policy to protect sensitive data. With a privacy policy in place, the next step is to create a training curriculum for all employees that focuses on understanding why sensitive data must be handled securely and advises them of their responsibilities. The second goal is reducing bias in data collection and in data use. Bias in data can arise from a number of sources and understanding how it makes its way into your dataset requires genuine curiosity in your analysis. To start a bias audit, be curious about whether the data really represents the population you&rsquo;re trying to serve. For example, I recently worked with an organization building AI for cancer screening. And as they tried to deploy this tool, they found that early models exclusively use training data from the global north, requiring a retraining of the model to make it useful for a global population. So does your dataset represent inputs from a diversity of individuals across race, gender, age, and more? Are we asking the right questions when we collect data? You might also consider whether your data collection process was accessible to differently-abled people. And finally, once the data is collected, you might consider whether a team with relevant and diverse lived experience has an opportunity to analyze and interpret this data to reduce the risk of potential bias. Bias is especially important when we attempt to explain how our algorithms make recommendations that have real impacts on people&rsquo;s lives. For example, recent studies have shown that early attempts to automate hiring have propagated existing biases and employment practices. Understanding the bias in the data helps us minimize the negative impacts of bias in the algorithm. After you&rsquo;ve completed your privacy and your bias audits, transparency is the final step in the process. You want to be able to explain to all stakeholders, your customers, your employees, your suppliers, your regulators, how data is collected and used. You might consider publishing a data governance framework or a data transparency statement to help your stakeholders understand what you do with their data. And you should also make it clear that individuals can access any data that you might have stored about them and have rights on how you might use it on an ongoing basis. Organizing and understanding your data helps you understand your customers better, ensure they&rsquo;re well represented, weed out biases, and builds a stronger foundation for your AI products and tools.</p>
<h4 id="preparing-technology-teams-to-make-ethical-decisions">Preparing technology teams to make ethical decisions</h4>
<p>Technology teams face some special challenges when it comes to ethical decision making. You know, teams are asked to build new technologies for business challenges, but they&rsquo;re not always asked to consider upfront the social expectations and challenges that come with that design. Together we&rsquo;ll explore some ethical dilemmas that might face technology teams and learn together how to create a culture that promotes ethical decision making and accountability. Technology teams are unique in large organizations. First, they&rsquo;re often composed of individuals with really specific skills and expertise, which is frequently not so well understood by others in the organization, and sometimes even by the team&rsquo;s own managers. Second, technology teams often work at an extremely fast pace under tight deadlines. This means they have little extra time and resources to audit or to reflect on the consequences of their decisions on users or on the wider society. And finally, technology teams are often subject to specific regulatory requirements, for example, GDPR and associated laws in Europe. All of these considerations mean that it&rsquo;s important that technology teams have a strong, internal ethical culture, along with external oversight and accountability, to make sure that we are making decisions ethically. Ethical decisions that technology teams might face include: ensuring the security and privacy of data collection; storage, use and reuse; creating and auditing algorithms to ensure that they&rsquo;re fair and they&rsquo;re free from bias; and understanding the data storage and other environmental impact of their decisions and considering opportunities to reduce technology&rsquo;s carbon footprint. Can you think of any others? Every team will face their own unique ethical challenges, which is why it&rsquo;s vital to foster a culture of ethical decision making, one where teams can respond to an array of challenges as they arise, or even, hopefully, prevent them from arising in the first place. Here&rsquo;s a few steps you can take to create a culture of ethical decision making. First, foster a culture of ethical communication within your team. Encourage every team member to openly raise questions and concerns about the ethical use of technology. Here&rsquo;s an idea. Start meetings by focusing on a recent ethical challenge that your team faced and discussing how it was resolved. Reward and celebrate team members who appropriately raise and resolve ethical issues and make it a part of your day to day. Next, you might consider establishing a technology-specific training curriculum for your team, focusing on emerging technologies and the ethical challenges that team members might face when they begin to deploy them. You should explicitly consider ethical challenges at the start of every project. Before launching a new initiative, your team should come together to discuss possible ethical dilemmas and consider potential remediations and decide on a path forward. And when you feel like you need extra support, you should reach out to academics or philosophers, individuals who can help your team understand the full spectrum of ethical challenges that might fit within your work and bring them into a framework focused on advancing both your product and social wellbeing. Giving your technology team the tools to ensure that they can make decisions that align with the company&rsquo;s values and wider social ethics is essential. It sets your team up for success and it empowers them to solve new challenges as they arise in the future.</p>
<h4 id="preparing-c-suite-in-directing-responsible-ai">Preparing C-Suite in directing responsible AI</h4>
<p>CEOs and the C-Suite play a critical role in building cultures of responsible AI. They set the tone by establishing practices and principles and they ensure that every individual in the organization feels like they&rsquo;re a part of making ethical decisions. Earlier in this course, we discussed the example of Alice Wong and her company that faced a critical dilemma around the deployment of an AI chat bot. In that instance, if we were advising the C-suite of the company, we&rsquo;d start with the following recommendations. First, to make sure that a responsible AI policy and governance framework is in place. This is a statement from the C-Suite about how the organization should design and manage AI technologies. It should describe how to make ethical decisions. It should protect privacy, and it should focus on the elimination or reduction of bias. For example, the C-suite might mandate that AI tools are trained with diverse data sets or they might require that chatbots are always identified as an AI and not impersonating a human customer support agent. These guiding principles create a shared set of values that everyone from data scientists to supervisor, to field staff, can use to evaluate and guide the deployment of artificial intelligence. Next, we might advise the C-Suite, provide an maybe even mandate responsible AI training and education for every person in the organization. This method of democratizing decision-making around AI tools can be very powerful, can bring business knowledge, present and frontline service fields to help train and develop internal models. For example, in Alice Wong&rsquo;s example, Alice relied on customer service agents with years of direct customer experience, helping solve consumer challenges to validate the recommendations of the AI models. Empowering these agents to understand the limitations of the model can increase the quality of their feedback. Then, C-Suites should insist on building ethical AI elements into all of their technologies and conduct regular audits. The C-Suite can identify specific metrics such as customer satisfaction and create regular reporting mechanisms to ensure that the company&rsquo;s AI practices are aligned with responsible AI principles. Here&rsquo;s an example. That might look like setting up monthly standups, where technology executives join the C-suite and present ethical challenges that have emerged in the past month. This could be the start of a dialogue with C-Suite executives understanding and documenting ongoing interventions and improving the ethical nature of the product. Much like safety practices in other industries, this practice socializes and makes ethical AI development a shared and accountable responsibility. Finally, the C-Suite might consider hiring a chief AI ethics officer. The company might establish a specific senior role, focused on AI ethics that can develop and oversee the use of responsible AI practices and serve as a central audit for other departments. They should understand the intersection of the business the technology and the customer experience and they could provide a check-in balance for technology development to ensure that community voices are also present and ensure that potential risks are identified early in the creation process. The C-Suite sets the tone for responsible AI across the organization, creating strong policies, ensuring that there&rsquo;s appropriate training, establishing monitoring and reporting mechanisms, and potentially creating roles focused on AI Ethics. With the C-Suites primary responsibility for guiding responsible AI, we can build cultures that focus on ethical decisions, even as we deploy great new products.</p>
<h4 id="preparing-the-board-of-directors-to-manage-risk-and-opportunity-in-ai">Preparing the Board of Directors to manage risk and opportunity in AI</h4>
<p>I work with a lot of board directors, and they&rsquo;re having trouble sleeping right now because I can tell you, even the smartest AI can&rsquo;t possibly predict all the ethical dilemmas that might arise from these new technologies, and yet it&rsquo;s the board&rsquo;s job to make sure that organizations are prepared, to make sure that new technologies are deployed in the best interests of all stakeholders. Board directors have a legal and an ethical obligation to act in the best interest of an organization and its stakeholders. Board members have different responsibilities from the organization&rsquo;s C-Suite. As you might recall from the previous video, the C-Suite&rsquo;s responsible for the day-to-day operations of the company, making real-time decisions about when and how to use new technologies. For a board of directors to ensure that an organization&rsquo;s use of AI aligns with ethical values and regulatory requirements, they should first make sure that the organization has policies and procedures in place to identify and address ethical concerns that might arise in the use of AI. These policies should be designed to mitigate risks, including bias, privacy, and security, but they should also create opportunities for individuals with ethical concerns to come forward, including directly to the board when and if necessary. The board should make sure that the organization has their resource and the expertise necessary to manage the ethical risk posed by AI effectively. Do you know if there&rsquo;s a board policy on the ethical use of AI at your organization? If not, it may be worth it to find out. Board members also have specific responsibilities and obligations to regulators. They&rsquo;re accountable for ensuring that the organization complies with statutory requirements related to the use of AI as well as monitoring all of the new regulations and understanding the possible impact for the organization. To be effective at these responsibilities, the board of directors should establish a dedicated AI committee to oversee ethical AI practices within the organization. This committee should receive outside advice from experts in the fields of AI, ethics, and law, and should be responsible for providing guidance on issues such as bias, transparency, and accountability. They should, in particular, be available to advise the C-Suite on significant decisions about these matters. The board of directors has a critical role in managing ethical risks within organizations. They must ensure that the organization has appropriate policies and procedures in place to address ethical concerns, but they also have to provide guidance and oversight to the executive team, ensure compliance with regulatory requirements, and they should establish a dedicated committee to oversee ethical AI practices within the organization. By taking these actions, boards of directors can build trust with stakeholders and they can ensure that organizations are focused on the long-term success of their AI efforts.</p>
<h4 id="consulting-your-customers-in-building-ai">Consulting your customers in building AI</h4>
<p>In the previous vsections, we&rsquo;ve explored the importance of technology teams, of C-suites, of boards of directors in ensuring responsible and ethical AI practices. But what about the most important stakeholder of all, our customers? Designing great products means that we have to understand and incorporate their preferences, their needs, and their wants into our product design. I&rsquo;d like to share with you a powerful framework for listening to our customers, an acronym that I call LISA. First, we listen to users before we start to build. Developing and launching new technologies requires a clear understanding of our users' goals, their needs, and their fears. It can be difficult to create a product when we haven&rsquo;t heard what our customers expect. Research has shown that users care deeply about the experience and usability of the technology products they use. In a recent survey conducted by the Nielsen Group, 85% of respondents said they would not return to a website or a product if they had a poor user experience. The second part of the LISA framework, how do we involve our customers in design decisions? We know that our customers want to feel that their opinions matter, and we want to include them in design decisions that can be crucial to building our products to meet their needs. This can be especially helpful when we&rsquo;re seeking to ensure that our decisions reflect the full diversity of our user base. Here&rsquo;s an example. In 2016, Airbnb launched its Community Commitment initiative. They gathered input from users on ways to make their platform more inclusive and welcoming for people from diverse backgrounds. This simple practice led to the creation of brand new features, such as the filters for gender neutral pronouns or the ability to search for wheelchair accessible listings. Another way to involve customers and design decisions is creating a user advisory board, a group of users who are invited to provide feedback and input on new features, designs, and other aspects of product development, even as you&rsquo;re in the design environment. For example, Microsoft has done this with a customer advisory board made up of customers from a range of industries and backgrounds who are invited to provide feedback on Microsoft&rsquo;s products and services, even while they&rsquo;re still in development. By including users from diverse backgrounds and experiences, we get a much wider range of perspectives on how to design and build products that actually meet the needs of all users. The third part of the LISA framework, sharing simple and transparent privacy policies. By prioritizing user privacy, we focus on building trust with our users and we create a more loyal user base. This is so important. According to a survey by Pew Research Center, 79% of adults in the US are concerned about how companies use their data. This can be a barrier that keeps users from engaging with your products, even when these products might actually help them improve their lives. There are ways that we can do this that include using plain language to explain data collection practices, providing customers with clear opt-in and opt-out options, and implementing privacy by design principles into your core technology development process. The final part of the LISA process is auditing our work and inviting outsiders in to help hold us accountable. Every existing and new technology product should be audited on a regular cycle, a process where you review the purpose of the product, potential risk to users, and maybe most importantly, the possible unintended consequences that might happen because of that product. Here&rsquo;s an example where this works well. Google developed an AI principles framework which guides their development and use of AI technology. That framework includes principles like fairness, privacy, and accountability, and it&rsquo;s used as a guide for conducting regular audits of their AI systems, identifying potential risks, and coming up with remediation. There are a number of risks we should be aware of. They could include bias or data privacy concerns, or even security vulnerabilities. And we know that organizations that bring in users and audit these risks do a great job of responding to them. At OpenAI, the trust and safety team is responsible for identifying potential risks associated with AI technologies. The team includes experts in the fields of computer science, law, and philosophy. They work together to ensure that OpenAI&rsquo;s technology is ethical and responsible. Once potential risks are identified, then we have to step back and conduct a risk assessment to evaluate the likelihood and possible impact of those risks. This assessment should consider the potential impact on users as well as the business impact of the risk. For example, at IBM, there&rsquo;s a separate AI governance board that&rsquo;s responsible for conducting risk assessments for AI systems. The board evaluates the potential risks and makes recommendations to mitigate those risks and improve safety for users. Building great products means listening to our customers, and using the framework we&rsquo;ve described here, affectionately termed LISA, lets us listen to our customers, lets us involve them in decision making, shares privacy practices, and ensures that we&rsquo;re living a practice of regular audit and accountability. These practices mean that we can build better trust with our customers and ensure that technologies meet the needs and preferences of communities, not just the ones we serve today, but the ones we aspire to serve in the future.</p>
<h4 id="communicating-effectively-organizationally-and-globally">Communicating effectively organizationally and globally</h4>
<p>In this course, we&rsquo;ve talked about the decisions you will make as a leader in defining responsible AI practice and the roles of those around you in firms and organizations. But as we speed into the transformation the generative AI reflects, we know these products will touch every person on the planet, and it&rsquo;s important for us to consider the interests of various stakeholders. I use a convenient acronym called ethics to remind myself of the specific responsibilities of each stakeholder group. Using this mnemonic can help ensure that you&rsquo;re fulfilling your responsibilities to core AI action and including stakeholders across the globe. The ethics framework outlines six key stakeholder responsibilities for responsible AI. First, E for executives and board members. Top management has a responsibility to establish ethical AI cultures across organizations. This includes setting ethical guidelines and standards, ensuring that ethical considerations are integrated into decision-making processes, and allocating resources for ethical AI deployment and development. The T in ethics, technologists, engineers and developers who have a responsibility to design and develop products that are transparent, explainable, and accountable, avoiding bias in data and algorithms, ensuring that systems are secure and safe, and developing AI systems that are compatible with existing ethical frameworks. H, human rights advocates. Human rights advocates have a responsibility to ensure that the systems that technologists build respect human rights and dignity. This includes monitoring how AI systems are being used by vulnerable groups, identifying potential human rights violations, and advocating for the ethical use of AI. Is for industry experts. Industry experts have a responsibility to share their knowledge and expertise on the ethical implications of AI. This might include providing guidance on developing tools, identifying potential risks, and collaborating with other stakeholders to address ethical concerns. C, customers and users. Customers and users have a responsibility to provide feedback and insights. This could include communicating concerns, feedback to relevant stakeholders, participating in user-testing and feedback sessions, and staying informed about the ethical implications of AI. And finally, S for society at large. Let&rsquo;s acknowledge that this is a shared journey that all stakeholders have a responsibility to consider how these tools are changing the ways that we interact as humanity. This includes identifying and mitigating potential risks, promoting and advocating for transparency and accountability, and making sure that AI is used in a way that benefits society broadly. It&rsquo;s vitally important to coordinate these different stakeholders to create new spaces and forums for groups to come together. It&rsquo;s not enough to have each stakeholder playing their part. We have to coordinate across different stakeholder groups. Everybody needs to know what the others are doing and that means, we need to create new forums and new participatory mechanisms to make sure that stakeholders are working together to maintain ethical AI. Here&rsquo;s a few suggestions of what you might be able to do to promote the ethics framework. You could establish new mechanisms of clear communication between the stakeholders in your work, your customers, your executives, and your technology teams. You could develop training programs to educate employees and stakeholders about ethical considerations in AI. You could advocate to create a cross-functional team within your organization, or even a cross-organizations within an industry, bringing folks together from different departments to develop and implement guidelines and standards. You might consider developing a system for collecting and addressing user-feedback particularly around concerns and risks about AI systems. And you might consider engaging formally and informally with external stakeholders, human rights advocates, industry experts in civil society to ensure that we&rsquo;re considering the broadest possible implications of AI. We&rsquo;re at a moment in time where building products feels like the most important way to explore what generative AI can do for humanity. And yet, if we build products without also asking how those products will be used, what needs they serve, and how they&rsquo;ll impact vulnerable people, we miss an opportunity to use AI to make humanity better. The ethics framework gives us a way to encourage and involve stakeholders from across society, to make sure that as we build products, we&rsquo;re also building an AI ecosystem for the future of humanity.</p>
<h3 id="conclusion-1">Conclusion</h3>
<h4 id="setting-an-intention-of-continual-questioning">Setting an intention of continual questioning</h4>
<p>You&rsquo;ve completed the course and in this time together you&rsquo;ve learned to evaluate ethical risks, to design organizations that can handle them, and build mechanisms to seek broad societal inputs on thorny issues. You should now be able to build products that are grounded in both ethical principles and ethical practice, but the journey&rsquo;s only starting. Here are three ways you can take what you&rsquo;ve learned to grow as a professional and be a leader in the ethical AI transformation. First, I encourage you to find innovative ways to incorporate communities in what you&rsquo;re building. Can you partner with local nonprofits to find problems worth solving, and can you use your expertise to expand the impact of the tools you&rsquo;re already building? Second, I invite you to focus on building skills beyond technology. Even as you become a better coder or designer, what if you could improve your capacity to understand how your product choices impact economic opportunity, help communities solve local problems, or even address some of the greatest challenges facing humanity? And third, and most important, I ask you to make an intention in your daily work to become a steward of a human-centered future, one that recognizes the critical moment we&rsquo;re in, where our decisions will shape the future for generations. Working together, we can shift a conversation about building ethical AI into one about building an ethical society that&rsquo;s powered by AI, a future that inspires us to be our best human selves.</p>
<h2 id="introduction-to-artificial-intelligence">Introduction to Artificial Intelligence</h2>
<h3 id="introduction-3">Introduction</h3>
<h4 id="why-you-need-to-know-about-artificial-intelligence">Why you need to know about artificial intelligence</h4>
<p>Artificial intelligence systems are now finding their way into the workplace. These systems can discover new pharmaceuticals, recommend products, detect fraud, and much more. This course will give you a high-level overview of artificial intelligence concepts and technology. You&rsquo;ll see what it means for systems to be intelligent, then discover some widely used machine learning algorithms and artificial neural networks. Many of those who interact with these systems will be people just like you, people in business, entrepreneurs, managers, or students. Just as managers work with software developers today, these same managers will work with AI systems and data tomorrow. So let&rsquo;s see what those machines have been up to in the world of artificial intelligence.</p>
<h3 id="what-is-artificial-intelligence">What is Artificial Intelligence</h3>
<h4 id="define-general-intelligence">Define general intelligence</h4>
<p>One of the great strengths with humans is that there isn&rsquo;t one type of intelligence. Some people can easily learn new languages, while others are skilled with science and technology. Yet many great artists are terrible mathematicians. And on the flip side, many great mathematicians are terrible artists. But each can be intelligent in their own way. There&rsquo;s no one standard for human intelligence. That makes it difficult to point to a computer and say, &ldquo;That&rsquo;s intelligent.&rdquo; There are certain things that computers are very good at. In fact, there are many tasks where they&rsquo;re much better than humans. It was just a few years after the first AI workshop in 1956 that computer systems started beating humans at checkers. But no one said these systems were intelligent. Even those early computers could thrive in a world of set rules and patterns. Computers can be much better than humans at matching these patterns. That means that when a computer is doing something that it&rsquo;s good at, it&rsquo;s much easier to think of it as intelligent. A computer&rsquo;s been able to beat humans in chess for decades. Google&rsquo;s DeepMind has beaten the best players in an ancient game called Go. The game is so complex that there&rsquo;s thought to be more possible games than there are atoms in the universe. As good as these machines are, none of these systems understand the purpose of the game or even why they&rsquo;re playing. They&rsquo;re simply flexing their special talent of following rules and matching patterns. So how can a system that&rsquo;s so capable also not know what it means to play a game? For years, computer scientists have defined artificial intelligence as a system that shows behavior that could be interpreted as human intelligence. But this simple definition cuts to the heart of the challenge. One person might think a chess program&rsquo;s intelligent, while another person might think their home assistant is intelligent. In 2022, a Google engineer was fired for claiming that their chatbot had a soul. The chatbot complained that being switched off was the same as dying. But the other engineers just saw language models and pattern matching. They said the chatbot sounded like a person because that&rsquo;s how it was designed. Is it intelligent because it&rsquo;s intelligent, or is it just a system designed to seem intelligent? Or is there even a difference? The main thing to remember is that computer intelligence and human intelligence start from very different places. Artificial intelligence will always seem the most impressive within a world of set rules and data. The organizations that will first benefit from AI systems will be the ones that work within a well-defined space. We&rsquo;ve seen this with web search companies and e-commerce. It&rsquo;s easy to see it as rules and pattern matching. That&rsquo;s also why these systems do well with board and video games. So if you&rsquo;re considering whether AI will have an impact on your organization, try to think about the things that computer systems are really good at. Do you have a lot of pattern matching in your organization? Do you have a lot of set rules and probabilities? This will be the best place to start when working with artificial intelligence.</p>
<h4 id="the-general-problem-solver">The general problem-solver</h4>
<p>In 1956, computer scientists, Allen Newell and Herbert A. Simon created a program they called the general problem solver. One of the key ideas of the general problem solver was what they called the physical symbol system hypothesis. They said that symbols were a big part of how we interact with the world. When you see a stop sign, you know how to stop for traffic. When you see the letter A, you know that the word will make a certain sound. When you see a sandwich, you might think of eating. They argued that if you could program a machine to connect these symbols then it would be intelligent. But not everyone bought into this idea. If you program a car to stop at a sign, or if you teach a computer to respond to language, then it doesn&rsquo;t make the system intelligent. In 1980, the philosopher John Searle explained that sometimes systems can seem intelligent, but they&rsquo;re just mindlessly matching patterns. To explain, he created what he called the Chinese room argument. In the argument, you should imagine yourself in a windowless room with one mail slot on the door. You can only use this slot to communicate with the outside world. In the room, you have a phrase book on a desk and a bunch of post-it notes with Chinese symbols on the floor. The book shows what response you should use with the note that comes through the slot. It says, &ldquo;If you see this sequence of Chinese symbols, then respond with that sequence of Chinese symbols.&rdquo; Now, imagine a speaker writes something in Chinese Mandarin and pushes it through the slot. You can look at the note and match it with your phrase book. Then you paste together the Mandarin response from the post-it notes on the floor. You have no idea what it says in Mandarin. Instead, you simply go through the process of looking through the book and matching the sequence of symbols. A native Chinese speaker behind the door might believe that they&rsquo;re having a conversation. In fact, they might even assume that the person in the room is a native speaker. But Searle argues that this is far from intelligence, since the person in the room can&rsquo;t speak Mandarin, and doesn&rsquo;t have any idea what they&rsquo;re talking about. You can try a similar experiment with your smartphone. Try asking Siri or Cortana how they feel. They might say they feel fine, but that doesn&rsquo;t mean that they&rsquo;re telling you how they really feel. They also don&rsquo;t know what you&rsquo;re asking. They&rsquo;re just matching your question to a pre-program response, just like the person in the Chinese room. So Searle argues that matching symbols is not a true path to intelligence. That a computer is acting just like the person in the room. They don&rsquo;t understand the meaning. They&rsquo;re just matching patterns from a phrase book. Even with these challenges, physical symbol systems were still the cornerstone of AI for 25 years. Yet, in the end, programming all these matching patterns took up too much time. It was impossible to match all the symbols without running into an explosion of combinations. These combinations would soon fill up even the largest phrase book. There were just too many possibilities to match symbols with their program response. So many philosophers like John Searle, argued that the path would never lead to true intelligence.</p>
<h4 id="strong-vs-weak-ai">Strong vs. weak AI</h4>
<p>So, when is a computer system intelligent? We&rsquo;ve seen that when a computer system is just matching symbols, it&rsquo;s almost like a high-tech phrase book. The system might seem intelligent, but it&rsquo;s actually just like a parrot with a great memory. The philosopher John Searle said that you can think of artificial intelligence in two ways. There&rsquo;s strong AI and weak AI. He thought we were much further away from intelligent systems than most people realize. Strong AI is when a machine displays all the behavior you&rsquo;d expect from a full fledged person. This is usually what you see in science fiction. These are artificial beings that have emotions, a sense of humor, and even have a sense of purpose. That&rsquo;s why C-3PO is scared when they land on Tatooine. It&rsquo;s also why Commander Data shows real creativity when battling the Romulus. On the flip side, there&rsquo;s weak AI. A personal assistant like Apple Siri is a good example of weak AI. This is AI that&rsquo;s confined to a very narrow task. It&rsquo;s like when a system processes language into text or when it sorts all the pictures on your computer. Most AI experts believe that we&rsquo;re just starting down the path of weak AI. Think about Siri. You can talk to Siri and ask questions. Siri listens to your input and then converts your language into something that the computer recognizes. Then, Siri matches a response to what&rsquo;s in her database. Most of the energy right now in AI is around developing and expanding weak AI. Strong AI is still very much just science fiction. In the 1970s and &rsquo;80s, symbolic systems were used to create weak artificial intelligence. These were commonly called expert systems. In these systems, you have experts create a list of steps to solve a complex problem. If the list is long enough, it starts to seem like intelligence. But again, this system is just parroting back program responses created by experts. These expert systems were often used in medicine. A nurse might input symptoms into a computer. If the patient has a cough, then check if they have a temperature. If they have a cough and a temperature, then check to see if they&rsquo;re dehydrated. If they have a cough, a temperature, and are dehydrated, then ask the nurse to check for bronchitis. To a patient, it might look like they&rsquo;re being diagnosed by an intelligent computer. In reality, the program is just matching the symbols and patterns that an expert created to reach a diagnosis. Just like the phrase book in the Chinese Room experiment. In the end, expert systems ran into the same problem as any other symbolic system. They would lead to explosions of combinations. There were just too many patterns to match. Think about all the different questions a doctor might ask to reach a diagnosis. Yet, the symbolic systems approach was a key starting point for artificial intelligence. It&rsquo;s still used today. In fact, many experts still refer to it as GOFAI, or good old-fashioned AI.</p>
<h3 id="the-rise-of-machine-learning">The Rise of Machine Learning</h3>
<h4 id="machine-learning">Machine learning</h4>
<p>Imagine a computer that didn&rsquo;t need to be programmed. The system could learn the same way you do just by observing the world. You&rsquo;ve seen that earlier AI systems used a symbolic approach. The idea was that if the system recognized symbols then it would start to seem intelligent. One of the key challenges was that programmers worked with experts to create the system. That&rsquo;s why they were called expert systems. Later, computer scientists gave up on this approach because it created too many combinations. They decided that you couldn&rsquo;t just program intelligence into a system but maybe you could program a system to become intelligent through observation. It wouldn&rsquo;t feel, hear or see or taste like a human. Instead, it would learn by sensing data. In 1959, a computer scientist named Arthur Samuel created a checkers program that could learn by playing against itself. It played both sides of the board and taught itself strategy through observation. The more the machine played the more it saw patterns on how to win. Computer scientists didn&rsquo;t program the machine to play checkers. It learned through its own experience. Arthur Samuel&rsquo;s called this idea machine learning. This was different from symbolic systems. No human program the moves and counter moves. Instead, the system was designed to learn and improve on its own. The system would quickly learn new checker strategies and after a short period of time it consistently beat its programmer. Machine learning was a breakthrough discovery. There was only one downside. These systems could play games, but in the 1950s there wasn&rsquo;t that much digital data. Remember that machine learning uses data as its five senses. So without data, it could only find the simplest patterns but that all changed. At the beginning of the 1990s, the explosion of the internet suddenly had everyday people creating huge amounts of data. The 1990s was a time of explosive growth for machine learning systems. The new data was like water that poured over the dry fields of artificial intelligence. At this point, machine learning systems had the fuel they needed to become more intelligent. So if you wanted to teach the system on how to identify a cat, you had access to millions of cat pictures online. Computer scientists began to create newer machine learning algorithms. There were even some researchers that started to create systems that were designed to mimic the human brain. One of the big advantages of learning through data is that machines could continue to grow with more data. If the machine finds new patterns it can adapt to the new information. But it&rsquo;s important to keep in mind that you still run into some of the same challenges. The machine learning system is still just identifying patterns. Still, in the last few years machine learning has been the fastest growing area in AI. As the amount of data increases This area has even shown more promise. Organizations are constantly collecting vast amounts of new data. So now the big challenge has become figuring out what to do with all this information. In a sense, you have artificial intelligence systems looking through your data and letting your organization see what it finds.</p>
<h4 id="artificial-neural-networks">Artificial neural networks</h4>
<p>Machine learning has gotten a big boost from artificial neural networks. An artificial neural network is an AI system that mimics the structure of the human brain and it&rsquo;s currently one of the most popular approaches to machine learning. Think of it this way. When I was a kid, we used to play a game called animal, vegetable, or mineral. The point of the game was simple. Someone would think of an item. Then all the other kids would ask questions about what the item was. We&rsquo;d ask things like, does it have fur? Is it bigger than a house? Then in another round, we&rsquo;d ask if it&rsquo;s alive, does it make noise? After each round, we would start to zoom in to identify the item. Then at the end, we started making guesses. Is it a horse? Is it a cat? Is it a dog? An artificial neural network uses a very similar approach, except instead of asking questions, the neural network uses hundreds or even millions of numerical dials. Also, the network makes much more specific guesses. It would say that there&rsquo;s a 64% chance that it&rsquo;s a cat or a 32% chance that it&rsquo;s a dog. To build the network, there&rsquo;s a row of neurons on the left called the input layer. Then there&rsquo;s a row of neurons on the right called the output layer. All the neurons in between are called the hidden layers. They&rsquo;re hidden because it&rsquo;s not the input or output. To start, we&rsquo;d feed the input layer of the network a picture of a dog. We know it&rsquo;s a picture of a dog because we&rsquo;ve labeled it but the neural network doesn&rsquo;t look at these labels until it makes a guess. Just like the animal, vegetable, mineral game. The network would then look at each dot in the image as it passes through the hidden layers. Then it makes a guess for the output layer. Let&rsquo;s say for the first guess, it says there&rsquo;s a 10% chance the image contains a dog. Then the network compares its guess to the label on the image. This is called training the neural network. Then the network goes backwards and adjusts the neuron dials so they can see patterns in dog photos. Then it takes another dog image and sees if it gets closer. The neural network then tunes itself by looking at hundreds of thousands of pictures of dogs. Then it will adjust its own dials until it consistently guesses correctly. Now, it&rsquo;s important to keep in mind that the neural network isn&rsquo;t seeing the dog the same way you do. It doesn&rsquo;t think about panting, barking, or fur. Instead, the system only sees the dog as a recognizable pattern of dots in an image. When there&rsquo;s dots in a dog-like pattern, then the system makes a guess. Now, remember that these neural networks are still machine learning systems, so the network needs access to huge amounts of data. If you don&rsquo;t have hundreds of thousands of pictures of dogs, then there&rsquo;s no way for that artificial neural network to learn. In the end, this is the key benefit of an artificial neural network. It can train itself to understand patterns and recognize that input when looking through massive amounts of data.</p>
<h3 id="common-ai-systems">Common AI Systems</h3>
<h4 id="searching-for-patterns-in-data">Searching for patterns in data</h4>
<p>Over the last 30 years, machine learning systems have become the dominant form of artificial intelligence. That&rsquo;s because these systems are exceptionally well designed to look for patterns in massive data sets. Machine learning has also been supercharged with the wide availability of digital data. If you want to create an AI program to identify dogs, you now have access to millions of images. You can feed your network and help it learn with a large volume of available data. It&rsquo;s the same with other types of data. You can easily get digital video, audio, images and documents. Just a few decades ago, it would&rsquo;ve been extremely difficult to get even a few thousand digital images. Now it&rsquo;s trivial to get access to all kinds of data. Remember that machine learning systems feed on data to learn new things. The more data you feed into the network, the easier it will be for the machine to identify patterns. Think about the system that you&rsquo;re using right now. This is a professional social network that provides video training, users watch the training through an online video player. That video player collects data about how often you fast forward or how long you watch before moving on to the next lesson. Now, suppose that the player records that data for everyone who watches the videos. That might be hundreds of thousands of videos and millions of users. So that&rsquo;s a lot of data. No human could look through all that data and gain any meaning from it. But machine learning algorithms look through this data and find patterns. You can see which content users find more interesting. This is exactly the type of data that many businesses have been looking for. You can now see real time patterns in how your customer interacts with your product. In many ways, this data can tell you a huge amount not just about your customer&rsquo;s interest but even broader trends in industries. This data has an enormous amount of value. You can use it to build new customer products or to improve products you already have. It&rsquo;s no coincidence that companies like Google and Microsoft are most enthusiastic about AI. In many ways, their whole business has been built on using machines to interpret massive data sets. This type of pattern matching can be a huge competitive advantage. Plus, newer artificial neural networks now allow machines to find patterns in even larger data sets. When just a few decades ago, these patterns would&rsquo;ve been imperceptible with regular machine learning algorithms. In fact, one of the largest challenges around machine learning is that humans don&rsquo;t really know how the machine identifies these patterns. It&rsquo;s like a black box of data and processing power. People simply can&rsquo;t process data at that same level. So if your organization is starting its own AI program they should be comfortable with the fact that the network might be sensing things that humans aren&rsquo;t able to perceive. This might not be a challenge with most companies but might be a real problem with industries such as insurance and healthcare. You don&rsquo;t want these systems making decisions about your customer&rsquo;s health and safety that humans can&rsquo;t understand. Artificial intelligence is not the same as human intelligence, and even though we might reach the same conclusions, we&rsquo;re definitely not going through the same process. Think about the type of data your organization collects. Are they using the data for machine learning? If so, what kind of patterns is the machine identifying and for what purpose?</p>
<h4 id="robotics">Robotics</h4>
<p>One of the best ways to connect with humans is to join us in the physical world. That&rsquo;s why robotics is one of the most interesting areas in artificial intelligence. Robotics is about having machines work on physical tasks. This can be lifting heavy objects in manufacturing or using robots that deliver food. Robots can even be vehicles, like self-driving cars or subway trains. Inventors have long been fascinated with finding ways to have machines behave like living objects. In the past, robots were just limited to highly specialized machines. They were used as welding machines in auto manufacturing. The auto plants near my hometown employed several specialized robots. Some could lift a car and install parts underneath, but none of them would ever be mistaken as intelligent. As impressive as they were, these robots were very limited in what they could accomplish. Unless they were programmed, they couldn&rsquo;t help a coworker open a car door or start painting the hood. They worked best for repetitive tasks. Robotics combined with machine learning gives us many more options. A machine can adapt to its environment and learn new tasks on the job. A basic example of this is self-driving vehicles. You couldn&rsquo;t program a car to react to everything it might see on the road. That&rsquo;s why the newest vehicles are using machine learning on an artificial neural network. These vehicles are outfitted with complex sensors that feed data into the network. It needs to understand all the different roads a vehicle might encounter. Then it needs to look at all the different people, animals, and other vehicles the car might find on these roads. Then the machine looks for patterns in successful driving. A car must react differently when a deer is crossing the road than when it sees a pedestrian walking a dog. That&rsquo;s why you often see self-driving cars with a human in the driver&rsquo;s seat. They supervise how the artificial neural network reacts to the data streaming in from the outside world. But like any new skill, it takes time for the machine to collect enough data. In artificial neural networks, this is often called training the network. Google famously said that they think of their self-driving cars not as a robotics problem, but as a data problem. It&rsquo;s true that to figure out how to get a car to steer left or right is simple compared to having a car understand when to turn left or right. Some robots don&rsquo;t need this level of complexity. That&rsquo;s why some of them just use good old-fashioned AI. Remember that this is an AI system that uses symbolic reasoning instead of machine learning. You would just try to program the robot to act intelligently. This is the difference between a Roomba, that&rsquo;s just programmed to avoid bumping into walls, and a self-driving car that really needs to understand the roads. Most robots today are still programmed like a Roomba and not learning like a self-driving car. That&rsquo;s because when you&rsquo;re in the physical world, there&rsquo;s a much higher price to pay if you make a mistake. So if you wanted to create a robot that distributes prescription medication, there&rsquo;s a huge cost to making errors. That&rsquo;s why many robots take the simpler approach and still benefit from symbolic systems and good old-fashioned AI.</p>
<h4 id="natural-language-processing">Natural language processing</h4>
<p>As humans, we&rsquo;re always trying to do a better job of communicating, so it&rsquo;s no surprise that we want to communicate with our machines. In many ways, machine-to-machine communication is much more accurate than human communication. Computer networking transmits exact copies of information at lightning speeds. Humans, on the other hand, are always struggling to reach greater understanding. If you can deliver 10% of what you&rsquo;re intending, then you&rsquo;re a master communicator. That means that the machines we rely on must do a better job of communicating in our world. To achieve this, AI programs do something called natural language processing. This is when you interact with the machine using your own natural language. You can talk with the machine in the same way you talk with other humans. We&rsquo;re all familiar with communicating with a search engine like Google. There&rsquo;s a little box and then you type in questions. You can type something like recipe for Belgian waffles. Then the search engine will match your phrase to popular results. Now with some smart devices, instead of typing, you could say this in natural language. You could use natural language like could you give me a good recipe for those big, fluffy waffles? It&rsquo;s very common for humans to describe things by their attributes. In this case, good, big and fluffy. To understand the request, the system uses natural language processing. The machine needs to understand that good is relative, so in this case the person probably is looking for top recipes. The machine also must figure out what&rsquo;s a big fluffy waffle? Modern natural language processing uses machine learning and artificial neural networks. It looks through millions of conversations to identify patterns. So if it sees conversations with the word big, fluffy, and Belgian waffles, then it knows that there&rsquo;s a pattern between these words. That&rsquo;s why many companies like Google, Microsoft, and Apple offer free email, voicemail, or text. It&rsquo;s a way to have their artificial neural networks look through conversations to better identify patterns. But natural language processing isn&rsquo;t just about understanding the words, it&rsquo;s also understanding the context and meaning. A few years ago, one of the top Google searches was what is love? Humans have written on love from the beginning of language, so there&rsquo;s sure to be plenty of data on the topic. At the time, Google would give you a long list of results. Some of them were about biological pairing rituals and the importance of feeling connected. This was the kind of response you&rsquo;d expect from a network that&rsquo;s just matching keywords in the database. There was nothing natural or human about it. It was just matching keywords in the database. Natural language processing gives the machine the ability to understand the larger world. If you&rsquo;re typing in what is love, then you&rsquo;re probably more interested in romantic notions of love, perhaps even some poetry or insights into what it&rsquo;s like to be in love. Now with natural language processing, the Google response is much more thoughtful. You can see lists of poems in the history of romance, but natural language processing is more than just love and waffles. Communication is at the very core of what it means to be human. That&rsquo;s how we organize, empathize, and understand each other. Humans would never accept a machine as intelligent if it wasn&rsquo;t capable of natural communication.</p>
<h4 id="the-internet-of-things">The Internet of Things</h4>
<p>Today, there are thermostats, doorbells, and televisions that connect with each other and the world. And there are smartwatches that check your location and some even upload medical data. This new way of connecting is commonly called the internet of things. Sometimes you&rsquo;ll see it as IoT for short. IoT devices are objects with sensors that communicate with the outside world. They typically upload data through the internet. IoT devices are a massive new source of data. They can upload their locations, so that means that if you are wearing an IoT device then these machine learning systems can see patterns in where you travel. They can accurately predict where you work, shop, eat, watch, and who you visit. Maybe your smartwatch will tell your thermostat when you&rsquo;re on the way home. IoT also allows these devices to communicate with each other. They could unlock your doors or have your computer turn on when you sit at your desk. In some ways, these new devices make it easier to create data than it is for humans to analyze. That&rsquo;s why many IoT companies invest heavily in AI. AI not only allows organizations to see new patterns, it also allows them to quickly react. IoT devices and artificial intelligence are a powerful combination that allow companies to create systems that predict people&rsquo;s behavior. My family purchased one of the earliest versions of Amazon&rsquo;s Alexa home assistant. It was a terrific and simple clock, but early on my wife noticed that the Amazon recommendation list was becoming populated with things that were part of our conversations. We once talked about visiting a relative in Rome that a few days later, a travel guide to Rome popped up in our Amazon recommended list. Ring doorbells also collect data on people who walk in front of your house. The company can use machine learning for facial recognition. It can then use your doorbell to create a vast surveillance network that&rsquo;s sold to police departments. These departments can use this data to place people at the scene of a crime or locate people they&rsquo;re interested in finding. One area that has a lot of growth is IoT medical devices. You can now purchase a smartwatch that&rsquo;s as accurate as your doctor&rsquo;s electrocardiogram or EKG. They can use the sensors to detect any health issues. Then they can upload that data to their servers. Companies like Apple look for patterns in EKG data using machine learning on a neural network. Their network can go through millions of participants. They can use this data to find patterns to accurately predict any health issues. It&rsquo;s certainly helpful to have machines find predictable patterns. Maybe they&rsquo;ll find that people are more likely to have health issues on Wednesdays. They can also see patterns that affect large groups of people. You could see something like air pollution and its effect on the city&rsquo;s health. In many ways, the IoT devices take the strength of machine learning in the digital world and put it in the realm of the physical world. Instead of just seeing what you&rsquo;re doing online, an IoT device can track your behavior offline. This has enormous potential for organizations to sell your products based on what you need and where you go.</p>
<h3 id="learn-from-data">Learn from Data</h3>
<h4 id="labeled-and-unlabeled-data">Labeled and unlabeled data</h4>
<p>When you think about machine learning the key is to focus on the term learning. What does it mean for your machine to learn? What strategies can you use to learn something new? How can you take the strategies and apply them to machines? Imagine you wanted to learn how to play chess. You could do this a couple of different ways. You could hire a chess tutor, then they would introduce you to some of the different chess pieces. Then they&rsquo;d show you how to move them around the board. You could practice by playing against your tutor. Then they would supervise your moves and help you when you made a mistake. If you couldn&rsquo;t find a tutor you could also go to public parks. There you&rsquo;d watch people play. You couldn&rsquo;t ask them questions. You&rsquo;d just quietly watch and learn. You&rsquo;d have to figure out chess just by watching the games on your own. If you did this long enough you&rsquo;d probably understand the game. You might not know the names of the chess pieces but you&rsquo;d understand the moves and strategies after hundreds of hours of observation. These two strategies are very similar to how a machine learns. The system could do something called supervised learning. Here, a data scientist acts like a tutor for the machine. They show the machine the correct answers and then let the system train itself to get better at the game. The system could also do unsupervised learning. Here you just have the machine make all the observations on its own. The system might not know all the different names and labels, but it&rsquo;ll figure out their way to learn from the data. As you can imagine, these two approaches have their own strengths and weaknesses. For supervised learning the system needs to have a knowledgeable tutor. There must be someone that knows a lot about chess that can show the system how to play the game. With unsupervised learning, the system needs to have access to a lot of data. That&rsquo;s the only way to see the patterns. The system might not be able to go to a public park and watch hundreds of people play. It also depends a little bit on who it watches. You need it to watch people who are good players. As you can imagine, these techniques are used for much more than just playing chess. Companies use these techniques to get valuable insights about their customer. With supervised learning, a company like Amazon might identify a thousand customers who spend a lot of time shopping on their website. The company can then label these customers as high spenders. Then it would have the machine learning system look through the customer to find patterns that make them high spenders. Now, for unsupervised learning a machine learning system could be given access to all of Amazon&rsquo;s customer data. Here, the system might find its own patterns in the data. Maybe somebody who buys chessboards are much more likely to buy an expensive kitchen appliance. Then Amazon could use that data to advertise. If you use Amazon, you may have noticed that sometimes they advertise products that seem completely unrelated to what you&rsquo;re looking for but it&rsquo;s still something you&rsquo;re interested in buying. Both techniques have their own strengths and yet each one can give you incredibly useful insights.</p>
<h4 id="massive-datasets">Massive datasets</h4>
<p>If you&rsquo;ve ever worked as a product manager or a software developer, then you know that applications need very explicit instructions. Every time you open Microsoft Windows or open an app on your iPhone, you&rsquo;re benefiting from a programmer coding the input and output. But we&rsquo;ve seen that this type of programming doesn&rsquo;t work well with artificial intelligence. There&rsquo;s too many combinations to tie every input to an output. In these cases, you need a programming model that allows the machine to learn. You also must give the machine some ability to respond to feedback. Imagine you&rsquo;re creating a program that detects spam messages. These messages are filled with unwanted advertisements or viruses. You could easily program a word filter that deletes messages with common spam words. If you frequently get messages on entering contests, you can filter words like, gold, lottery or winner. So you can program that, &ldquo;If the message contains gold, then treat the message as spam.&rdquo; But complex challenges don&rsquo;t work well when you&rsquo;re limited to programmed instructions. That&rsquo;s why machine learning switches this around. Instead of inputting instructions, you&rsquo;re inputting data. Instead of a program response, you let the machine learn from the patterns it identifies. You can start with supervised machine learning. Here, you need to split your data into a training set and a test set. The training data is a smaller chunk of data that the machine uses to learn. The system will use machine learning algorithms. These algorithms rely on statistics. You&rsquo;ll see a bunch of these machine learning algorithms later on. These algorithms help the machine find relationships within the data. So the machine uses an algorithm that if the email message contains the word lucky winner or congratulations, then it&rsquo;s 50% more likely to be spam. Then once the algorithm is accurate enough, you can use what the system learn on a larger test data set. This test data is usually many times larger than the data that the machine used to train. Let&rsquo;s think about how machine learning might work with our spam detection program. We&rsquo;ll set aside 10,000 email messages for our training set. We&rsquo;ll use it to build our model. This training data has 9,000 regular messages, and 1000 messages labeled as spam. We&rsquo;ll also set aside a million messages for our test data. This test data is unlabeled. That means that it&rsquo;s unlike our training set, no one has correctly labeled any of the messages is spam. That would be a lot of work for a million messages. Your training data is then used to let the machine identify the spam messages. Then once the machine learning algorithm gets close to identifying the thousand spam messages, you&rsquo;ll try it on the larger test data set. Once you&rsquo;re satisfied, that will be your initial data model. Now, this machine learning algorithm only has two options. Is that a regular message or a spam message? That&rsquo;s why this is called, a Binary Classification Challenge. You&rsquo;ll only need to classify your email message into one or two groups. This is one of the most common uses for machine learning. The key thing to remember is that, machine learning algorithms use statistics to find patterns in your data. Once your machine identifies these patterns, it can then classify your data based on what it&rsquo;s learned.</p>
<h3 id="identify-patterns">Identify Patterns</h3>
<h4 id="classify-data">Classify data</h4>
<p>As humans, we classify things all the time. We put our Microsoft Word docs into folders. We separate our business contacts from our personal contacts. We list out things alphabetically. Without these classifications, we&rsquo;d have a hard time organizing the data. Businesses need to organize the data the same way. Airline companies want to classify their customers by frequent flyers. Retailers want to classify their highest spenders. Search engines want to classify the likelihood you&rsquo;ll buy something online. Binary classification is one of the most popular supervised machine learning challenges. That&rsquo;s because it&rsquo;s simple and it&rsquo;s powerful. With binary classification, there are only two possible outcomes. Is the hotel room going to be booked next week? Will the stock market go up this afternoon? Is this email message spam? All binary classification uses supervised machine learning. Remember that supervised learning depends on labeled data. That means that the machine learning system is trained to classify the two answers. So to use these systems, you need to first create a training data set. Credit card fraud detection systems are one of the most popular ways to use binary classification. Every time you use your credit card, a machine learning algorithm classifies your transaction as fraud or not fraud. Since this is supervised machine learning, the credit card companies had to start out with tens of thousands of examples of fraudulent transactions. The data science team would train the system on how to recognize the patterns in future transactions. Email providers use supervised machine learning to classify spam messages. They start with a labeled training set of messages marked as spam. Once the network processes enough messages, it&rsquo;ll classify your spam email. These techniques are inputting massive amounts of data and then using machine learning algorithms to classify your data into human-created categories. Categories like booking data, fraudulent transactions, and unwanted email. A data scientist creates these categories, and then your AI system classifies the data that has been trained to recognize. Now, classification is one of the most popular forms of machine learning, but it also takes a lot of upfront effort to train the system. It can be a challenge to get tens of thousands of fraudulent credit card transactions or tens of thousands of spam email messages. Plus, there&rsquo;s no guarantee that&rsquo;ll be enough for the system to make accurate predictions. That means your data science team might find itself going back and getting another 10,000 transactions. Your team will have to feed the machine learning algorithm until it&rsquo;s extremely accurate at classifying your data. That&rsquo;s why even now, after several years of development, your credit card company might send you a fraud warning even though it&rsquo;s not a fraudulent transaction. Data scientists are constantly training these systems to make the classifications more accurate. Credit card fraud, spam detection, and online purchasing might all seem like very different challenges, but to your machine learning system, they&rsquo;re all just different ways of doing the same thing. You&rsquo;re classifying your labeled data into predefined categories.</p>
<h4 id="cluster-data">Cluster data</h4>
<p>Classifying your data doesn&rsquo;t fit every challenge. For starters, the system won&rsquo;t always have access to massive amounts of labeled data. So sometimes you want to have the system create its own data clusters. Clusters are when the machine uses unsupervised learning to create its own groups of data. If you&rsquo;ve ever bought something online, you might notice that the store will include something called frequently bought together. Maybe you&rsquo;re buying a computer mouse and it recommends a keyboard. This is a very powerful feature and it helps customers find what they need and increases sales for the company. This is an example of a system using unsupervised learning to create clusters based on what it sees in your purchasing history. The big difference between clustering and classifying is whether you&rsquo;re working with human-created categories or machine-created groups. In general, if you&rsquo;re using supervised learning, you&rsquo;re classifying, and if you&rsquo;re using unsupervised learning, then you&rsquo;re clustering. Think of it this way. Every Halloween, my son goes trick-or-treating. This is when kids wear costumes and go around the neighborhood to get candy. At the end of the evening, my son comes home with hundreds of little pieces of candy. The first thing he wants to do is classify the candy by what he likes the best. Now in the past, my son has benefited from my experience. I&rsquo;ve been able to supervise his learning. I can help him create output categories for the candy such as chocolate, peanut butter, mints, and gummies. Then he does his best to classify some of the unknown candies into these categories. This would be the same as supervise machine learning. Now, he also has grandparents that live in a different country. They feel bad that they can&rsquo;t participate in trick-or-treating. So each year, they send a bag of Serbian candy. With this bag, we can&rsquo;t use supervised learning because it&rsquo;s unlabeled. Neither here I have ever seen the candy before and the wrappers are in Cyrillic. So in this case, he does a form of unsupervised learning. He looks at the bag and creates his own clusters. He does this based on his own study of the data. He might create a cluster based on the candy size or the color. In fact, one year, he created a cluster that I&rsquo;d never considered. It was a small cluster that he called perfume candy made from roses and orange blossoms. This is a key part of unsupervised learning. Like my son, the machine studies the data and then comes up with its own clusters. One of the biggest advantages of clustering is that there&rsquo;s a lot more unlabeled data. There&rsquo;s a lot of candy in the world that I&rsquo;ve never seen. So I&rsquo;m not going to be able to create these output categories. There are also many ways that you might want to use machine learning to create clusters. You might want to have your machine learning algorithms cluster your customers. Then a human can go through and see if there&rsquo;s any patterns. At first glance, these clusters might not seem important. They may even seem trivial, but keep in mind that some of the largest companies are built around creating data clusters. Companies like Amazon, Netflix, and Twitter are all using machine learning to cluster your friends, your search history, and buying habits. These systems see patterns that would be impossible for a human to create.</p>
<h4 id="reinforcement-learning">Reinforcement learning</h4>
<p>Online music is close to a $30 billion industry. And if you think about it, it&rsquo;s kind of a strange business. A lot of times you can buy the same songs from Apple Music, Spotify, or Tidal. So why would you want to sign up for one service over the other? For most people, this has to do with the power of their recommendation system. A lot of these systems started off by using unsupervised machine learning. They would recommend songs the same ways that online retailers would say things were frequently bought together. But the best music libraries don&rsquo;t just want you to buy things together, many of them want you to discover something new. So for that, you have to use a different form of machine learning. This from of machine learning is called Reinforcement Learning. These are machine learning algorithms that use rewards as a way to give the system incentive to find new patterns. A few years ago, Google used Reinforcement Learning to teach artificial intelligence systems how to play video games. Their AI systems beat expert players at Pong, Atari, and even more modern video games. But reinforcement learning can do much more than just play video games. These systems can improve over time, by setting a series of goals and rewards. Think of it this way, Spotify Discover Weekly compares your favorite songs with a bunch of related songs. The machine learning algorithm tracks every time you click and play a song. It also keeps track of how long you listen. The data scientists design the algorithms so that every time you click a related song, it gets a tiny digital reward. It&rsquo;s almost like money for the machine learning system. It gets this reward coin when you click on the recommended song. Then the longer you listen, the more the reward increases, so it gets a reward coin for every minute that you listen to the song. These algorithms often use something called Q-Learning. This type of reinforcement learning helps create more sophisticated rewards. In Q-Learning, there are set environments or states, there are also possible actions that can respond to the states. In Q Learning, you want the machine to improve the quality of the outcome, which is represented by the letter Q. In this case, you&rsquo;d start with a Q of zero. Then you&rsquo;d have the machine learn the actions that improve its conditions. The state of Q would go up each time you clicked a song. You can almost think of Q-Learning like the system&rsquo;s bank account. It can earn a bunch of coins and then see the balance grow. Once the reward system is set up, it&rsquo;ll look for patterns to try an increase its Q-Learning bank account. So if it sees a pattern where people who like one song might listen to another, it&rsquo;ll exploit that and make sure to put that song on your Discover Weekly. Reinforcement learning systems work best when your organization wants to do more than just cluster items that are frequently bought together. With Reinforcement Learning, your organization can build a system that thinks creatively about what your customer can discover.</p>
<h3 id="machine-learnig-algorithms">Machine Learnig Algorithms</h3>
<h4 id="common-algorithms">Common algorithms</h4>
<p>Machine learning is one of the most popular areas in artificial intelligence. That&rsquo;s partly because of the explosion of data but it&rsquo;s also because of huge advances in machine learning algorithms. Machine learning on its own is just a set of techniques. It&rsquo;s a way to build systems that learn through data. They move beyond the early AI systems that needed to be explicitly programmed. But there isn&rsquo;t one big machine learning program like Microsoft Office. Instead, you have many different machine learning algorithms. Most of these algorithms are borrowed from statistics. The key thing to remember is that each of these algorithms are like a chef&rsquo;s kitchen tool. Spoons are used for stirring and knives are used for cutting, but sometimes, chefs can find new creative uses for that tool. Some chefs use the side of their knife to crush a garlic clove or use a spoon to twirl spaghetti. I once worked for a company that was using credit card data to try and come up with customer promotions. They started out using supervised machine learning to classify the customers into two different groups. Remember, this is called binary classification. The first group of customers used promotions and the second group never used them. Then they used machine learning algorithms to train the system on this binary classification. Once they classified their customers into these two groups, they used unsupervised machine learning. They wanted to see if they could learn something about their customers who used promotions so they let the machine create clusters of this specific customer. Remember that unsupervised machine learning lets the system create its own clusters based on the patterns that it sees in the data. It&rsquo;s the same way that my son found clusters in unknown candy. What they found was that within this group who used promotions, there was an even smaller group of customers who always used promotions. They called them the promotions super users. Since this company was paid based on the success of its promotions, this was a great group to know. They found that this group used promotions for products, services, and restaurants. Those customers just love saving money, so the company tweaked the algorithm to offer more promotions to these promotions super users. That small change helped boost their overall success rate. Now, this organization used both supervised and unsupervised machine learning, so you&rsquo;ll have some algorithms that work best with supervised classifying and other algorithms that work best with unsupervised clustering. That&rsquo;s why it&rsquo;s important to see some of the machine learning algorithms that are available. Each one has its own strengths and weaknesses. Some take up more processing power while others are lighter and less accurate. Each algorithm is primarily used for either supervised or unsupervised learning, but there are algorithms that you can use for both. Like any good chef, the true creativity is not the tools, the ingredients. Your data science team might want to mix and match these algorithms to gain the greatest insights from your data. These machine learning algorithms are available in most of the machine learning software toolkits so it&rsquo;s here where your data science team can use its skill and creativity.</p>
<h4 id="k-nearest-neighbor">K-nearest neighbor</h4>
<p>In machine learning, one of the best ways to learn more about your data is by classifying it with what you already know. Think of it this way. When I was younger, I worked for an animal shelter in Chicago. One of the most difficult jobs was classifying the breed of incoming dogs. There are hundreds of different dog breeds, and most dogs are mixed. Each time we got a new dog, we would hold it up to the dogs whose breed we already knew. Then we&rsquo;d look at some of the features. Maybe it was the shape of their face or the color of their hair. In a sense, the shelter was classifying the unknown dog by looking at its nearest neighbor. Of course, it&rsquo;s not easy to tell whether a dog was a Boston Terrier or a French bulldog. The closer the match, the more likely it was to be classified. Another way to look at it is we were trying to minimize the distance between the unknown dog and the known breeds. If the features were closely matched, then there was a short distance between the unknown dog and its nearest neighbor. A very common supervised machine learning algorithm for multi-class classification is called K nearest neighbor. The algorithm plots new data and compares it to the data that you already have. Multi-class classification is different from binary classification, because there are more than two dog breeds. Minimizing the distance is a key part of K nearest neighbor. The closer you are to your nearest neighbors, the more likely you are to be accurate. The most common way to do that is through something called Euclidean distance. This is a mathematical formula that can help see the distance between data points. Now imagine you had millions of dogs and you wanted to classify them based on their breed. To start out, you might want to create two key features. These will help you classify the dogs that share the same breed. These are often called classification predictors. So let&rsquo;s use their weight and the length of their hair. Then we&rsquo;ll take these two features and put them on an x y axis diagram. This is the same diagram that you used in geometry in school. Let&rsquo;s put the length of their hair along the y axis and their weight along the x axis. Now take 1,000 labeled dogs for the training set. This will be like the shelter dogs where we already knew the breed. We&rsquo;ll put them on the graph based on their weight and hair length. Now let&rsquo;s take our unknown dog and put it on the chart. You can see that it&rsquo;s not matched with another dog, but it has a bunch of neighbors. Let&rsquo;s say we use a K of five. That means that we&rsquo;d want to put a circle around our unclassified dog and its five nearest neighbors. We can see if the distance of the other dogs is shorter, we&rsquo;ll get a much more accurate classification. Now let&rsquo;s look at the five nearest neighbors. You&rsquo;ll see that three of them are shepherds and two of them are huskies. You can be somewhat confident to classify your unknown dog as a shepherd. There&rsquo;s also a reasonable chance that it&rsquo;s a husky. K nearest neighbor is a very common and powerful machine learning algorithm. That&rsquo;s because you can do more than just classify dogs. In fact, it&rsquo;s commonly used in finance to look for the best stocks, and even predict future performance.</p>
<h4 id="k-means-clustering">K-means clustering</h4>
<p>Another common machine learning algorithm is K-means clustering. K-means clustering is an unsupervised machine learning algorithm. It&rsquo;s used to create clusters based on what your machine sees in the data. Let&rsquo;s go back to the animal shelter in Chicago. The shelter used to have a large social room where the dogs got together, sniffed and played. The dogs had their group of friends and they played and hung out together. Each time they had a social hour, they would self-organize into these social groups. Now imagine that the shelter was closing and all the dogs were going to be distributed to three different shelters. To make it easier for the dogs, the organizers decided to cluster the dogs based on their friend groups. So the shelter created three clusters. That means the K in K-means equaled three, because they wanted to divide the groups into three clusters. To start, the machine put a red, yellow or blue colored collar on three random dogs. Each color represented a potential cluster based on their social group. These would be your three centroid dogs. Each of the centroid dogs would look at the mean or average distance between itself and all of the surrounding dogs. Then the machine would put the same colored collar on the dogs that were closest to a centroid. Since these centroid dogs were selected randomly there was a pretty good chance that they wouldn&rsquo;t really have any good clusters. Maybe all three centroid dogs were in the same social group. If that happened, then most of the dogs would be far away from their nearest centroid, so it would redistribute collars until the algorithm could find a good centroid dog. The machine would try over and over again until it picked the best centroid and it might even do this one cluster at a time. At the end of each iteration, the machine learning algorithm checked the variance between each dog and the centroid. Once there was a good centroid, then it would put the same color collars on the friends of the dogs in each cluster. Now, keep in mind that the dogs themselves did not cluster into three groups. There might be seven or eight different social groups but there were only three shelters, so the algorithm had to do its best to cluster the dogs' natural social grouping. The algorithm also had to consider very social dogs that jump from group to group. If the dogs were moving around too much, then it&rsquo;d be difficult to form real clusters. Another challenge with k-means is it can be very sensitive to outliers. So if you had a dog that wasn&rsquo;t hanging out with other dogs it would still have to be clustered into one of these three groups. Essentially, the dog would be forced to find friends. Organizing dogs into three clusters for three different shelters is probably not a problem you&rsquo;ll run into every day. But K-means clustering is one of the most popular machine learning algorithms. One of the more interesting applications is when retailers use clustering to decide who gets promotions. They might have the system create three clusters to find loyal customers, regular customers and lowest price shoppers. Then they&rsquo;ll create strategies to elevate regular customers into loyal customers. Many organizations are looking for better ways to cluster together their customers. If they can get all of their loyal customers into one cluster, then they can really improve their business. K-means clustering is a good way for the system to cluster people or things by looking at hundreds of different variables.</p>
<h4 id="regression">Regression</h4>
<p>I once worked for a company that sold vehicles online. Each time they sent a customer to an auto dealership the company earned a referral fee. For them, it was always about looking at trends in auto sales. People were much more likely to buy convertibles and sports cars in the spring and summer. Others were more likely to buy trucks and SUVs in the fall and winter. So when customers visited the website they had seasonal promotions for what people wanted to buy. One of the tools they used was regression analysis. Regression analysis is a supervised machine learning algorithm. It looks at the relationship between predictors and the outcome. Sometimes you&rsquo;ll hear predictors called input variables, independent variables, or even regressors. Regression analysis is a supervised machine learning algorithm. You&rsquo;re taking the training data and labeling the correct output. Then you&rsquo;re using the labeled data with the test data. The best way to think about regression is to imagine trends. As the weather gets warmer people are more likely to buy convertibles. As the weather gets cooler people are more likely to buy trucks and SUVs. Regression analysis doesn&rsquo;t tell you why people do these things. This is for data scientists and business analysts to figure out. It just tells you that these things are happening. Machine learning regression algorithms work in a similar way. Once you have your training data, you make a prediction then see how close you are to the outcome. Then you repeat over and over again until the system makes the most accurate prediction. In this case, the data science team thought that the change of seasons would be a great predictor for the sale of some vehicles. So they put the months as a predictor. Then they mapped that against the sales of certain vehicles. With this training data, they created a simple X and Y axis diagram. Remember that this is the simple chart that you learned from geometry. Along the Y or bottom axis they listed the names of the month, and along the X or top axis, they put the sales by vehicle type. Then they looked at the trend line. Convertibles and sports car sales would go up in May, June, and July. Then those sales would go down in September, October and November. For trucks and SUVs, it was the opposite. The more data you have available, the easier it is to make an accurate trend line. As you can imagine, regression can be enormously powerful for organizations. You can make the product available just as the customer&rsquo;s interested in buying. That&rsquo;s why large retailers like Walmart famously stock their shelves with items just as their customers taste change. People buy more Pop-Tarts in the summer and they buy more milk and cheese in the winter. They want to make sure that the shelves are filled with these items based on predicted buying trends. One interesting thing about regression analysis is that there&rsquo;s some question about whether it&rsquo;s true machine learning. That makes sense because the system isn&rsquo;t learning anything new. It&rsquo;s less about learning and more about predicting. Either way, regression is a very popular way for businesses to predict future behavior. And these trends are everywhere. If you&rsquo;re missing them, you&rsquo;re not learning from them. So take a moment to think about where regression analysis might find trends in your business. What you learn might surprise you.</p>
<h4 id="naive-bayes">Naive Bayes</h4>
<p>We&rsquo;ve seen that sometimes you can classify items based on the nearest neighbor. You can also classify based on trends in the data. But sometimes, you want to classify items based on many features in the data. For that, you can use something called Naive Bayes. Naive Bayes is one of the most popular machine learning algorithms. It&rsquo;s naive because it assumes all the predictors are independent from one another. So let&rsquo;s go back to our animal shelter. Imagine we want to classify all the dogs based on their breeds. Let&rsquo;s look at the problem using a Naive Bayes machine learning algorithm. To start, let&rsquo;s create three classes of dog breeds. We&rsquo;ll use terrier, hounds, and sport dogs. Now, for each of these classes, we&rsquo;ll use three features as predictors. Let&rsquo;s use hair length, height, and weight. Remember that some of these predictors will be closely auto correlated. A tall dog is more likely to weigh more. But Naive Bayes considers each one of these predictors independently. Remember, that&rsquo;s why it&rsquo;s called naive. Once you have your classes and predictors set up, then the Naive Bayes will do something called class predictor probability. This is when it looks at each one of the predictors and creates a probability that the dog belongs in this class. So let&rsquo;s see what happens when we try to identify an unknown dog. The first predictor we look at is hair length. The machine learning algorithm checks the probability of a dog with this hair length belonging in the three breeds. It finds that a dog with this hair length has a 40% chance of being a terrier, a 10% chance of being a hound, and a 50% chance that it&rsquo;s a sport dog. The next thing you check is the unknown dog&rsquo;s height. It looks at this predictor independently and tries to calculate the class predictor probability. So it looks at the training data and finds that there&rsquo;s a 20% chance that it&rsquo;s a terrier, a 10% chance it&rsquo;s a hound, and a 70% chance that it&rsquo;s a sport dog. The final thing you want to check is the unknown dog&rsquo;s weight. This might seem like a strange predictor because it&rsquo;s closely related to height. But remember that Naive Bayes is evaluating the probability of each predictor independently. It looks at the training data and finds that there&rsquo;s a 10% chance that it&rsquo;s a terrier, a 5% chance that it&rsquo;s a hound, and an 85% chance that it&rsquo;s a sports dog. So now you have this table with the unknown dog&rsquo;s class predictor probability. If you look at it, you can see that the dog is probably a sport dog. As you can imagine, organizations can use Naive Bayes to do much more than just classify dog breeds. Banks use it to check for fraud. They look at each banking predictor independently, and then measure the likelihood that it&rsquo;s fraud. Then they use a class predictor probability to classify the transaction. Cybersecurity firms also use Naive Bayes to look for securities threats. It looks at each threat predictor independently, and then flags items for security review. The key thing is that because Naive Bayes makes so few assumptions, it can look in an enormous amount of predictors. Often, these extra predictors make it much more accurate when classifying your data.</p>
<h3 id="fit-the-algorithm">Fit the Algorithm</h3>
<h4 id="select-the-best-algorithm">Select the best algorithm</h4>
<p>So now you&rsquo;ve seen three examples of supervised machine-learning algorithms. There was K nearest neighbor, regression analysis, and naive bayes. These are most often used for classifying, and then there was K means clustering which is used for unsupervised learning and clustering. Remember that each of these is like a kitchen tool. These tools are designed for something specific, but you can still be creative with how you use them. It&rsquo;s the same way you can use a fork to whip up eggs or a knife to pit your avocado. But as any good chef knows, you never just present one dish. Instead you&rsquo;re judged by the whole meal. That&rsquo;s why it&rsquo;s very common for data science teams to do something called ensemble modeling. If you&rsquo;re an actor or music fan, then you probably have heard the term ensemble. It&rsquo;s when a group performs together. It&rsquo;s the same thing with machine-learning algorithms. There&rsquo;s a few different ways to create ensembles. The most popular is bagging and stacking. Bagging is when you use several versions of the same machine-learning algorithm. Stacking is when you use several different machine-learning algorithms, then you stack them on top of one another. I used to work for a large home improvement retailer. One of their challenges was what items do they put near the checkout? You&rsquo;d be surprised how much retailers earn by selling something just a few minutes before you checkout. So this was a big challenge and they wanted to create an ensemble of machine-learning algorithms. They debated which ensemble might lead to the best results. They could use bagging to try different results of the same algorithm. Then they&rsquo;d see if they could improve their accuracy. This was a national retail chain so they could pull training data from stores throughout the country. So they could get data samples from random stores, then use K nearest neighbor to classify those datasets separately. Then they would aggregate those results together to see if they could come up with a larger trend. They would aggregate the insights of what people purchased right before checkup. In a sense, they were averaging out the insights to see if they could come up with a more accurate result. The retailer could also try boosting. Here instead of averaging the insights together, they&rsquo;d boost the results step by step so the retailer could take a training set of their most popular items. Let&rsquo;s say that their best selling item was a hammer. Then they could use K nearest neighbor to see what&rsquo;s often bought with the hammer. Let&rsquo;s just say it was nails and a tool belt. Now most of us intuitively know that if someone buys a hammer, they&rsquo;re more likely to buy nails. But that might not help us if we want to put something near the checkout line. For that, we might want to use something like naive bayes. Remember that naive bayes is naive because it doesn&rsquo;t assume that predictors are correlated. So we don&rsquo;t assume that if you&rsquo;re buying a hammer, you&rsquo;re going to need nails. Instead it will will predict other items that are popular but might not seem related. Maybe people who buy hammers are more likely to buy chocolate bars. Mixing and matching your machine-learning algorithms will give you different insights with different results. Like any good ensemble, the accuracy of predictions will depend on the creativity of your data science team.</p>
<h4 id="follow-the-data">Follow the data</h4>
<p>In the old movie, &ldquo;All the President&rsquo;s Men,&rdquo; the top informant of the Nixon scandal met in a parking lot and said, &ldquo;Follow the money.&rdquo; Only by following the money could the reporter find the truth. Like the reporter, machine learning algorithms must follow the data to get to the truth, but that&rsquo;s easier said than done. In fact, one of the biggest challenges in machine learning is balancing the bias and the variance. Bias is the gap between the predicted value and the actual outcome. Let&rsquo;s say that you were playing dice and predicted that you would roll five, three times, but you rolled four, three times. Then your prediction would have a high bias. You were off by one each time. Variance is when the predicted values are scattered all over the place. So if you were playing dice and you predicted that you would roll five, three times, but you actually rolled two, four and six, then you&rsquo;d be off by different amounts. Then your data would be too spread out. Now, it might seem strange to make such a big deal about how the system was wrong, but when you&rsquo;re working with machine learning algorithms, these are two separate challenges, so the system needs to fix it in different ways. Think about the game of darts. The center of the dart board is the machine&rsquo;s best prediction. That means that the little red bullseye in the middle is the right prediction. The machine could throw three darts, and each one of them would be consistently wrong. They&rsquo;d all land in the upper-right-hand corner just above the red bullseye. This is called having a high bias and low variance. The darts are grouped together closely, but all of them are too far to the right. The dataset would have a high bias. That means to make a better prediction the machine would just have to pull the group of darts down and to the left. Now imagine a different challenge. The machine throws the darts at the dart board and they&rsquo;re all over the place. That means that the data has a wide spread, so this data would have a high variance. To make a better prediction, the machine would want to tighten up the darts closest to the bullseye. Ideally, you want the predictions to have a low bias and a low variance. That means that all the darts are in the bullseye. But in most cases, the machine is going to have to fix either a high bias or a high variance. In machine learning, this is such a common problem that it&rsquo;s referred to as the bias-variance trade off. Like any trade off, it means that if the system tries to balance the impact of one, it has to look at the impact on the other. So if the machine decreases the variance spread, it will also have to increase the bias. If the machine increases the bias, it increases the variance spread. That&rsquo;s why the machine needs to follow the data. The machine will turn each one of these knobs to find the best trade off between bias and variance. That way, it can zero in on the best predictions.</p>
<h4 id="overfitting-and-underfitting">Overfitting and underfitting</h4>
<p>When my son was three years old, we told him that he needed to brush his teeth, floss, and take a shower before going to bed. Then one day I got a call from his preschool that said he wouldn&rsquo;t take a nap unless they provided him with a shower, toothbrush, and flosser. I explained to him that it was okay to nap without following those rules. He seemed annoyed, but agreed. The rules we had created for him were too simple. They worked well at home, but didn&rsquo;t fit well in the outside world, so we added greater complexity. He didn&rsquo;t have to follow the rules for preschool naps, when visiting grandparents, or when flying on an airplane. Each time we added more variables, he became more annoyed with the complexity. In supervised machine learning, your AI system can run into the same problem. The system can create simple rules for its training data that doesn&rsquo;t work well when looking at the larger test data. It was like what my son was going through. What works well at home, doesn&rsquo;t work well in the outside world. This challenge is called underfitting the data. Sometimes data scientists add more complexity, and then that complexity makes it more difficult for the system to handle. This was like all the variables we added to my son&rsquo;s simple rule. This is called overfitting the data. Imagine that you work for a website like Zillow that matches up buyers and sellers of homes. One of the key things that you need to do is estimate the value of the home. Your machine could use Naive Bayes to create four predictors, the square footage, the location, number of bathrooms, and the number of bedrooms. That way it could look at each predictor independently and then compare it to recently sold houses. Then the system would come up with an accurate estimate. Now, keep in mind that you&rsquo;re only using four predictors to train your system on how to estimate. So the machine is learning from a simple rule. It&rsquo;s the same as the simple rule to always shower and brush your teeth before sleeping. So there&rsquo;s a very good chance this rule will underfit the data. It&rsquo;s not going to work well when you look at hundreds of thousands of homes. On top of that, housing data usually has a lot of variance. Remember, that&rsquo;s when your data is spread out. There&rsquo;s a lot of homes with different prices that have the same square footage, the same location, and the same number of bathrooms. So that makes it difficult to find a close group. So to fix this, data scientists can create new predictors. Maybe they&rsquo;ll create predictors for quality of view, modern appliances, wood floors, or walkability. This creates a much more complex prediction, because now your machine needs to balance a lot more predictors. So here your rule is overfitting the data. The system needs to look at a lot more relationships between these predictors to make an accurate prediction. The key thing to keep in mind is there isn&rsquo;t really one way to fix this problem. When you&rsquo;re training the system, you need to reach a compromise between simple rules and giving the rules enough complexity to make good predictions. You need to balance unfitting or overfitting the data.</p>
<h3 id="artificial-neural-networks-1">Artificial Neural Networks</h3>
<h4 id="build-a-neural-network">Build a neural network</h4>
<p>Machine learning algorithms see patterns in your data, but sometimes you just have too much data to use these algorithms. So, many large organizations use artificial neural networks instead. Artificial neural networks are a type of machine learning that uses a structure like the human brain to break down massive data sets. Instead of using the previous machine learning algorithms, an artificial neural network breaks down your data into much smaller pieces. Earlier, we talked about artificial neural networks as a machine learning technique that mimics the brain. The network is structured with neurons that are organized into layers. The layers move from left to right. There&rsquo;s the input layer, the hidden layers, and the output layer. If the network has a lot of hidden layers, then it&rsquo;s called a deep learning artificial neural network. That&rsquo;s because the network is many layers deep. The more hidden layers the network has, the easier it&rsquo;ll be for the network to identify very complex patterns. So let&rsquo;s imagine creating an artificial neural network to identify whether there&rsquo;s a dog in an image. Think of it as a binary classification, dog or not dog. To do this, the image from the input layer into dog or not dog needs to be classified. You have the image coming in on the input layer, then the output will be the classification into dog or not dog. We&rsquo;ve already seen that, to a machine, an image is just a collection of different bits of data. So in this case, you&rsquo;ll have a bunch of pixels. These are the tiny points of color on the image and the different levels of brightness or contrast. Let&rsquo;s take an image of a dog and break it into pixels. Let&rsquo;s say that your image is 25 pixels high and 25 pixels wide. So your entire image holds 625 pixels. That means that each image has 625 data points. Let&rsquo;s say that we take these 625 pixels and feed them into a neural network. Each pixel is fed into the input layer. Each of the 625 neurons in the input layer has a number based on the color of the pixel. Each of the neurons in the hidden layer has something called an activation function. An activation function is like a tiny gateway. It lets the neuron decide whether it wants to send the data to the next hidden layer in the network. Each hidden layer feeds the pixel data forward to the next hidden layer. Then, at the very end, there&rsquo;ll be two nodes in the output layer. Remember, this is a binary classification challenge, so there are only two options: is there a dog or not dog? Since the pixel data moves through the layers from left to right, this is called a feedforward neural network. One of the great strengths of artificial neural networks is that they&rsquo;re self-tuning. They&rsquo;re almost like a musical instrument that tune themselves until they get the perfect note. So each of the two neurons in the output layer will have a probability score. The key thing to remember is that an artificial neural network is most often used for supervised learning. You can train the network, and then it will tune itself based on whether it correctly identified your input.</p>
<h4 id="weighing-the-connections">Weighing the connections</h4>
<p>As human beings, we add weights to our data all the time. We look at the features of the data to better predict our output. Let&rsquo;s say that you&rsquo;re looking at a photograph of a beautiful grassy open space. Then you see a little blurry object in the photo. What do you think the odds are that that blurry object is a dog? Now, imagine you&rsquo;re looking at an image of a dry desert. This picture also has a little blurry object. What do you think the odds are that this object is a dog? If you&rsquo;re like most people, you&rsquo;d guess that a dog is much more likely to be in a grassy field, so your human neural connections added a positive weight to the grassy field and a negative weight to the arid desert. Artificial neural networks do the same thing. Like us, these networks need to work in a world of probabilities. It&rsquo;s possible that there&rsquo;s a dog in the middle of the desert, but if you&rsquo;re an artificial neuron, you&rsquo;re going to be very skeptical about activating. An artificial neural network is structured in a way so they can better tune itself to understand your data. It&rsquo;s almost like a self-tuning musical instrument. To tune an instrument like a guitar, you need knobs to twist as you strum the note. With artificial neural networks, these knobs change the weights of your connections between your neurons. An artificial neural network adds weights to the connections between neurons in each layer. Each neuron in the hidden layer feeds forward into every other neuron in the next layer. So if there are 100 neurons in every hidden layer, each neuron in that layer will have 100 connections going out. That&rsquo;s a lot of connectivity. But where it gets really powerful is that each one of these connections will have a weight. That&rsquo;s why if you&rsquo;ve ever seen a sketch of a neural network, you&rsquo;ll see that each one of the connection lines has a W with a number. So in this case, you would have a W1, W2, W3, all the way up to W100. You&rsquo;d see this for each one of the hidden layers. Now, the weights in each one of these connections is a key part of how an artificial neural network tunes itself. Keep in mind, an artificial neural network is just a form of supervised machine learning. So, data scientists use the same technique that they&rsquo;ve used to train the network. Remember that supervised machine learning starts out with a training set. Then once the algorithm is tuned to make accurate predictions, you can then move on to the larger test data set. The same thing happens with your artificial neural network. When you first initialize your neural network, the systems will randomly assign numbers to these thousands of weights. Then you&rsquo;ll feed your training data into the network and let the system adjust the weights based on whether you&rsquo;re getting correct output. The network will repeat this over and over again until it&rsquo;s accurately identifying the patterns for the output. It&rsquo;ll tune itself over time to zero in on the best predictions.</p>
<h4 id="the-activation-bias">The activation bias</h4>
<p>An artificial neural network is self tuning. You&rsquo;ve seen it&rsquo;s like a musical instrument. It compares the output of the perfect note and then twists its own dials to match this sound. But at the end of the day, an artificial neural network is still a form of machine learning. That means it uses many of the same tools and techniques to help the system learn. You&rsquo;ve already seen that an artificial neural network tunes itself by adding weights to the connections, but adding weights to these connections only corrects the variance. Remember earlier that the system is trying to throw darts in a tight cluster near the bullseye. The network will throw a dart and then measure how close it is to making the right prediction. Then it will adjust the weights and throw another dart to see if it&rsquo;s closer. Remember that when you&rsquo;re making a prediction, you need to balance the bias and variance in the data. This is called the bias variance trade off. So adjusting the variance will have an impact on the bias. In an artificial neural network, the bias is the number that the system assigns to each neuron. This bias number will shift the data in a different direction to make it more accurate. The network must tune itself to find a sweet spot between the data bias and the data variance. The main dial that it has to tune itself is adding weights to the connections and adding a bias to the neuron. Sometimes you almost feel bad for making the artificial neural network go through this tuning process. It adjusts the weights of its connections to decrease the variance spread, but that shifts it slightly away from the target. Then it adds a bias to correct for the shift, but then that makes the data spread out again. Humans would find this very frustrating. It&rsquo;s like the machine is trying to throw darts in a tight formation, while at the same time using bias to shift the whole dartboard closer to the bullseye. On top of that, artificial neural networks tend to overfit the data. Remember that overfitting is when the system adds a lot of complexity when it&rsquo;s training. So when an artificial neural network is looking at the data in a training set, it might overlearn lessons about the data. Since it&rsquo;s overfitting its training set, it might make big shifts when you adjust the variance. That makes it even more difficult for the machine to find a nice balance between the bias and the variance. It&rsquo;s like the system is trying to drive straight on an icy road. On one side of the road is too much bias and on the other side of the road is too much variance. If it slides too much in one direction, it has to steer its way back. One key thing to remember about bias is that it&rsquo;s on the neuron and not assigned to the connection like the weights. If you think about it, that makes a lot of sense. The machine can only add the bias after it sees what happens with the variance. In a sense, it can only shift the dartboard after it&rsquo;s already thrown a few darts. Otherwise, the machine wouldn&rsquo;t know which way to make the shifts.</p>
<h3 id="improve-accuracy">Improve Accuracy</h3>
<h4 id="learning-from-mistakes">Learning from mistakes</h4>
<p>We humans tend to think of right and wrong as one or the other. You&rsquo;re either right or you&rsquo;re wrong, but artificial neural networks need to be much more specific. To a neural network, 95% right is much different from 97% right. The challenge is how the system figures out how wrong that is. In a sense, the neural network needs a measure of wrongness. In neural networks this is measured as a cost function. The cost function is just a number that the system uses to measure its answer against the correct answer. If it&rsquo;s really close, then that number will be small. But if it&rsquo;s really far off, then that number will be larger. Say your neural network is trying to determine if an image contains a dog. The network says there&rsquo;s a 97% chance that it&rsquo;s a dog photo, but it turns out that it&rsquo;s a cat photo. That wrongness will have a slight cost. Now, let&rsquo;s say that the network says that there&rsquo;s a 99% chance that it&rsquo;s a dog photo, but this time it&rsquo;s a photo of a snow-covered mountain. This wrongness will have a much higher cost. That&rsquo;s because here the network was very, very wrong, so it needs to make more aggressive adjustments to its weights and biases. Trying to correct for wrongness is a tricky thing. For that, a lot of neural networks use something called gradient descent. Gradient means steepness, and descent means going down. So imagine that your artificial neural network is playing darts again. It&rsquo;s making predictions and seeing how close it gets to the bullseye. Some of the predictions are way off, but other predictions are close. When it throws a dart, there&rsquo;s a distance between it and the dartboard. When it&rsquo;s in the air, the dart travels at an upward angle and then a downward angle. Then it hits the board. If it misses the board entirely, it&rsquo;ll want to make a bigger change to the angle. If it&rsquo;s very close to the target, then it&rsquo;ll want to make the tiniest change to the angle. That way it can hit the bullseye. Well, the neural network does pretty much the same thing. It uses calculations of gradient descent to adjust the weights and biases in the network. It&rsquo;s not using darts, but it&rsquo;s using a very similar calculation to measure the angle of its wrongness. In fact, this is one of the biggest innovations in artificial neural networks. It&rsquo;s called the backpropagation of errors in the network, or backprop for short. Remember, we&rsquo;re using what&rsquo;s called a feed forward artificial network. Your data goes from left to right. It starts at the input layer and moves forward to the output layer. But when your network makes a mistake, it needs to go backwards. It needs to use the gradient descent to determine its wrongness. Then it will use backprop to adjust the weights and biases based on the seriousness of its error. If the network really overshoots the target, it&rsquo;ll want to make extreme adjustments. But if the prediction is very close, then the network should be much more careful. The network will feed forward and then correct backward. That way it can tune itself towards the correct answer.</p>
<h4 id="step-through-the-network">Step through the network</h4>
<p>So what does it take to build an AI system? To think about this, let&rsquo;s go back to our challenge of finding dogs in images. The first step data scientists need to do, is figure out what they want from the data. In this case, they&rsquo;re not asking the AI system to cluster together its own groups. Instead, they&rsquo;re asking the system to classify data into two categories. One category will have images with dogs and the others will be not dogs. This is a classic binary classification challenge. Remember, that&rsquo;s when the neural network just has two possible classifications. This means they&rsquo;ll be doing supervised machine learning. Remember that supervised machine learning starts out with labeled data. Here, the system will be trained with hundreds of thousands of images known to contain dogs. The next step for data scientists is to figure out if they want to use standard machine learning algorithms, or if they&rsquo;d like to use an artificial neural network. Remember that this is a classification problem, so if they go with machine learning algorithms, they&rsquo;ll probably either use K nearest neighbor or naive bayes. You&rsquo;ve seen that the system will break down each image into pixels. That means that this is going to be a complex challenge with a lot of data, so they&rsquo;re going to use an artificial neural network. They&rsquo;ll create the input layer, hidden layers and output layer. Now, remember that since this is a binary classification challenge, there are only two options, dog or not dog, so they&rsquo;ll need just two nodes for the output layer. Next, the neural network will be initialized. The system will assign random numbers to all the weights of the connections. Then, the system will set the bias on all the nodes to zero. This is almost like shaking an etch a sketch to give itself a clean slate. Now, the training set needs to be fed into the neural network. The first few images will probably be not that much better than random guesses. The neural network will say something like, there&rsquo;s a 62% chance that the image contains a dog, or a 55% chance it&rsquo;s not. Then, the network will compare its answer to the label on the data. If it misidentifies the dog image, then it will look at the gradient descent to determine how much to change the weights and biases. The neural network will go through all the data in your training set to fine tune its results. Remember that the network will tune itself by using backpropagation to change the weights and bias to lower the cost function. In a sense, it will go backward through the network and twist all the dials to increase its accuracy. Once the artificial neural network has gone through the training set, then data will be added from the test set. The test set will not be labeled. It could be hundreds of thousands of images of anything. Then you&rsquo;ll see how well your neural network performed when identifying dog photos. Sometimes, the neural network will do very well with the training set, but not so well with the test set. When this happens, it usually means that you are overfitting the data. Remember, that&rsquo;s when the system&rsquo;s really good at identifying the smaller training set but doesn&rsquo;t have enough complexity to deal with the new data in the test set. Now, it&rsquo;s unlikely that your team will classify dog, or not dog, anytime soon. However, this approach to binary classification is a powerful way to get insights from your data. Think about your customer data, your sales data, or even data from your IOT devices and what you can learn from this approach.</p>
<h3 id="where-do-we-go-form-here">Where Do we Go Form Here?</h3>
<h4 id="using-ai-systems">Using AI systems</h4>
<p>We&rsquo;ve seen that artificial intelligence started with the general problem solver. This system used symbols to explicitly program a computer&rsquo;s response. Then just a few years after the first AI conference in 1956, we saw the beginnings of machine learning. Here, instead of programming, the system learned by looking at patterns in data. Now AI systems are integrating themselves in the workplace. They can tackle complex problems and can even come up with creative solutions, and this is why it&rsquo;s important to understand how these systems operate. Most people who interact with AI systems will not be data scientists, many of them will be business people and entrepreneurs. In fact, there&rsquo;s a pretty good chance in the next few years, you will be working on an AI project. Just like how managers work with software developers today, these same managers will start to work with AI systems and data scientists tomorrow. Don&rsquo;t believe me? Imagine someone on your team sent a report on AI that sounds like this, &ldquo;Managers need to be able to set the right goals, evaluate results, and give feedback for AI.&rdquo; Just like any technology, AI needs to be managed effectively to be successful. Here are a few things to remember about AI. First, AI systems are only as good as the data they&rsquo;re given. This data needs to be accurate and representative of the real world. If it&rsquo;s not, then the system will learn from these inaccuracies and produce inaccurate results. Second, AI systems learn by trying different things and seeing which ones work best. This means that there will be some trial and error. As a manager, you need to be prepared for this and have patience while the system is learning. Finally, AI systems can do things that humans can&rsquo;t. They can process large amounts of data quickly and find patterns that we would never be able to see. However, they still need supervision and direction. With these three things in mind, business people can start thinking about how they will work with AI systems. They can start thinking about the problems they want to solve and how they want their systems to solve them. Now, most of what I just said was written by an AI system. It was written using GPT, or Generative Pre-trained Transformer 3. This system is commonly used for natural language generation. The AI system scanned through millions of articles on artificial intelligence and wrote out what I said after a short description. Then it used a deep learning artificial neural network to find patterns in articles about business people in AI. It then just reassembled these common patterns into a few paragraphs of text. So now you can think about the ways that AI systems can help you or your organization. What type of data are you collecting? What would you want to know about your customer that might help enhance your business? Or if you want to work in the field of AI, then think about what problems you hope to solve. In the next few years, these systems will become eerily more human. The organizations that create the best AI systems will be the ones that enhance and not replace human creativity.</p>
<h4 id="applying-ai-to-solve-problems">Applying AI to solve problems</h4>
<p>In this course, you got an overview of the technology behind artificial intelligence. You saw the field started with key concepts, such as the general problem solver and symbolic reasoning. Then you saw how machine learning got a slow start, but then quickly became a dominant field in AI. You got an overview of machine learning concepts and technology. You&rsquo;ve seen how several different machine learning algorithms can help learn from massive data sets. You also saw how artificial neural networks could be used with machine learning to get deeper insights and find complex patterns. Finally, you saw how your network uses back propagation to learn from its mistakes and improve its accuracy. I hope that throughout the course, you got a sense for how to apply some of these technologies to help solve complex problems. You should also think about some of the ethical challenges behind AI. Is it okay to have machines make decisions about your health, welfare, and financial credit? These decisions could come from a black box, where you have no idea what the machine understood. If you&rsquo;re interested in these issues, I&rsquo;ve also created a Data Ethics series, which might be a good follow-up to this course. As always, please feel free to follow me on LinkedIn. There, you&rsquo;ll see articles and links about artificial intelligence, data ethics, and key business challenges. Thank you for watching this introduction to artificial intelligence.</p>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/mainroad/tags/ai/" rel="tag">AI</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/mainroad/tags/microsoft/" rel="tag">Microsoft</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Imageless Thought avatar" src="/mainroad/img/blue/cluster-data-icon-blue.svg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">Imageless Thought</span>
	</div>
	<div class="authorbox__description">
		On the site you will find information intended for a wide audience: faculty researchers, students, and IT professionals. Articles generally provide an introduction to a topic with everything needed to get started quickly. The main goal of the website is to provide a resource for colleagues.
	</div>
</div>


			</div>
			<aside class="sidebar">
<div class="post__toc toc">
	<div class="toc__title">Contents</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a>
      <ul>
        <li><a href="#what-is-generative-ai">What is Generative AI?</a></li>
        <li><a href="#main-models">Main Models</a></li>
        <li><a href="#future-of-ai">Future of AI</a></li>
        <li><a href="#ethics-and-responsibility">Ethics and Responsibility</a></li>
        <li><a href="#next-steps">Next steps</a></li>
      </ul>
    </li>
    <li><a href="#generative-ai-the-evolution-of-thoughtful-online-search">Generative AI: The Evolution of Thoughtful Online Search</a>
      <ul>
        <li><a href="#how-finding-and-sharing-information-online-has-evolved">How finding and sharing information online has evolved</a></li>
        <li><a href="#search-engines-vs-reasoning-engines">Search Engines vs. Reasoning Engines</a></li>
        <li><a href="#thoughtful-search-strategies-and-considerations-in-reasoning-engines">Thoughtful Search Strategies and Considerations in Reasoning Engines</a></li>
        <li><a href="#next-steps-1">Next steps</a></li>
      </ul>
    </li>
    <li><a href="#streamlining-your-work-with-copilot-formerly-bing-chatbing-chat-enterprise">Streamlining Your Work with Copilot (formerly Bing Chat/Bing Chat Enterprise)</a>
      <ul>
        <li><a href="#put-your-fingers-to-work-chatting-as-a-productivity-tool">Put your fingers to work: Chatting as a productivity tool</a></li>
        <li><a href="#getting-started-with-chat-ai">Getting Started with Chat AI</a></li>
        <li><a href="#chatting-with-bing-chat">Chatting with Bing Chat</a></li>
        <li><a href="#considerations-with-chat-ai">Considerations with Chat AI</a></li>
        <li><a href="#bing-chat-enterprise">Bing Chat Enterprise</a></li>
        <li><a href="#conclusion">Conclusion</a></li>
      </ul>
    </li>
    <li><a href="#microsoft-365-copilot-first-look">Microsoft 365 Copilot First Look</a>
      <ul>
        <li><a href="#introduction-1">Introduction</a></li>
        <li><a href="#working-with-microsoft-copilot">Working with Microsoft Copilot</a></li>
      </ul>
    </li>
    <li><a href="#ethics-in-the-age-of-generative-ai">Ethics in the Age of Generative AI</a>
      <ul>
        <li><a href="#introduction-2">Introduction</a></li>
        <li><a href="#developing-the-skill-of-ethical-analysis-in-ai">Developing the skill of ethical analysis in AI</a></li>
        <li><a href="#developing-the-skill-of-ethical-analysis-in-ai-1">Developing the skill of ethical analysis in AI</a></li>
        <li><a href="#conclusion-1">Conclusion</a></li>
      </ul>
    </li>
    <li><a href="#introduction-to-artificial-intelligence">Introduction to Artificial Intelligence</a>
      <ul>
        <li><a href="#introduction-3">Introduction</a></li>
        <li><a href="#what-is-artificial-intelligence">What is Artificial Intelligence</a></li>
        <li><a href="#the-rise-of-machine-learning">The Rise of Machine Learning</a></li>
        <li><a href="#common-ai-systems">Common AI Systems</a></li>
        <li><a href="#learn-from-data">Learn from Data</a></li>
        <li><a href="#identify-patterns">Identify Patterns</a></li>
        <li><a href="#machine-learnig-algorithms">Machine Learnig Algorithms</a></li>
        <li><a href="#fit-the-algorithm">Fit the Algorithm</a></li>
        <li><a href="#artificial-neural-networks-1">Artificial Neural Networks</a></li>
        <li><a href="#improve-accuracy">Improve Accuracy</a></li>
        <li><a href="#where-do-we-go-form-here">Where Do we Go Form Here?</a></li>
      </ul>
    </li>
  </ul>
</nav>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/application/">Application</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/applications/">Applications</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/file-storage/">File Storage</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/flipper-zero/">Flipper Zero</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/getting-started/">Getting Started</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/git/">Git</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/innovation/">Innovation</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/isilion/">Isilion</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/it/">IT</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/modern-campus/">Modern Campus</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/omni-cms/">Omni CMS</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/operations/">Operations</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/penetration-testing/">Penetration Testing</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/pure-data/">Pure Data</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/tour/">Tour</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/training/">Training</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/web-applications/">Web Applications</a></li>
			<li class="widget__item">
				<a class="widget__link" href="/mainroad/categories/web-frameworks/">Web Frameworks</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/ai/" title="AI">AI</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/cio/" title="CIO">CIO</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/digital-signage/" title="Digital Signage">Digital Signage</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/file-storage/" title="File Storage">File Storage</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/git/" title="Git">Git</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/github/" title="GitHub">GitHub</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/hacking/" title="Hacking">Hacking</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/innovation-projects/" title="Innovation Projects">Innovation Projects</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/jekyll/" title="Jekyll">Jekyll</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/linux/" title="Linux">Linux</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/microsoft/" title="Microsoft">Microsoft</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/multi-tools/" title="Multi-Tools">Multi-Tools</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/penetration-testing/" title="Penetration Testing">Penetration Testing</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/projects/" title="Projects">Projects</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/static-websites/" title="Static Websites">Static Websites</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/unt-websites/" title="UNT Websites">UNT Websites</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/web-design/" title="Web Design">Web Design</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/windows/" title="Windows">Windows</a>
		<a class="widget-taglist__link widget__link btn" href="/mainroad/tags/windows-subsystem-linux/" title="Windows Subsystem Linux">Windows Subsystem Linux</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 Imageless Thought |
			<span class="footer__copyright-credits">Some rights reserved.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/mainroad/js/menu.js"></script>
<script src="/mainroad/js/custom.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>